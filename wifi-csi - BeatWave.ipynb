{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: \n",
    "### Please use the code cells after cell number 14 for evaluation of the final model. The cells before are kept for version control and are not meant to generate the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchvision 0.22.1+cu118\n",
      "Uninstalling torchvision-0.22.1+cu118:\n",
      "  Successfully uninstalled torchvision-0.22.1+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "WARNING: Skipping torch as it is not installed.\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "WARNING: Skipping torchaudio as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall torch torchvision torchaudio --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: resampy in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numba>=0.53 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from resampy) (0.61.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from numba>=0.53->resampy) (0.44.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: tqdm in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: soundfile in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: filelock in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu129\n",
      "Requirement already satisfied: torch in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (2.8.0)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu129/torchvision-0.23.0%2Bcu129-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu129/torchaudio-2.8.0%2Bcu129-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu129/torch-2.8.0%2Bcu129-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\wifi-csi-based-activity-recognition\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu129/torchvision-0.23.0%2Bcu129-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.1 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/8.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.3/8.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu129/torch-2.8.0%2Bcu129-cp312-cp312-win_amd64.whl (3571.8 MB)\n",
      "   ---------------------------------------- 0.0/3.6 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.6 GB 15.4 MB/s eta 0:03:52\n",
      "   ---------------------------------------- 0.0/3.6 GB 14.2 MB/s eta 0:04:12\n",
      "   ---------------------------------------- 0.0/3.6 GB 14.6 MB/s eta 0:04:05\n",
      "   ---------------------------------------- 0.0/3.6 GB 14.2 MB/s eta 0:04:11\n",
      "   ---------------------------------------- 0.0/3.6 GB 13.7 MB/s eta 0:04:20\n",
      "   ---------------------------------------- 0.0/3.6 GB 14.0 MB/s eta 0:04:14\n",
      "   ---------------------------------------- 0.0/3.6 GB 13.7 MB/s eta 0:04:20\n",
      "   ---------------------------------------- 0.0/3.6 GB 13.6 MB/s eta 0:04:22\n",
      "   ---------------------------------------- 0.0/3.6 GB 13.3 MB/s eta 0:04:28\n",
      "   ---------------------------------------- 0.0/3.6 GB 13.1 MB/s eta 0:04:31\n",
      "   ---------------------------------------- 0.0/3.6 GB 13.1 MB/s eta 0:04:31\n",
      "   ---------------------------------------- 0.0/3.6 GB 12.9 MB/s eta 0:04:34\n",
      "   ---------------------------------------- 0.0/3.6 GB 12.9 MB/s eta 0:04:36\n",
      "   ---------------------------------------- 0.0/3.6 GB 12.7 MB/s eta 0:04:39\n",
      "   ---------------------------------------- 0.0/3.6 GB 12.3 MB/s eta 0:04:48\n",
      "   ---------------------------------------- 0.0/3.6 GB 12.3 MB/s eta 0:04:47\n",
      "   ---------------------------------------- 0.0/3.6 GB 11.7 MB/s eta 0:05:02\n",
      "   ---------------------------------------- 0.0/3.6 GB 11.8 MB/s eta 0:04:59\n",
      "    --------------------------------------- 0.0/3.6 GB 11.8 MB/s eta 0:04:59\n",
      "    --------------------------------------- 0.0/3.6 GB 11.8 MB/s eta 0:04:58\n",
      "    --------------------------------------- 0.1/3.6 GB 11.7 MB/s eta 0:05:01\n",
      "    --------------------------------------- 0.1/3.6 GB 11.8 MB/s eta 0:04:59\n",
      "    --------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:54\n",
      "    --------------------------------------- 0.1/3.6 GB 12.2 MB/s eta 0:04:49\n",
      "    --------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:47\n",
      "    --------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:46\n",
      "    --------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:45\n",
      "    --------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:45\n",
      "    --------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:45\n",
      "    --------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:45\n",
      "    --------------------------------------- 0.1/3.6 GB 12.2 MB/s eta 0:04:47\n",
      "    --------------------------------------- 0.1/3.6 GB 12.2 MB/s eta 0:04:48\n",
      "    --------------------------------------- 0.1/3.6 GB 12.2 MB/s eta 0:04:46\n",
      "    --------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:45\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:43\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.3 MB/s eta 0:04:42\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.5 MB/s eta 0:04:39\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.4 MB/s eta 0:04:42\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.4 MB/s eta 0:04:40\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.4 MB/s eta 0:04:41\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.5 MB/s eta 0:04:39\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.5 MB/s eta 0:04:37\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.5 MB/s eta 0:04:36\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.6 MB/s eta 0:04:35\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.6 MB/s eta 0:04:35\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.2 MB/s eta 0:04:44\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.1 MB/s eta 0:04:46\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:48\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:48\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:48\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:48\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:47\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:48\n",
      "   - -------------------------------------- 0.1/3.6 GB 11.9 MB/s eta 0:04:49\n",
      "   - -------------------------------------- 0.1/3.6 GB 11.9 MB/s eta 0:04:49\n",
      "   - -------------------------------------- 0.1/3.6 GB 11.9 MB/s eta 0:04:48\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:47\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:46\n",
      "   - -------------------------------------- 0.1/3.6 GB 12.0 MB/s eta 0:04:47\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:45\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:45\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.1 MB/s eta 0:04:44\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:45\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:44\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:45\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:43\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:43\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:43\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:43\n",
      "   - -------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:42\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:45\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:46\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:45\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:44\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:45\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:45\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:44\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:42\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:42\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:40\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:40\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:40\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:40\n",
      "   -- ------------------------------------- 0.2/3.6 GB 12.0 MB/s eta 0:04:40\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.9 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:43\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:42\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:42\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:42\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.2/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.3/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.3/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.3/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.3/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.3/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.3/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   -- ------------------------------------- 0.3/3.6 GB 11.8 MB/s eta 0:04:40\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:41\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:40\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:41\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:40\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:41\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:41\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:41\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:41\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:42\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:42\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:41\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.7 MB/s eta 0:04:39\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:39\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.9 MB/s eta 0:04:36\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:36\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:37\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:38\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:38\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:37\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:37\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:37\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:37\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.8 MB/s eta 0:04:37\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.5 MB/s eta 0:04:44\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.4 MB/s eta 0:04:44\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.4 MB/s eta 0:04:45\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.4 MB/s eta 0:04:45\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.4 MB/s eta 0:04:45\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.4 MB/s eta 0:04:46\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.3 MB/s eta 0:04:48\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.3 MB/s eta 0:04:48\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.2 MB/s eta 0:04:48\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.2 MB/s eta 0:04:49\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.2 MB/s eta 0:04:49\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.2 MB/s eta 0:04:49\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.2 MB/s eta 0:04:49\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.1 MB/s eta 0:04:50\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.1 MB/s eta 0:04:50\n",
      "   --- ------------------------------------ 0.3/3.6 GB 11.1 MB/s eta 0:04:50\n",
      "   --- ------------------------------------ 0.4/3.6 GB 11.1 MB/s eta 0:04:51\n",
      "   --- ------------------------------------ 0.4/3.6 GB 11.1 MB/s eta 0:04:51\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:52\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:53\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:51\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:52\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:52\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:51\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:52\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:52\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:52\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:47\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.2 MB/s eta 0:04:46\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.2 MB/s eta 0:04:44\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.2 MB/s eta 0:04:45\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.2 MB/s eta 0:04:44\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.2 MB/s eta 0:04:43\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.3 MB/s eta 0:04:42\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.3 MB/s eta 0:04:41\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.3 MB/s eta 0:04:41\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.3 MB/s eta 0:04:41\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.3 MB/s eta 0:04:40\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.3 MB/s eta 0:04:40\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.3 MB/s eta 0:04:41\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.3 MB/s eta 0:04:41\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:44\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:44\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:45\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:44\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:44\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:44\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:44\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:45\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:46\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:46\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:46\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:45\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:45\n",
      "   ---- ----------------------------------- 0.4/3.6 GB 11.0 MB/s eta 0:04:44\n",
      "   ----- ---------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:43\n",
      "   ----- ---------------------------------- 0.4/3.6 GB 11.1 MB/s eta 0:04:43\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.0 MB/s eta 0:04:43\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:42\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.1 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:45\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:44\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:45\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:45\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.8 MB/s eta 0:04:46\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.8 MB/s eta 0:04:46\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:43\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:43\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.0 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.0 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.0 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.0 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.0 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 11.0 MB/s eta 0:04:39\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.9 MB/s eta 0:04:40\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.8 MB/s eta 0:04:41\n",
      "   ----- ---------------------------------- 0.5/3.6 GB 10.8 MB/s eta 0:04:43\n",
      "   ------ --------------------------------- 0.5/3.6 GB 10.8 MB/s eta 0:04:42\n",
      "   ------ --------------------------------- 0.5/3.6 GB 10.8 MB/s eta 0:04:43\n",
      "   ------ --------------------------------- 0.5/3.6 GB 10.8 MB/s eta 0:04:42\n",
      "   ------ --------------------------------- 0.5/3.6 GB 10.7 MB/s eta 0:04:43\n",
      "   ------ --------------------------------- 0.5/3.6 GB 10.7 MB/s eta 0:04:43\n",
      "   ------ --------------------------------- 0.5/3.6 GB 10.7 MB/s eta 0:04:43\n",
      "   ------ --------------------------------- 0.5/3.6 GB 10.7 MB/s eta 0:04:43\n",
      "   ------ --------------------------------- 0.5/3.6 GB 10.7 MB/s eta 0:04:44\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:45\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:46\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:47\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.5 MB/s eta 0:04:48\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.5 MB/s eta 0:04:49\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.5 MB/s eta 0:04:48\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.5 MB/s eta 0:04:47\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.5 MB/s eta 0:04:48\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.4 MB/s eta 0:04:48\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.4 MB/s eta 0:04:48\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.5 MB/s eta 0:04:47\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.5 MB/s eta 0:04:47\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.5 MB/s eta 0:04:47\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.4 MB/s eta 0:04:50\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.4 MB/s eta 0:04:50\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:43\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:42\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:42\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:42\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:42\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.7 MB/s eta 0:04:39\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.7 MB/s eta 0:04:39\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:43\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:42\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:40\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:39\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.7 MB/s eta 0:04:39\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.7 MB/s eta 0:04:37\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:36\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:34\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:33\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.9 MB/s eta 0:04:31\n",
      "   ------ --------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:33\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.9 MB/s eta 0:04:31\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.9 MB/s eta 0:04:31\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.8 MB/s eta 0:04:33\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.7 MB/s eta 0:04:33\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.7 MB/s eta 0:04:34\n",
      "   ------- -------------------------------- 0.6/3.6 GB 10.6 MB/s eta 0:04:35\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:35\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:35\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:37\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:38\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:38\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:38\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:38\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:38\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:38\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:34\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:34\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:34\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:33\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:32\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:35\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:35\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:36\n",
      "   ------- -------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:33\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:34\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.5 MB/s eta 0:04:33\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:34\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:33\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:34\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:33\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:33\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.4 MB/s eta 0:04:32\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.6 MB/s eta 0:04:28\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.7 MB/s eta 0:04:26\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.7 MB/s eta 0:04:24\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.8 MB/s eta 0:04:23\n",
      "   -------- ------------------------------- 0.7/3.6 GB 10.7 MB/s eta 0:04:24\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.7 MB/s eta 0:04:24\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.7 MB/s eta 0:04:24\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.7 MB/s eta 0:04:24\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.7 MB/s eta 0:04:25\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.6 MB/s eta 0:04:25\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.6 MB/s eta 0:04:26\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.6 MB/s eta 0:04:26\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.6 MB/s eta 0:04:26\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.6 MB/s eta 0:04:26\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.5 MB/s eta 0:04:26\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.5 MB/s eta 0:04:27\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.5 MB/s eta 0:04:28\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.5 MB/s eta 0:04:28\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.5 MB/s eta 0:04:28\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.5 MB/s eta 0:04:27\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.4 MB/s eta 0:04:28\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.4 MB/s eta 0:04:28\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.4 MB/s eta 0:04:29\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.4 MB/s eta 0:04:30\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.4 MB/s eta 0:04:29\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.3 MB/s eta 0:04:30\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.3 MB/s eta 0:04:30\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.3 MB/s eta 0:04:29\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.4 MB/s eta 0:04:28\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.4 MB/s eta 0:04:28\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.3 MB/s eta 0:04:31\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.3 MB/s eta 0:04:31\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.2 MB/s eta 0:04:31\n",
      "   -------- ------------------------------- 0.8/3.6 GB 10.3 MB/s eta 0:04:30\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.4 MB/s eta 0:04:28\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.3 MB/s eta 0:04:28\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.4 MB/s eta 0:04:26\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.5 MB/s eta 0:04:23\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.5 MB/s eta 0:04:22\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.6 MB/s eta 0:04:20\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.6 MB/s eta 0:04:21\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.6 MB/s eta 0:04:20\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.6 MB/s eta 0:04:20\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.6 MB/s eta 0:04:20\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.6 MB/s eta 0:04:18\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.6 MB/s eta 0:04:18\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.6 MB/s eta 0:04:18\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.8 MB/s eta 0:04:14\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.8 MB/s eta 0:04:14\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.8 MB/s eta 0:04:14\n",
      "   --------- ------------------------------ 0.8/3.6 GB 10.8 MB/s eta 0:04:13\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:13\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:13\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.9 MB/s eta 0:04:09\n",
      "   --------- ------------------------------ 0.9/3.6 GB 11.0 MB/s eta 0:04:08\n",
      "   --------- ------------------------------ 0.9/3.6 GB 11.0 MB/s eta 0:04:08\n",
      "   --------- ------------------------------ 0.9/3.6 GB 11.0 MB/s eta 0:04:07\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.9 MB/s eta 0:04:08\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.9 MB/s eta 0:04:08\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.9 MB/s eta 0:04:07\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.9 MB/s eta 0:04:07\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.9 MB/s eta 0:04:07\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:10\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:11\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:10\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:09\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:09\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:09\n",
      "   --------- ------------------------------ 0.9/3.6 GB 10.8 MB/s eta 0:04:09\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.8 MB/s eta 0:04:09\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.8 MB/s eta 0:04:09\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.8 MB/s eta 0:04:08\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.8 MB/s eta 0:04:07\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:05\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:05\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:03\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:03\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:03\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:03\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:03\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:03\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:02\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:01\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:01\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:01\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:00\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:00\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:03:59\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:03:59\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 11.0 MB/s eta 0:04:00\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:01\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:01\n",
      "   ---------- ----------------------------- 0.9/3.6 GB 10.9 MB/s eta 0:04:02\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:59\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 10.9 MB/s eta 0:03:59\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 11.0 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 10.9 MB/s eta 0:03:58\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 10.8 MB/s eta 0:04:00\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 10.8 MB/s eta 0:04:01\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 10.8 MB/s eta 0:04:02\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 10.8 MB/s eta 0:04:02\n",
      "   ---------- ----------------------------- 1.0/3.6 GB 10.7 MB/s eta 0:04:02\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.7 MB/s eta 0:04:02\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:04\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:04\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:04\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:04\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:05\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:05\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:06\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:05\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:05\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:04\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:04\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:03\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:03\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.7 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.7 MB/s eta 0:03:58\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.7 MB/s eta 0:03:58\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.7 MB/s eta 0:03:59\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.6 MB/s eta 0:03:59\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.0/3.6 GB 10.5 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.5 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.5 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.4 MB/s eta 0:04:02\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.4 MB/s eta 0:04:03\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.5 MB/s eta 0:03:59\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.6 MB/s eta 0:03:58\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.5 MB/s eta 0:03:59\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.5 MB/s eta 0:03:59\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.5 MB/s eta 0:04:00\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.4 MB/s eta 0:04:01\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.3 MB/s eta 0:04:03\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.3 MB/s eta 0:04:04\n",
      "   ----------- ---------------------------- 1.1/3.6 GB 10.3 MB/s eta 0:04:03\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.3 MB/s eta 0:04:04\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.3 MB/s eta 0:04:04\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.2 MB/s eta 0:04:05\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.1 MB/s eta 0:04:06\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.1 MB/s eta 0:04:07\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.1 MB/s eta 0:04:07\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.1 MB/s eta 0:04:07\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.1 MB/s eta 0:04:06\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.1 MB/s eta 0:04:07\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.1 MB/s eta 0:04:07\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.0 MB/s eta 0:04:08\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.0 MB/s eta 0:04:08\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.0 MB/s eta 0:04:08\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.0 MB/s eta 0:04:07\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.0 MB/s eta 0:04:08\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.0 MB/s eta 0:04:08\n",
      "   ------------ --------------------------- 1.1/3.6 GB 10.0 MB/s eta 0:04:08\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.9 MB/s eta 0:04:08\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.9 MB/s eta 0:04:09\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.9 MB/s eta 0:04:10\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.8 MB/s eta 0:04:11\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.8 MB/s eta 0:04:12\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.8 MB/s eta 0:04:12\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.7 MB/s eta 0:04:13\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.7 MB/s eta 0:04:13\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.7 MB/s eta 0:04:14\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:15\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:15\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:15\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:14\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:15\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:15\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.5 MB/s eta 0:04:16\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.5 MB/s eta 0:04:16\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.5 MB/s eta 0:04:17\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:14\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:14\n",
      "   ------------ --------------------------- 1.1/3.6 GB 9.6 MB/s eta 0:04:14\n",
      "   ------------ --------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:13\n",
      "   ------------ --------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:12\n",
      "   ------------ --------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:12\n",
      "   ------------ --------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:11\n",
      "   ------------ --------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:11\n",
      "   ------------ --------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:11\n",
      "   ------------ --------------------------- 1.2/3.6 GB 9.5 MB/s eta 0:04:14\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.5 MB/s eta 0:04:14\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.5 MB/s eta 0:04:15\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.4 MB/s eta 0:04:15\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.4 MB/s eta 0:04:15\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.4 MB/s eta 0:04:15\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.4 MB/s eta 0:04:14\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.5 MB/s eta 0:04:13\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.5 MB/s eta 0:04:13\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.5 MB/s eta 0:04:12\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.5 MB/s eta 0:04:11\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:10\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:09\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:09\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:09\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:09\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:08\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:08\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.6 MB/s eta 0:04:07\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.7 MB/s eta 0:04:05\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.7 MB/s eta 0:04:03\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.8 MB/s eta 0:04:02\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.8 MB/s eta 0:04:00\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.9 MB/s eta 0:03:57\n",
      "   ------------- -------------------------- 1.2/3.6 GB 9.9 MB/s eta 0:03:57\n",
      "   ------------- -------------------------- 1.2/3.6 GB 10.0 MB/s eta 0:03:55\n",
      "   ------------- -------------------------- 1.2/3.6 GB 10.0 MB/s eta 0:03:54\n",
      "   ------------- -------------------------- 1.2/3.6 GB 10.2 MB/s eta 0:03:50\n",
      "   ------------- -------------------------- 1.2/3.6 GB 10.2 MB/s eta 0:03:49\n",
      "   ------------- -------------------------- 1.2/3.6 GB 10.2 MB/s eta 0:03:49\n",
      "   ------------- -------------------------- 1.2/3.6 GB 10.3 MB/s eta 0:03:46\n",
      "   ------------- -------------------------- 1.2/3.6 GB 10.3 MB/s eta 0:03:46\n",
      "   ------------- -------------------------- 1.2/3.6 GB 10.3 MB/s eta 0:03:46\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:45\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:45\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:46\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:46\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:46\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:45\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:45\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.2 MB/s eta 0:03:45\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:45\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:44\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:44\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:44\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.2 MB/s eta 0:03:44\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:43\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:43\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:41\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:41\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:41\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:41\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.2 MB/s eta 0:03:43\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:42\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:42\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:40\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.3 MB/s eta 0:03:40\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.4 MB/s eta 0:03:39\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.4 MB/s eta 0:03:38\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.4 MB/s eta 0:03:37\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.4 MB/s eta 0:03:37\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.5 MB/s eta 0:03:36\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.5 MB/s eta 0:03:34\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.5 MB/s eta 0:03:34\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.6 MB/s eta 0:03:34\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.6 MB/s eta 0:03:33\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.7 MB/s eta 0:03:31\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.9 MB/s eta 0:03:27\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.9 MB/s eta 0:03:26\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.9 MB/s eta 0:03:25\n",
      "   -------------- ------------------------- 1.3/3.6 GB 10.9 MB/s eta 0:03:25\n",
      "   --------------- ------------------------ 1.3/3.6 GB 11.1 MB/s eta 0:03:22\n",
      "   --------------- ------------------------ 1.3/3.6 GB 11.1 MB/s eta 0:03:21\n",
      "   --------------- ------------------------ 1.3/3.6 GB 11.1 MB/s eta 0:03:20\n",
      "   --------------- ------------------------ 1.3/3.6 GB 11.2 MB/s eta 0:03:19\n",
      "   --------------- ------------------------ 1.3/3.6 GB 11.2 MB/s eta 0:03:19\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.0 MB/s eta 0:03:23\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.1 MB/s eta 0:03:20\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.1 MB/s eta 0:03:19\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.1 MB/s eta 0:03:19\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.2 MB/s eta 0:03:18\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.2 MB/s eta 0:03:17\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.2 MB/s eta 0:03:18\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.3 MB/s eta 0:03:15\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.4 MB/s eta 0:03:13\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.5 MB/s eta 0:03:12\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.6 MB/s eta 0:03:10\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.7 MB/s eta 0:03:08\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.7 MB/s eta 0:03:08\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.7 MB/s eta 0:03:07\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.7 MB/s eta 0:03:07\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.7 MB/s eta 0:03:06\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:05\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:05\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:04\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:04\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:04\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:04\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:04\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:04\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:04\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.6 MB/s eta 0:03:06\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.6 MB/s eta 0:03:06\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.6 MB/s eta 0:03:06\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.6 MB/s eta 0:03:07\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:03\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:03\n",
      "   --------------- ------------------------ 1.4/3.6 GB 11.8 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.8 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.8 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.8 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.8 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.7 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.8 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.8 MB/s eta 0:03:01\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.6 MB/s eta 0:03:04\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.6 MB/s eta 0:03:04\n",
      "   ---------------- ----------------------- 1.4/3.6 GB 11.6 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:04\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:01\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.6 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.5 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.5 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.5 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.4 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.4 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.4 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.4 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.3 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.3 MB/s eta 0:03:04\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.3 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.3 MB/s eta 0:03:03\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.4 MB/s eta 0:03:02\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.4 MB/s eta 0:03:01\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.4 MB/s eta 0:03:01\n",
      "   ---------------- ----------------------- 1.5/3.6 GB 11.4 MB/s eta 0:03:00\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.5 MB/s eta 0:02:59\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.5 MB/s eta 0:02:58\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.5 MB/s eta 0:02:59\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.5 MB/s eta 0:02:59\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.5 MB/s eta 0:02:59\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.4 MB/s eta 0:02:59\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.3 MB/s eta 0:03:01\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.3 MB/s eta 0:03:01\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.2 MB/s eta 0:03:02\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.2 MB/s eta 0:03:02\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.2 MB/s eta 0:03:03\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.1 MB/s eta 0:03:03\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.1 MB/s eta 0:03:03\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.1 MB/s eta 0:03:03\n",
      "   ----------------- ---------------------- 1.5/3.6 GB 11.1 MB/s eta 0:03:03\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.1 MB/s eta 0:03:02\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.1 MB/s eta 0:03:02\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.2 MB/s eta 0:03:00\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:58\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:58\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:58\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:58\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:57\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:57\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:56\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:55\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:55\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:55\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:54\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:55\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:54\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:54\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:54\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:54\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.4 MB/s eta 0:02:54\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:55\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:55\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:55\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:55\n",
      "   ----------------- ---------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:56\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:56\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.1 MB/s eta 0:02:57\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.3 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.2 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.0 MB/s eta 0:02:57\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.0 MB/s eta 0:02:57\n",
      "   ------------------ --------------------- 1.6/3.6 GB 10.9 MB/s eta 0:02:57\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.0 MB/s eta 0:02:56\n",
      "   ------------------ --------------------- 1.6/3.6 GB 10.9 MB/s eta 0:02:57\n",
      "   ------------------ --------------------- 1.6/3.6 GB 11.0 MB/s eta 0:02:56\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.0 MB/s eta 0:02:55\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.0 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.0 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.0 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.0 MB/s eta 0:02:54\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.0 MB/s eta 0:02:53\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.1 MB/s eta 0:02:52\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.1 MB/s eta 0:02:51\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.2 MB/s eta 0:02:49\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.3 MB/s eta 0:02:48\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.4 MB/s eta 0:02:47\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.4 MB/s eta 0:02:46\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.4 MB/s eta 0:02:45\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.5 MB/s eta 0:02:44\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.5 MB/s eta 0:02:44\n",
      "   ------------------ --------------------- 1.7/3.6 GB 11.4 MB/s eta 0:02:45\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.4 MB/s eta 0:02:44\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.5 MB/s eta 0:02:43\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.5 MB/s eta 0:02:43\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:40\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:40\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:40\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:39\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:39\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:39\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:39\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.6 MB/s eta 0:02:40\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.6 MB/s eta 0:02:40\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.6 MB/s eta 0:02:40\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.6 MB/s eta 0:02:39\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.6 MB/s eta 0:02:39\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.6 MB/s eta 0:02:38\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:36\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.7 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.7/3.6 GB 11.6 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.6 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.6 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.7 MB/s eta 0:02:36\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.7 MB/s eta 0:02:36\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.6 MB/s eta 0:02:36\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.5 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.6 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.5 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.5 MB/s eta 0:02:37\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.5 MB/s eta 0:02:36\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.6 MB/s eta 0:02:36\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.5 MB/s eta 0:02:36\n",
      "   ------------------- -------------------- 1.8/3.6 GB 11.6 MB/s eta 0:02:35\n",
      "   -------------------- ------------------- 1.8/3.6 GB 11.6 MB/s eta 0:02:35\n",
      "   -------------------- ------------------- 1.8/3.6 GB 11.6 MB/s eta 0:02:34\n",
      "   -------------------- ------------------- 1.8/3.6 GB 11.8 MB/s eta 0:02:31\n",
      "   -------------------- ------------------- 1.8/3.6 GB 11.8 MB/s eta 0:02:30\n",
      "   -------------------- ------------------- 1.8/3.6 GB 11.9 MB/s eta 0:02:30\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.0 MB/s eta 0:02:28\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.0 MB/s eta 0:02:27\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:26\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:26\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.2 MB/s eta 0:02:25\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.2 MB/s eta 0:02:25\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:25\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:25\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.2 MB/s eta 0:02:24\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.2 MB/s eta 0:02:23\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.2 MB/s eta 0:02:24\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.2 MB/s eta 0:02:24\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:24\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.2 MB/s eta 0:02:23\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:24\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:23\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:24\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:23\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:23\n",
      "   -------------------- ------------------- 1.8/3.6 GB 12.1 MB/s eta 0:02:23\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.1 MB/s eta 0:02:23\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.1 MB/s eta 0:02:23\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.1 MB/s eta 0:02:22\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.2 MB/s eta 0:02:21\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.3 MB/s eta 0:02:20\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.3 MB/s eta 0:02:19\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.3 MB/s eta 0:02:19\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.3 MB/s eta 0:02:19\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.3 MB/s eta 0:02:19\n",
      "   -------------------- ------------------- 1.9/3.6 GB 12.3 MB/s eta 0:02:19\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.3 MB/s eta 0:02:19\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.3 MB/s eta 0:02:18\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.3 MB/s eta 0:02:18\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.3 MB/s eta 0:02:18\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.3 MB/s eta 0:02:18\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.3 MB/s eta 0:02:17\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.3 MB/s eta 0:02:17\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.5 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.5 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.5 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.5 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.6 MB/s eta 0:02:13\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.6 MB/s eta 0:02:12\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.6 MB/s eta 0:02:12\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.6 MB/s eta 0:02:12\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:13\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:13\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:12\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:13\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:13\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.4 MB/s eta 0:02:12\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.2 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.2 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.2 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.1 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 1.9/3.6 GB 12.1 MB/s eta 0:02:15\n",
      "   --------------------- ------------------ 2.0/3.6 GB 12.1 MB/s eta 0:02:15\n",
      "   --------------------- ------------------ 2.0/3.6 GB 12.0 MB/s eta 0:02:15\n",
      "   --------------------- ------------------ 2.0/3.6 GB 12.1 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 2.0/3.6 GB 12.1 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 2.0/3.6 GB 12.1 MB/s eta 0:02:14\n",
      "   --------------------- ------------------ 2.0/3.6 GB 12.1 MB/s eta 0:02:14\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:13\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:13\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:13\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:12\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:12\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:11\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.2 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.3 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.3 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.2 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.2 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.2 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.2 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.2 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.2 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.0 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.0 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.0 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 11.9 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 11.9 MB/s eta 0:02:10\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.0 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.0 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:08\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.1 MB/s eta 0:02:08\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.0 MB/s eta 0:02:08\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 12.0 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 11.9 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 11.9 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 11.9 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 11.9 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.0/3.6 GB 11.8 MB/s eta 0:02:09\n",
      "   ---------------------- ----------------- 2.1/3.6 GB 11.8 MB/s eta 0:02:09\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:08\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:08\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.7 MB/s eta 0:02:10\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.7 MB/s eta 0:02:09\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.7 MB/s eta 0:02:09\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.7 MB/s eta 0:02:09\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.7 MB/s eta 0:02:09\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.7 MB/s eta 0:02:09\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.7 MB/s eta 0:02:09\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.7 MB/s eta 0:02:09\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.8 MB/s eta 0:02:07\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.8 MB/s eta 0:02:06\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:06\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:05\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:04\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:04\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:03\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:04\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:03\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:03\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:02\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:02\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:02\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:02\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:02\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:02\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 11.9 MB/s eta 0:02:02\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:01\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:00\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:00\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.1 MB/s eta 0:02:00\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:01:59\n",
      "   ----------------------- ---------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:00\n",
      "   ------------------------ --------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:00\n",
      "   ------------------------ --------------- 2.1/3.6 GB 12.0 MB/s eta 0:02:00\n",
      "   ------------------------ --------------- 2.1/3.6 GB 12.0 MB/s eta 0:01:59\n",
      "   ------------------------ --------------- 2.1/3.6 GB 12.0 MB/s eta 0:01:59\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:59\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:59\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:58\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:58\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:57\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:58\n",
      "   ------------------------ --------------- 2.2/3.6 GB 11.9 MB/s eta 0:01:58\n",
      "   ------------------------ --------------- 2.2/3.6 GB 11.9 MB/s eta 0:01:58\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:56\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:56\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:56\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:55\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:55\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:56\n",
      "   ------------------------ --------------- 2.2/3.6 GB 11.9 MB/s eta 0:01:56\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:56\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:56\n",
      "   ------------------------ --------------- 2.2/3.6 GB 11.9 MB/s eta 0:01:57\n",
      "   ------------------------ --------------- 2.2/3.6 GB 11.9 MB/s eta 0:01:56\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:55\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:54\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:54\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:53\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.2 MB/s eta 0:01:52\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.3 MB/s eta 0:01:51\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.3 MB/s eta 0:01:50\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.3 MB/s eta 0:01:50\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.3 MB/s eta 0:01:50\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.2 MB/s eta 0:01:51\n",
      "   ------------------------ --------------- 2.2/3.6 GB 12.2 MB/s eta 0:01:50\n",
      "   ------------------------- -------------- 2.2/3.6 GB 12.2 MB/s eta 0:01:50\n",
      "   ------------------------- -------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.2/3.6 GB 12.1 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.2/3.6 GB 12.0 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.2/3.6 GB 11.9 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.3/3.6 GB 11.9 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.3/3.6 GB 11.9 MB/s eta 0:01:51\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.0 MB/s eta 0:01:50\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.0 MB/s eta 0:01:50\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.0 MB/s eta 0:01:50\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.0 MB/s eta 0:01:49\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.2 MB/s eta 0:01:48\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.3 MB/s eta 0:01:46\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.4 MB/s eta 0:01:45\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.5 MB/s eta 0:01:44\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:43\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:42\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.5 MB/s eta 0:01:43\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.5 MB/s eta 0:01:43\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:42\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:41\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.7 MB/s eta 0:01:41\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.7 MB/s eta 0:01:41\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.7 MB/s eta 0:01:40\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.7 MB/s eta 0:01:40\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.7 MB/s eta 0:01:40\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:40\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:40\n",
      "   ------------------------- -------------- 2.3/3.6 GB 12.7 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.7 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.7 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:40\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.5 MB/s eta 0:01:40\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.6 MB/s eta 0:01:38\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.5 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.3/3.6 GB 12.5 MB/s eta 0:01:39\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:38\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:38\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:38\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:38\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.4 MB/s eta 0:01:38\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:38\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.4 MB/s eta 0:01:38\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.4 MB/s eta 0:01:37\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:36\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:37\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:36\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.6 MB/s eta 0:01:35\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.6 MB/s eta 0:01:35\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:35\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.5 MB/s eta 0:01:35\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.6 MB/s eta 0:01:34\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.6 MB/s eta 0:01:34\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.7 MB/s eta 0:01:33\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.7 MB/s eta 0:01:33\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.8 MB/s eta 0:01:32\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.7 MB/s eta 0:01:32\n",
      "   -------------------------- ------------- 2.4/3.6 GB 12.7 MB/s eta 0:01:32\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.9 MB/s eta 0:01:31\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.8 MB/s eta 0:01:31\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.8 MB/s eta 0:01:30\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.8 MB/s eta 0:01:30\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.8 MB/s eta 0:01:30\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.9 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.8 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.8 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.8 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.7 MB/s eta 0:01:30\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.7 MB/s eta 0:01:30\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.7 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.7 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.9 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.4/3.6 GB 12.8 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.8 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.7 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.7 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.7 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.7 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.7 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.7 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.7 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.6 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.6 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.5 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.5 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.5 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.3 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.3 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.3 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.3 MB/s eta 0:01:29\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.4 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.4 MB/s eta 0:01:28\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.4 MB/s eta 0:01:27\n",
      "   --------------------------- ------------ 2.5/3.6 GB 12.5 MB/s eta 0:01:26\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.5 MB/s eta 0:01:26\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.6 MB/s eta 0:01:26\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.6 MB/s eta 0:01:25\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.4 MB/s eta 0:01:26\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.5 MB/s eta 0:01:26\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.5 MB/s eta 0:01:25\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.5 MB/s eta 0:01:25\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.5 MB/s eta 0:01:25\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.5 MB/s eta 0:01:25\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.4 MB/s eta 0:01:25\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.4 MB/s eta 0:01:25\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.3 MB/s eta 0:01:26\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.2 MB/s eta 0:01:26\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.1 MB/s eta 0:01:26\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.1 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.0 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 12.0 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 11.9 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 11.9 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 11.9 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 11.8 MB/s eta 0:01:28\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 11.7 MB/s eta 0:01:28\n",
      "   ---------------------------- ----------- 2.5/3.6 GB 11.7 MB/s eta 0:01:28\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.7 MB/s eta 0:01:28\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.7 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.7 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.7 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.6 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.6 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.6 MB/s eta 0:01:28\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.6 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.6 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.6 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.5 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.5 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.5 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.5 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.5 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.4 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.4 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.3 MB/s eta 0:01:27\n",
      "   ---------------------------- ----------- 2.6/3.6 GB 11.5 MB/s eta 0:01:26\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.4 MB/s eta 0:01:26\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.4 MB/s eta 0:01:26\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.3 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.3 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.2 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.2 MB/s eta 0:01:28\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.1 MB/s eta 0:01:28\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.1 MB/s eta 0:01:28\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 11.1 MB/s eta 0:01:28\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.9 MB/s eta 0:01:29\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.9 MB/s eta 0:01:29\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.8 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.8 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.8 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.6 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.6 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.6 MB/s eta 0:01:30\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:29\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:29\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:28\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:28\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:28\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.6/3.6 GB 10.7 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.7 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.7 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.6 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.6 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.6 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.5 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.5 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.4 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.5 MB/s eta 0:01:27\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.5 MB/s eta 0:01:26\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.5 MB/s eta 0:01:26\n",
      "   ----------------------------- ---------- 2.7/3.6 GB 10.5 MB/s eta 0:01:26\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:26\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:25\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:25\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:24\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:24\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:24\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:24\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:23\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:23\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.4 MB/s eta 0:01:24\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.4 MB/s eta 0:01:23\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.4 MB/s eta 0:01:23\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.4 MB/s eta 0:01:23\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:22\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:22\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:22\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.4 MB/s eta 0:01:22\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:22\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:22\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.4 MB/s eta 0:01:21\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.4 MB/s eta 0:01:21\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:21\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:21\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:20\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.6 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.6 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.6 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.6 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.7/3.6 GB 10.5 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.5 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.5 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.5 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.4 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.4 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.3 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.3 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.3 MB/s eta 0:01:19\n",
      "   ------------------------------ --------- 2.8/3.6 GB 10.4 MB/s eta 0:01:18\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.4 MB/s eta 0:01:18\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.4 MB/s eta 0:01:17\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.4 MB/s eta 0:01:17\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.4 MB/s eta 0:01:16\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.5 MB/s eta 0:01:16\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.5 MB/s eta 0:01:15\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.6 MB/s eta 0:01:14\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.7 MB/s eta 0:01:13\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.7 MB/s eta 0:01:13\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.7 MB/s eta 0:01:13\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.8 MB/s eta 0:01:12\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.8 MB/s eta 0:01:12\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.9 MB/s eta 0:01:11\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.9 MB/s eta 0:01:11\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.9 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.9 MB/s eta 0:01:10\n",
      "   ------------------------------- -------- 2.8/3.6 GB 10.9 MB/s eta 0:01:09\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.0 MB/s eta 0:01:09\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.0 MB/s eta 0:01:08\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:08\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:08\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.0 MB/s eta 0:01:07\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.8/3.6 GB 11.1 MB/s eta 0:01:05\n",
      "   ------------------------------- -------- 2.9/3.6 GB 11.1 MB/s eta 0:01:05\n",
      "   ------------------------------- -------- 2.9/3.6 GB 11.0 MB/s eta 0:01:06\n",
      "   ------------------------------- -------- 2.9/3.6 GB 11.1 MB/s eta 0:01:05\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.2 MB/s eta 0:01:04\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.3 MB/s eta 0:01:03\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.4 MB/s eta 0:01:02\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.7 MB/s eta 0:01:01\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:01:00\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:59\n",
      "   -------------------------------- ------- 2.9/3.6 GB 12.0 MB/s eta 0:00:58\n",
      "   -------------------------------- ------- 2.9/3.6 GB 12.0 MB/s eta 0:00:58\n",
      "   -------------------------------- ------- 2.9/3.6 GB 12.0 MB/s eta 0:00:58\n",
      "   -------------------------------- ------- 2.9/3.6 GB 12.0 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 12.0 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 12.0 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:58\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:58\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:58\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.7 MB/s eta 0:00:58\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.7 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.7 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:57\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.7 MB/s eta 0:00:56\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:56\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:56\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.8 MB/s eta 0:00:55\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:55\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:54\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:54\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:54\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:54\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:53\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:53\n",
      "   -------------------------------- ------- 2.9/3.6 GB 11.9 MB/s eta 0:00:53\n",
      "   --------------------------------- ------ 2.9/3.6 GB 11.8 MB/s eta 0:00:53\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.8 MB/s eta 0:00:53\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.8 MB/s eta 0:00:53\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.8 MB/s eta 0:00:53\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.9 MB/s eta 0:00:52\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:51\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:51\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:51\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.9 MB/s eta 0:00:51\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.9 MB/s eta 0:00:51\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.9 MB/s eta 0:00:51\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.9 MB/s eta 0:00:51\n",
      "   --------------------------------- ------ 3.0/3.6 GB 11.9 MB/s eta 0:00:50\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:50\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:49\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:49\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:49\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:49\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:48\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.0 MB/s eta 0:00:48\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.1 MB/s eta 0:00:48\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.1 MB/s eta 0:00:48\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.2 MB/s eta 0:00:47\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.3 MB/s eta 0:00:46\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.5 MB/s eta 0:00:45\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.7 MB/s eta 0:00:44\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.7 MB/s eta 0:00:44\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.8 MB/s eta 0:00:43\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.8 MB/s eta 0:00:43\n",
      "   --------------------------------- ------ 3.0/3.6 GB 12.8 MB/s eta 0:00:42\n",
      "   ---------------------------------- ----- 3.0/3.6 GB 12.9 MB/s eta 0:00:42\n",
      "   ---------------------------------- ----- 3.0/3.6 GB 12.9 MB/s eta 0:00:42\n",
      "   ---------------------------------- ----- 3.0/3.6 GB 12.9 MB/s eta 0:00:42\n",
      "   ---------------------------------- ----- 3.0/3.6 GB 12.8 MB/s eta 0:00:42\n",
      "   ---------------------------------- ----- 3.0/3.6 GB 12.8 MB/s eta 0:00:42\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:41\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:41\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:41\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:41\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:41\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:41\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:40\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:40\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:40\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:40\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.6 MB/s eta 0:00:40\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:39\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.6 MB/s eta 0:00:39\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.6 MB/s eta 0:00:39\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.5 MB/s eta 0:00:39\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.5 MB/s eta 0:00:39\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.6 MB/s eta 0:00:39\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.6 MB/s eta 0:00:39\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.6 MB/s eta 0:00:38\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.7 MB/s eta 0:00:38\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.9 MB/s eta 0:00:37\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 12.9 MB/s eta 0:00:36\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 13.0 MB/s eta 0:00:36\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 13.2 MB/s eta 0:00:35\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 13.3 MB/s eta 0:00:35\n",
      "   ---------------------------------- ----- 3.1/3.6 GB 13.3 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 13.3 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 13.3 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 13.3 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 13.3 MB/s eta 0:00:33\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 13.3 MB/s eta 0:00:33\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 13.0 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 12.9 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 12.9 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 12.9 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 12.9 MB/s eta 0:00:34\n",
      "   ----------------------------------- ---- 3.1/3.6 GB 12.9 MB/s eta 0:00:33\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 12.8 MB/s eta 0:00:33\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 12.9 MB/s eta 0:00:33\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.0 MB/s eta 0:00:32\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.1 MB/s eta 0:00:32\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.2 MB/s eta 0:00:31\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.2 MB/s eta 0:00:31\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.4 MB/s eta 0:00:30\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.4 MB/s eta 0:00:30\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.4 MB/s eta 0:00:30\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.5 MB/s eta 0:00:29\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.5 MB/s eta 0:00:29\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.5 MB/s eta 0:00:29\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.5 MB/s eta 0:00:29\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.4 MB/s eta 0:00:29\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.4 MB/s eta 0:00:29\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.4 MB/s eta 0:00:29\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.3 MB/s eta 0:00:28\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.3 MB/s eta 0:00:28\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.3 MB/s eta 0:00:28\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.3 MB/s eta 0:00:28\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.3 MB/s eta 0:00:28\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.3 MB/s eta 0:00:28\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.2 MB/s eta 0:00:28\n",
      "   ----------------------------------- ---- 3.2/3.6 GB 13.2 MB/s eta 0:00:28\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.2 MB/s eta 0:00:27\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.2 MB/s eta 0:00:27\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.1 MB/s eta 0:00:27\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.1 MB/s eta 0:00:27\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.1 MB/s eta 0:00:27\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.2 MB/s eta 0:00:26\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.1 MB/s eta 0:00:27\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.1 MB/s eta 0:00:26\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.1 MB/s eta 0:00:26\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.1 MB/s eta 0:00:26\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.2 MB/s eta 0:00:26\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.2 MB/s eta 0:00:25\n",
      "   ------------------------------------ --- 3.2/3.6 GB 13.3 MB/s eta 0:00:25\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.2 MB/s eta 0:00:25\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.2 MB/s eta 0:00:25\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.2 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.2 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.2 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.2 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.1 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.1 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.0 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 13.0 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.9 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.8 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.7 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.6 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.5 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.5 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.5 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.5 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.3 MB/s eta 0:00:24\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.3 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.2 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.2 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.1 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.1 MB/s eta 0:00:23\n",
      "   ------------------------------------ --- 3.3/3.6 GB 12.0 MB/s eta 0:00:23\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.1 MB/s eta 0:00:23\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.1 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.1 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.1 MB/s eta 0:00:22\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.2 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.2 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.1 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.2 MB/s eta 0:00:21\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.2 MB/s eta 0:00:20\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.3 MB/s eta 0:00:20\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.4 MB/s eta 0:00:19\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.4 MB/s eta 0:00:19\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.5 MB/s eta 0:00:19\n",
      "   ------------------------------------- -- 3.3/3.6 GB 12.6 MB/s eta 0:00:18\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.6 MB/s eta 0:00:18\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.6 MB/s eta 0:00:18\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.6 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.6 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.5 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.5 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.4 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.3 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.3 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.2 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.1 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.1 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.1 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.0 MB/s eta 0:00:17\n",
      "   ------------------------------------- -- 3.4/3.6 GB 12.0 MB/s eta 0:00:16\n",
      "   ------------------------------------- -- 3.4/3.6 GB 11.9 MB/s eta 0:00:16\n",
      "   ------------------------------------- -- 3.4/3.6 GB 11.9 MB/s eta 0:00:16\n",
      "   ------------------------------------- -- 3.4/3.6 GB 11.9 MB/s eta 0:00:16\n",
      "   -------------------------------------- - 3.4/3.6 GB 11.9 MB/s eta 0:00:15\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.2 MB/s eta 0:00:15\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.3 MB/s eta 0:00:14\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.3 MB/s eta 0:00:14\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.3 MB/s eta 0:00:14\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.4 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.4 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.3 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.2 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.3 MB/s eta 0:00:13\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.3 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.2 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.2 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.2 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.2 MB/s eta 0:00:12\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.1 MB/s eta 0:00:11\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.1 MB/s eta 0:00:11\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.1 MB/s eta 0:00:11\n",
      "   -------------------------------------- - 3.4/3.6 GB 12.1 MB/s eta 0:00:11\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.2 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.2 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.2 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.2 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.0 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.1 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.1 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.1 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.2 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.3 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.3 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 3.5/3.6 GB 12.4 MB/s eta 0:00:08\n",
      "   ---------------------------------------  3.5/3.6 GB 12.4 MB/s eta 0:00:07\n",
      "   ---------------------------------------  3.5/3.6 GB 12.4 MB/s eta 0:00:07\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:07\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:07\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:06\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:06\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:06\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:05\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:05\n",
      "   ---------------------------------------  3.5/3.6 GB 12.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:05\n",
      "   ---------------------------------------  3.5/3.6 GB 12.7 MB/s eta 0:00:05\n",
      "   ---------------------------------------  3.5/3.6 GB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------------------  3.5/3.6 GB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------------------  3.5/3.6 GB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------------------  3.5/3.6 GB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------------------  3.5/3.6 GB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------------------  3.5/3.6 GB 12.8 MB/s eta 0:00:03\n",
      "   ---------------------------------------  3.5/3.6 GB 12.9 MB/s eta 0:00:03\n",
      "   ---------------------------------------  3.5/3.6 GB 13.0 MB/s eta 0:00:03\n",
      "   ---------------------------------------  3.5/3.6 GB 13.0 MB/s eta 0:00:03\n",
      "   ---------------------------------------  3.5/3.6 GB 13.0 MB/s eta 0:00:03\n",
      "   ---------------------------------------  3.5/3.6 GB 13.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  3.5/3.6 GB 13.2 MB/s eta 0:00:02\n",
      "   ---------------------------------------  3.6/3.6 GB 13.2 MB/s eta 0:00:02\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.6 GB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.6/3.6 GB 8.4 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu129/torchaudio-2.8.0%2Bcu129-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.4/4.3 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 11.3 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\n",
      "  Attempting uninstall: torch\n",
      "\n",
      "    Found existing installation: torch 2.8.0\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "    Uninstalling torch-2.8.0:\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "      Successfully uninstalled torch-2.8.0\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   ---------------------------------------- 3/3 [torchaudio]\n",
      "\n",
      "Successfully installed torch-2.8.0+cu129 torchaudio-2.8.0+cu129 torchvision-0.23.0+cu129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas resampy\n",
    "! pip install torch pandas numpy tqdm matplotlib soundfile\n",
    "! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:11.789376Z",
     "iopub.status.busy": "2025-01-23T20:14:11.789076Z",
     "iopub.status.idle": "2025-01-23T20:14:13.496307Z",
     "shell.execute_reply": "2025-01-23T20:14:13.494845Z",
     "shell.execute_reply.started": "2025-01-23T20:14:11.789350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only 44 matched pairs found. Some files are missing their pair.\n",
      "Dataset CSV created at data_paths.csv with 44 matched pairs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "csi_directory = \"../data_capture/csi/top_csi_filtered\"\n",
    "audio_directory = \"../data_capture/audio_matrix\"\n",
    "\n",
    "# Helper to extract timestamp from filename\n",
    "def extract_timestamp(filename):\n",
    "    # For audio: 2025-05-07_20-11-50.303.csv\n",
    "    # For csi: top8_filtered_csi_data_2025-05-07_20-11-50.303.csv\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\\.\\d+)', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Build lookup for CSI files by timestamp\n",
    "csi_files = [f for f in os.listdir(csi_directory) if f.startswith('top8_filtered_csi_data_') and f.endswith('.csv')]\n",
    "csi_timestamps = {extract_timestamp(f): f for f in csi_files}\n",
    "\n",
    "audio_files = [f for f in os.listdir(audio_directory) if f.endswith('.csv')]\n",
    "audio_timestamps = {extract_timestamp(f): f for f in audio_files}\n",
    "\n",
    "common_timestamps = sorted(set(csi_timestamps) & set(audio_timestamps))\n",
    "\n",
    "if len(common_timestamps) < len(csi_timestamps) or len(common_timestamps) < len(audio_timestamps):\n",
    "    print(f\"Warning: Only {len(common_timestamps)} matched pairs found. Some files are missing their pair.\")\n",
    "\n",
    "data = []\n",
    "for ts in common_timestamps:\n",
    "    csi_file = csi_timestamps[ts]\n",
    "    audio_file = audio_timestamps[ts]\n",
    "    data.append({\n",
    "        \"csi_path\": os.path.join(csi_directory, csi_file),\n",
    "        \"audio_path\": os.path.join(audio_directory, audio_file)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "output_path = \"data_paths.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Dataset CSV created at {output_path} with {len(df)} matched pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:13.498017Z",
     "iopub.status.busy": "2025-01-23T20:14:13.497575Z",
     "iopub.status.idle": "2025-01-23T20:14:18.865097Z",
     "shell.execute_reply": "2025-01-23T20:14:18.863875Z",
     "shell.execute_reply.started": "2025-01-23T20:14:13.497973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrow, Rectangle\n",
    "from matplotlib.sankey import Sankey\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:18.867570Z",
     "iopub.status.busy": "2025-01-23T20:14:18.866895Z",
     "iopub.status.idle": "2025-01-23T20:14:19.390396Z",
     "shell.execute_reply": "2025-01-23T20:14:19.388552Z",
     "shell.execute_reply.started": "2025-01-23T20:14:18.867524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "latent_dim = 128            \n",
    "csi_channels = 1         \n",
    "audio_channels = 1       \n",
    "epochs = 100              \n",
    "batch_size = 64         \n",
    "learning_rate = 0.0002   \n",
    "beta1 = 0.9                 \n",
    "beta2 = 0.999               \n",
    "sample_rate = 16000         \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def visualize_hyperparameters(params):\n",
    "    # Split the parameters into two categories: small values and large values\n",
    "    small_values = {key: value for key, value in params.items() if value < 1}\n",
    "    large_values = {key: value for key, value in params.items() if value >= 1}\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if small_values:\n",
    "        names = list(small_values.keys())\n",
    "        values = list(small_values.values())\n",
    "        plt.barh(names, values, color=\"lightcoral\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.title(\"Hyperparameters with Small Values (< 1)\")\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if large_values:\n",
    "        names = list(large_values.keys())\n",
    "        values = list(large_values.values())\n",
    "        plt.barh(names, values, color=\"skyblue\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.show()\n",
    "\n",
    "hyperparameters = {\n",
    "    \"latent_dim\": latent_dim,\n",
    "    \"csi_channels\": csi_channels,\n",
    "    \"audio_channels\": audio_channels,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"beta1\": beta1,\n",
    "    \"beta2\": beta2,\n",
    "    \"sample_rate\": sample_rate\n",
    "}\n",
    "\n",
    "visualize_hyperparameters(hyperparameters)\n",
    "\n",
    "class CSIAudioDataset(Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        logging.debug(f\"Processing index: {idx}\")\n",
    "        csi_path = self.data_df.iloc[idx]['csi_path']\n",
    "        csi_data = pd.read_csv(csi_path)\n",
    "        subcarrier_cols = [col for col in csi_data.columns if col.startswith('subcarrier_')]\n",
    "        csi_data = csi_data[subcarrier_cols].values.T  # shape: (8, N)\n",
    "        target_len = 400\n",
    "        csi_data_resampled = np.array([\n",
    "            np.interp(\n",
    "                np.linspace(0, len(channel) - 1, target_len),\n",
    "                np.arange(len(channel)),\n",
    "                channel\n",
    "            ) for channel in csi_data\n",
    "        ])\n",
    "        csi_tensor = torch.tensor(csi_data_resampled, dtype=torch.float32)\n",
    "\n",
    "        audio_path = self.data_df.iloc[idx]['audio_path']\n",
    "        audio_data = pd.read_csv(audio_path, header=None).values.flatten()\n",
    "        audio_data_resampled = np.interp(\n",
    "            np.linspace(0, len(audio_data) - 1, target_len),\n",
    "            np.arange(len(audio_data)),\n",
    "            audio_data\n",
    "        )\n",
    "        audio_tensor = torch.tensor(audio_data_resampled, dtype=torch.float32)\n",
    "\n",
    "        min_len = min(csi_tensor.shape[1], len(audio_tensor))\n",
    "        if min_len == 0:\n",
    "            logging.warning(f\"No valid audio length for index: {idx}\")\n",
    "            return None\n",
    "\n",
    "        csi_tensor_trimmed = csi_tensor[:, :min_len]\n",
    "        audio_trimmed = audio_tensor[:min_len]\n",
    "        return csi_tensor_trimmed, audio_trimmed\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:19.393124Z",
     "iopub.status.busy": "2025-01-23T20:14:19.391965Z",
     "iopub.status.idle": "2025-01-23T20:14:19.652175Z",
     "shell.execute_reply": "2025-01-23T20:14:19.650926Z",
     "shell.execute_reply.started": "2025-01-23T20:14:19.393056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data_paths.csv\")\n",
    "train_dataset = CSIAudioDataset(train_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    [data for data in train_dataset if data is not None],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:19.653479Z",
     "iopub.status.busy": "2025-01-23T20:14:19.653183Z",
     "iopub.status.idle": "2025-01-23T20:14:19.915587Z",
     "shell.execute_reply": "2025-01-23T20:14:19.914222Z",
     "shell.execute_reply.started": "2025-01-23T20:14:19.653450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, csi_channels, audio_channels=1, csi_length=400):\n",
    "        super(Generator, self).__init__()\n",
    "        input_dim = latent_dim + csi_channels * csi_length  # Total input dimension after concatenation\n",
    "        self.fc = nn.Linear(input_dim, 256 * 100)  # Project to 256 channels with 100 time steps\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, audio_channels, kernel_size=1)  # Final projection to audio\n",
    "\n",
    "    def forward(self, z, csi_data):\n",
    "        # Flatten CSI data\n",
    "        csi_flat = csi_data.view(csi_data.size(0), -1)\n",
    "        \n",
    "        # Concatenate latent vector and flattened CSI data\n",
    "        x = torch.cat([z, csi_flat], dim=1)\n",
    "        \n",
    "        # Project to the correct shape for transposed convolutions\n",
    "        x = self.fc(x).view(-1, 256, 100)  # Reshape to (batch_size, channels, time_steps)\n",
    "        \n",
    "        # Apply transposed convolutions\n",
    "        x = F.relu(self.conv1(x))  # Output: (batch_size, 128, 200)\n",
    "        x = F.relu(self.conv2(x))  # Output: (batch_size, 64, 400)\n",
    "        x = self.conv3(x)          # Output: (batch_size, 1, 400)\n",
    "        \n",
    "        return torch.sigmoid(x.view(-1, 400))  # Reshape to (batch_size, 400)\n",
    "\n",
    "def visualize_generator_architecture():\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    layers = [\n",
    "        {\"name\": \"Input \\n(Noise + CSI)\", \"desc\": \"Concatenates latent vector \\nand flattened CSI\", \"pos\": (0, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"Dense +\\n Reshape\", \"desc\": \"Linear: (noise+csi)  256*100\\nReshape: 256100\", \"pos\": (4, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"ConvTranspose1D\\n(256128)\", \"desc\": \"Kernel=4, Stride=2\\nOutput: 128200\", \"pos\": (8, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"ConvTranspose1D\\n(12864)\", \"desc\": \"Kernel=4, Stride=2\\nOutput: 64400\", \"pos\": (12, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"Conv1D\\n(641)\", \"desc\": \"Final projection to\\naudio dimensions\", \"pos\": (16, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"Output\", \"desc\": \"Generated audio waveform\\n(400 points)\", \"pos\": (20, 0), \"size\": (3, 2)}\n",
    "    ]\n",
    "    \n",
    "    # Updated data shapes\n",
    "    data_shapes = [\n",
    "        \"(Batch x (latent_dim + \\ncsi_channels*400))\",\n",
    "        \"(Batch x 256 x 100)\",\n",
    "        \"(Batch x 128 x 200)\",\n",
    "        \"(Batch x 64 x 400)\",\n",
    "        \"(Batch x 1 x 400)\",\n",
    "        \"(Batch x 400)\"\n",
    "    ]\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        rect = Rectangle(layer[\"pos\"], *layer[\"size\"], edgecolor=\"black\", facecolor=\"lightgreen\", linewidth=2.5)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(\n",
    "            layer[\"pos\"][0] + layer[\"size\"][0] / 2,\n",
    "            layer[\"pos\"][1] + layer[\"size\"][1] / 2 + 0.2,\n",
    "            layer[\"name\"],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\"\n",
    "        )\n",
    "        \n",
    "        ax.text(\n",
    "            layer[\"pos\"][0] + layer[\"size\"][0] / 2,\n",
    "            layer[\"pos\"][1] - 0.8,\n",
    "            layer[\"desc\"],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=10,\n",
    "            color=\"darkgray\"\n",
    "        )\n",
    "        \n",
    "        ax.text(\n",
    "            layer[\"pos\"][0] + layer[\"size\"][0] / 2,\n",
    "            layer[\"pos\"][1] + layer[\"size\"][1] + 0.6,\n",
    "            data_shapes[i],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=10,\n",
    "            color=\"darkblue\"\n",
    "        )\n",
    "        \n",
    "        if i < len(layers) - 1:\n",
    "            start_x = layer[\"pos\"][0] + layer[\"size\"][0]\n",
    "            start_y = layer[\"pos\"][1] + layer[\"size\"][1] / 2\n",
    "            end_x = layers[i + 1][\"pos\"][0]\n",
    "            end_y = layers[i + 1][\"pos\"][1] + layers[i + 1][\"size\"][1] / 2\n",
    "            arrow = FancyArrow(start_x, start_y, end_x - start_x - 0.2, end_y - start_y, width=0.1, color=\"black\")\n",
    "            ax.add_patch(arrow)\n",
    "\n",
    "    ax.set_xlim(-1, 25)\n",
    "    ax.set_ylim(-3, 4)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Generator Architecture\", fontsize=16, fontweight=\"bold\", pad=20)\n",
    "    plt.show()\n",
    "\n",
    "visualize_generator_architecture()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:19.917455Z",
     "iopub.status.busy": "2025-01-23T20:14:19.917008Z",
     "iopub.status.idle": "2025-01-23T20:14:20.150061Z",
     "shell.execute_reply": "2025-01-23T20:14:20.148707Z",
     "shell.execute_reply.started": "2025-01-23T20:14:19.917411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, csi_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, audio, csi):\n",
    "        csi_flat = csi.view(csi.size(0), -1)\n",
    "        x = torch.cat([audio, csi_flat], dim=1).unsqueeze(1)  # [batch, 1, L]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x).squeeze(-1)  # [batch, 256]\n",
    "        return torch.sigmoid(self.fc(x))  # [batch, 1]\n",
    "\n",
    "\n",
    "def visualize_discriminator_architecture():\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    layers = [\n",
    "        {\"name\": \"Input \\n(Audio + CSI)\", \"desc\": \"Concatenates audio \\nand flattened CSI\", \"pos\": (0, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"Conv1D\\n(164)\", \"desc\": \"Kernel=4, Stride=2\\nOutput: 64200\", \"pos\": (4, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"Conv1D\\n(64128)\", \"desc\": \"Kernel=4, Stride=2\\nOutput: 128100\", \"pos\": (8, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"Conv1D\\n(128256)\", \"desc\": \"Kernel=4, Stride=2\\nOutput: 25650\", \"pos\": (12, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"AdaptiveAvgPool1D\", \"desc\": \"Global pooling to\\nsingle value\", \"pos\": (16, 0), \"size\": (3, 2)},\n",
    "        {\"name\": \"Linear\\n(2561)\", \"desc\": \"Final authenticity\\nscore\", \"pos\": (20, 0), \"size\": (3, 2)}\n",
    "    ]\n",
    "    \n",
    "    data_shapes = [\n",
    "        \"(Batch x (400 + \\ncsi_channels*400))\",\n",
    "        \"(Batch x 64 x 200)\",\n",
    "        \"(Batch x 128 x 100)\",\n",
    "        \"(Batch x 256 x 50)\",\n",
    "        \"(Batch x 256 x 1)\",\n",
    "        \"(Batch x 1)\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        rect = Rectangle(layer[\"pos\"], *layer[\"size\"], edgecolor=\"black\", facecolor=\"lightblue\", linewidth=2.5)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(\n",
    "            layer[\"pos\"][0] + layer[\"size\"][0] / 2,\n",
    "            layer[\"pos\"][1] + layer[\"size\"][1] / 2 + 0.2,\n",
    "            layer[\"name\"],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\"\n",
    "        )\n",
    "        \n",
    "        ax.text(\n",
    "            layer[\"pos\"][0] + layer[\"size\"][0] / 2,\n",
    "            layer[\"pos\"][1] - 0.8,\n",
    "            layer[\"desc\"],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=10,\n",
    "            color=\"darkgray\"\n",
    "        )\n",
    "        \n",
    "        ax.text(\n",
    "            layer[\"pos\"][0] + layer[\"size\"][0] / 2,\n",
    "            layer[\"pos\"][1] + layer[\"size\"][1] + 0.6,\n",
    "            data_shapes[i],\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=10,\n",
    "            color=\"darkblue\"\n",
    "        )\n",
    "        \n",
    "        if i < len(layers) - 1:\n",
    "            start_x = layer[\"pos\"][0] + layer[\"size\"][0]\n",
    "            start_y = layer[\"pos\"][1] + layer[\"size\"][1] / 2\n",
    "            end_x = layers[i + 1][\"pos\"][0]\n",
    "            end_y = layers[i + 1][\"pos\"][1] + layers[i + 1][\"size\"][1] / 2\n",
    "            arrow = FancyArrow(start_x, start_y, end_x - start_x - 0.2, end_y - start_y, width=0.1, color=\"black\")\n",
    "            ax.add_patch(arrow)\n",
    "\n",
    "    ax.set_xlim(-1, 20)\n",
    "    ax.set_ylim(-3, 4)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Discriminator Architecture\", fontsize=16, fontweight=\"bold\", pad=20)\n",
    "    plt.show()\n",
    "\n",
    "visualize_discriminator_architecture()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:20.151808Z",
     "iopub.status.busy": "2025-01-23T20:14:20.151452Z",
     "iopub.status.idle": "2025-01-23T20:14:20.157467Z",
     "shell.execute_reply": "2025-01-23T20:14:20.156071Z",
     "shell.execute_reply.started": "2025-01-23T20:14:20.151766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generator_loss(d_output):\n",
    "    return F.binary_cross_entropy(d_output, torch.ones_like(d_output))\n",
    "\n",
    "def discriminator_loss(d_output_real, d_output_fake):\n",
    "    real_loss = F.binary_cross_entropy(d_output_real, torch.ones_like(d_output_real))\n",
    "    fake_loss = F.binary_cross_entropy(d_output_fake, torch.zeros_like(d_output_fake))\n",
    "    return real_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Models and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:20.160398Z",
     "iopub.status.busy": "2025-01-23T20:14:20.159863Z",
     "iopub.status.idle": "2025-01-23T20:14:23.237593Z",
     "shell.execute_reply": "2025-01-23T20:14:23.236235Z",
     "shell.execute_reply.started": "2025-01-23T20:14:20.160361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim, csi_channels).to(device)\n",
    "discriminator = Discriminator(csi_channels).to(device)\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:23.239447Z",
     "iopub.status.busy": "2025-01-23T20:14:23.238999Z",
     "iopub.status.idle": "2025-01-23T20:14:44.456280Z",
     "shell.execute_reply": "2025-01-23T20:14:44.454882Z",
     "shell.execute_reply.started": "2025-01-23T20:14:23.239419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0it [00:00, ?it/s]\n",
      "Epoch 2: 0it [00:00, ?it/s]\n",
      "Epoch 3: 0it [00:00, ?it/s]\n",
      "Epoch 4: 0it [00:00, ?it/s]\n",
      "Epoch 5: 0it [00:00, ?it/s]\n",
      "Epoch 6: 0it [00:00, ?it/s]\n",
      "Epoch 7: 0it [00:00, ?it/s]\n",
      "Epoch 8: 0it [00:00, ?it/s]\n",
      "Epoch 9: 0it [00:00, ?it/s]\n",
      "Epoch 10: 0it [00:00, ?it/s]\n",
      "Epoch 11: 0it [00:00, ?it/s]\n",
      "Epoch 12: 0it [00:00, ?it/s]\n",
      "Epoch 13: 0it [00:00, ?it/s]\n",
      "Epoch 14: 0it [00:00, ?it/s]\n",
      "Epoch 15: 0it [00:00, ?it/s]\n",
      "Epoch 16: 0it [00:00, ?it/s]\n",
      "Epoch 17: 0it [00:00, ?it/s]\n",
      "Epoch 18: 0it [00:00, ?it/s]\n",
      "Epoch 19: 0it [00:00, ?it/s]\n",
      "Epoch 20: 0it [00:00, ?it/s]\n",
      "Epoch 21: 0it [00:00, ?it/s]\n",
      "Epoch 22: 0it [00:00, ?it/s]\n",
      "Epoch 23: 0it [00:00, ?it/s]\n",
      "Epoch 24: 0it [00:00, ?it/s]\n",
      "Epoch 25: 0it [00:00, ?it/s]\n",
      "Epoch 26: 0it [00:00, ?it/s]\n",
      "Epoch 27: 0it [00:00, ?it/s]\n",
      "Epoch 28: 0it [00:00, ?it/s]\n",
      "Epoch 29: 0it [00:00, ?it/s]\n",
      "Epoch 30: 0it [00:00, ?it/s]\n",
      "Epoch 31: 0it [00:00, ?it/s]\n",
      "Epoch 32: 0it [00:00, ?it/s]\n",
      "Epoch 33: 0it [00:00, ?it/s]\n",
      "Epoch 34: 0it [00:00, ?it/s]\n",
      "Epoch 35: 0it [00:00, ?it/s]\n",
      "Epoch 36: 0it [00:00, ?it/s]\n",
      "Epoch 37: 0it [00:00, ?it/s]\n",
      "Epoch 38: 0it [00:00, ?it/s]\n",
      "Epoch 39: 0it [00:00, ?it/s]\n",
      "Epoch 40: 0it [00:00, ?it/s]\n",
      "Epoch 41: 0it [00:00, ?it/s]\n",
      "Epoch 42: 0it [00:00, ?it/s]\n",
      "Epoch 43: 0it [00:00, ?it/s]\n",
      "Epoch 44: 0it [00:00, ?it/s]\n",
      "Epoch 45: 0it [00:00, ?it/s]\n",
      "Epoch 46: 0it [00:00, ?it/s]\n",
      "Epoch 47: 0it [00:00, ?it/s]\n",
      "Epoch 48: 0it [00:00, ?it/s]\n",
      "Epoch 49: 0it [00:00, ?it/s]\n",
      "Epoch 50: 0it [00:00, ?it/s]\n",
      "Epoch 51: 0it [00:00, ?it/s]\n",
      "Epoch 52: 0it [00:00, ?it/s]\n",
      "Epoch 53: 0it [00:00, ?it/s]\n",
      "Epoch 54: 0it [00:00, ?it/s]\n",
      "Epoch 55: 0it [00:00, ?it/s]\n",
      "Epoch 56: 0it [00:00, ?it/s]\n",
      "Epoch 57: 0it [00:00, ?it/s]\n",
      "Epoch 58: 0it [00:00, ?it/s]\n",
      "Epoch 59: 0it [00:00, ?it/s]\n",
      "Epoch 60: 0it [00:00, ?it/s]\n",
      "Epoch 61: 0it [00:00, ?it/s]\n",
      "Epoch 62: 0it [00:00, ?it/s]\n",
      "Epoch 63: 0it [00:00, ?it/s]\n",
      "Epoch 64: 0it [00:00, ?it/s]\n",
      "Epoch 65: 0it [00:00, ?it/s]\n",
      "Epoch 66: 0it [00:00, ?it/s]\n",
      "Epoch 67: 0it [00:00, ?it/s]\n",
      "Epoch 68: 0it [00:00, ?it/s]\n",
      "Epoch 69: 0it [00:00, ?it/s]\n",
      "Epoch 70: 0it [00:00, ?it/s]\n",
      "Epoch 71: 0it [00:00, ?it/s]\n",
      "Epoch 72: 0it [00:00, ?it/s]\n",
      "Epoch 73: 0it [00:00, ?it/s]\n",
      "Epoch 74: 0it [00:00, ?it/s]\n",
      "Epoch 75: 0it [00:00, ?it/s]\n",
      "Epoch 76: 0it [00:00, ?it/s]\n",
      "Epoch 77: 0it [00:00, ?it/s]\n",
      "Epoch 78: 0it [00:00, ?it/s]\n",
      "Epoch 79: 0it [00:00, ?it/s]\n",
      "Epoch 80: 0it [00:00, ?it/s]\n",
      "Epoch 81: 0it [00:00, ?it/s]\n",
      "Epoch 82: 0it [00:00, ?it/s]\n",
      "Epoch 83: 0it [00:00, ?it/s]\n",
      "Epoch 84: 0it [00:00, ?it/s]\n",
      "Epoch 85: 0it [00:00, ?it/s]\n",
      "Epoch 86: 0it [00:00, ?it/s]\n",
      "Epoch 87: 0it [00:00, ?it/s]\n",
      "Epoch 88: 0it [00:00, ?it/s]\n",
      "Epoch 89: 0it [00:00, ?it/s]\n",
      "Epoch 90: 0it [00:00, ?it/s]\n",
      "Epoch 91: 0it [00:00, ?it/s]\n",
      "Epoch 92: 0it [00:00, ?it/s]\n",
      "Epoch 93: 0it [00:00, ?it/s]\n",
      "Epoch 94: 0it [00:00, ?it/s]\n",
      "Epoch 95: 0it [00:00, ?it/s]\n",
      "Epoch 96: 0it [00:00, ?it/s]\n",
      "Epoch 97: 0it [00:00, ?it/s]\n",
      "Epoch 98: 0it [00:00, ?it/s]\n",
      "Epoch 99: 0it [00:00, ?it/s]\n",
      "Epoch 100: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "class WindowedTrainer:\n",
    "    def __init__(self, generator, discriminator, dataset, hyperparameters):\n",
    "        \"\"\"\n",
    "        Initialize the windowed trainer.\n",
    "        \n",
    "        Args:\n",
    "            generator: Generator model\n",
    "            discriminator: Discriminator model\n",
    "            dataset: CSIAudioDataset instance\n",
    "            hyperparameters: Dictionary containing training hyperparameters\n",
    "        \"\"\"\n",
    "        self.generator = generator.to(hyperparameters['device'])\n",
    "        self.discriminator = discriminator.to(hyperparameters['device'])\n",
    "        self.dataset = dataset\n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "        self.optimizer_g = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=hyperparameters['learning_rate'],\n",
    "            betas=(hyperparameters['beta1'], hyperparameters['beta2'])\n",
    "        )\n",
    "        self.optimizer_d = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=hyperparameters['learning_rate'],\n",
    "            betas=(hyperparameters['beta1'], hyperparameters['beta2'])\n",
    "        )\n",
    "        \n",
    "        # Initialize data loader with collate function to handle None values\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=hyperparameters['batch_size'],\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "        \n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        if len(batch) == 0:\n",
    "            return None\n",
    "        \n",
    "        csi_data = [item[0] for item in batch]\n",
    "        audio_data = [item[1] for item in batch]\n",
    "        \n",
    "        csi_batch = torch.stack(csi_data)\n",
    "        audio_batch = torch.stack(audio_data)\n",
    "        \n",
    "        return csi_batch, audio_batch\n",
    "    \n",
    "    def create_windows(self, data, window_size, stride):\n",
    "        \"\"\"\n",
    "        Create windows from a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            data: Tensor of shape (batch_size, channels/features, sequence_length)\n",
    "            window_size: Number of time steps in each window\n",
    "            stride: Number of time steps to move between windows\n",
    "            \n",
    "        Returns:\n",
    "            List of windows, each of shape (batch_size, channels/features, window_size)\n",
    "        \"\"\"\n",
    "        windows = []\n",
    "        # Handle the case where data is 2D (batch_size, sequence_length)\n",
    "        if len(data.shape) == 2:\n",
    "            data = data.unsqueeze(1)  # Add channel dimension\n",
    "            \n",
    "        for i in range(0, data.shape[2] - window_size + 1, stride):\n",
    "            window = data[:, :, i:i + window_size]\n",
    "            windows.append(window)\n",
    "        return windows\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "\n",
    "        running_d_loss = 0.0\n",
    "        running_g_loss = 0.0\n",
    "        total_windows = 0\n",
    "        \n",
    "        window_size = 64 \n",
    "        stride = 32  \n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(tqdm(self.train_loader, desc=f\"Epoch {epoch + 1}\")):\n",
    "            if batch_data is None:\n",
    "                continue\n",
    "                \n",
    "            csi_data, audio_data = batch_data\n",
    "            \n",
    "            csi_data = csi_data.to(self.hyperparameters['device'])  # Shape: (batch_size, 8, sequence_length)\n",
    "            audio_data = audio_data.to(self.hyperparameters['device'])  # Shape: (batch_size, sequence_length)\n",
    "            \n",
    "            csi_windows = self.create_windows(csi_data, window_size, stride)\n",
    "            audio_windows = self.create_windows(audio_data, window_size, stride)\n",
    "            \n",
    "            total_windows += len(csi_windows)\n",
    "            \n",
    "            for csi_window, audio_window in zip(csi_windows, audio_windows):\n",
    "                if len(audio_window.shape) == 2: \n",
    "                    audio_window = audio_window.unsqueeze(1) \n",
    "                \n",
    "                self.optimizer_d.zero_grad()\n",
    "                \n",
    "                z = torch.randn(\n",
    "                    csi_window.size(0),\n",
    "                    self.hyperparameters['latent_dim'],\n",
    "                    device=self.hyperparameters['device']\n",
    "                )\n",
    "                fake_audio = self.generator(z, csi_window)\n",
    "                \n",
    "                d_real = self.discriminator(audio_window, csi_window)\n",
    "                d_fake = self.discriminator(fake_audio.detach(), csi_window)\n",
    "                \n",
    "                d_loss_real = -torch.mean(d_real)\n",
    "                d_loss_fake = torch.mean(d_fake)\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "                \n",
    "                gp = self.gradient_penalty(audio_window, fake_audio.detach(), csi_window)\n",
    "                d_loss += 10.0 * gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                self.optimizer_d.step()\n",
    "                \n",
    "                self.optimizer_g.zero_grad()\n",
    "                \n",
    "                fake_audio = self.generator(z, csi_window)\n",
    "                d_fake = self.discriminator(fake_audio, csi_window)\n",
    "                \n",
    "                g_loss = -torch.mean(d_fake)\n",
    "                \n",
    "                g_loss.backward()\n",
    "                self.optimizer_g.step()\n",
    "                \n",
    "                running_d_loss += d_loss.item()\n",
    "                running_g_loss += g_loss.item()\n",
    "        \n",
    "        avg_d_loss = running_d_loss / total_windows if total_windows > 0 else float('inf')\n",
    "        avg_g_loss = running_g_loss / total_windows if total_windows > 0 else float('inf')\n",
    "        \n",
    "        self.d_losses.append(avg_d_loss)\n",
    "        self.g_losses.append(avg_g_loss)\n",
    "        \n",
    "        return avg_d_loss, avg_g_loss\n",
    "    \n",
    "    def gradient_penalty(self, real_data, fake_data, condition):\n",
    "\n",
    "        batch_size = real_data.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1).to(self.hyperparameters['device'])\n",
    "        alpha = alpha.expand_as(real_data)\n",
    "        \n",
    "        interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "        interpolated.requires_grad_(True)\n",
    "        \n",
    "        d_interpolated = self.discriminator(interpolated, condition)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_interpolated),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_norm = gradients.norm(2, dim=1)\n",
    "        gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "        \n",
    "        return gradient_penalty\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for epoch in range(self.hyperparameters['epochs']):\n",
    "            d_loss, g_loss = self.train_epoch(epoch)\n",
    "            \n",
    "            logging.info(\n",
    "                f\"Epoch [{epoch + 1}/{self.hyperparameters['epochs']}] \"\n",
    "                f\"D_loss: {d_loss:.4f}, G_loss: {g_loss:.4f}\"\n",
    "            )\n",
    "            \n",
    "        \n",
    "        self.plot_training_progress()\n",
    "    \n",
    "    def plot_training_progress(self):\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.d_losses, label='Discriminator Loss', color='red')\n",
    "        plt.plot(self.g_losses, label='Generator Loss', color='blue')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('training_progress.png')\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    hyperparameters = {\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"csi_channels\": csi_channels,\n",
    "        \"audio_channels\": audio_channels,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"beta1\": beta1,\n",
    "        \"beta2\": beta2,\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"device\": device\n",
    "    }\n",
    "    \n",
    "    dataset = CSIAudioDataset(train_dataset)\n",
    "    \n",
    "    generator = Generator(latent_dim, csi_channels).to(device)\n",
    "    discriminator = Discriminator(csi_channels).to(device)\n",
    "    \n",
    "    trainer = WindowedTrainer(generator, discriminator, dataset, hyperparameters)\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:44.457595Z",
     "iopub.status.busy": "2025-01-23T20:14:44.457309Z",
     "iopub.status.idle": "2025-01-23T20:14:44.579058Z",
     "shell.execute_reply": "2025-01-23T20:14:44.577771Z",
     "shell.execute_reply.started": "2025-01-23T20:14:44.457570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), \"generator.pth\")\n",
    "torch.save(discriminator.state_dict(), \"discriminator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full code in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0it [00:00, ?it/s]\n",
      "Epoch 2: 0it [00:00, ?it/s]\n",
      "Epoch 3: 0it [00:00, ?it/s]\n",
      "Epoch 4: 0it [00:00, ?it/s]\n",
      "Epoch 5: 0it [00:00, ?it/s]\n",
      "Epoch 6: 0it [00:00, ?it/s]\n",
      "Epoch 7: 0it [00:00, ?it/s]\n",
      "Epoch 8: 0it [00:00, ?it/s]\n",
      "Epoch 9: 0it [00:00, ?it/s]\n",
      "Epoch 10: 0it [00:00, ?it/s]\n",
      "Epoch 11: 0it [00:00, ?it/s]\n",
      "Epoch 12: 0it [00:00, ?it/s]\n",
      "Epoch 13: 0it [00:00, ?it/s]\n",
      "Epoch 14: 0it [00:00, ?it/s]\n",
      "Epoch 15: 0it [00:00, ?it/s]\n",
      "Epoch 16: 0it [00:00, ?it/s]\n",
      "Epoch 17: 0it [00:00, ?it/s]\n",
      "Epoch 18: 0it [00:00, ?it/s]\n",
      "Epoch 19: 0it [00:00, ?it/s]\n",
      "Epoch 20: 0it [00:00, ?it/s]\n",
      "Epoch 21: 0it [00:00, ?it/s]\n",
      "Epoch 22: 0it [00:00, ?it/s]\n",
      "Epoch 23: 0it [00:00, ?it/s]\n",
      "Epoch 24: 0it [00:00, ?it/s]\n",
      "Epoch 25: 0it [00:00, ?it/s]\n",
      "Epoch 26: 0it [00:00, ?it/s]\n",
      "Epoch 27: 0it [00:00, ?it/s]\n",
      "Epoch 28: 0it [00:00, ?it/s]\n",
      "Epoch 29: 0it [00:00, ?it/s]\n",
      "Epoch 30: 0it [00:00, ?it/s]\n",
      "Epoch 31: 0it [00:00, ?it/s]\n",
      "Epoch 32: 0it [00:00, ?it/s]\n",
      "Epoch 33: 0it [00:00, ?it/s]\n",
      "Epoch 34: 0it [00:00, ?it/s]\n",
      "Epoch 35: 0it [00:00, ?it/s]\n",
      "Epoch 36: 0it [00:00, ?it/s]\n",
      "Epoch 37: 0it [00:00, ?it/s]\n",
      "Epoch 38: 0it [00:00, ?it/s]\n",
      "Epoch 39: 0it [00:00, ?it/s]\n",
      "Epoch 40: 0it [00:00, ?it/s]\n",
      "Epoch 41: 0it [00:00, ?it/s]\n",
      "Epoch 42: 0it [00:00, ?it/s]\n",
      "Epoch 43: 0it [00:00, ?it/s]\n",
      "Epoch 44: 0it [00:00, ?it/s]\n",
      "Epoch 45: 0it [00:00, ?it/s]\n",
      "Epoch 46: 0it [00:00, ?it/s]\n",
      "Epoch 47: 0it [00:00, ?it/s]\n",
      "Epoch 48: 0it [00:00, ?it/s]\n",
      "Epoch 49: 0it [00:00, ?it/s]\n",
      "Epoch 50: 0it [00:00, ?it/s]\n",
      "Epoch 51: 0it [00:00, ?it/s]\n",
      "Epoch 52: 0it [00:00, ?it/s]\n",
      "Epoch 53: 0it [00:00, ?it/s]\n",
      "Epoch 54: 0it [00:00, ?it/s]\n",
      "Epoch 55: 0it [00:00, ?it/s]\n",
      "Epoch 56: 0it [00:00, ?it/s]\n",
      "Epoch 57: 0it [00:00, ?it/s]\n",
      "Epoch 58: 0it [00:00, ?it/s]\n",
      "Epoch 59: 0it [00:00, ?it/s]\n",
      "Epoch 60: 0it [00:00, ?it/s]\n",
      "Epoch 61: 0it [00:00, ?it/s]\n",
      "Epoch 62: 0it [00:00, ?it/s]\n",
      "Epoch 63: 0it [00:00, ?it/s]\n",
      "Epoch 64: 0it [00:00, ?it/s]\n",
      "Epoch 65: 0it [00:00, ?it/s]\n",
      "Epoch 66: 0it [00:00, ?it/s]\n",
      "Epoch 67: 0it [00:00, ?it/s]\n",
      "Epoch 68: 0it [00:00, ?it/s]\n",
      "Epoch 69: 0it [00:00, ?it/s]\n",
      "Epoch 70: 0it [00:00, ?it/s]\n",
      "Epoch 71: 0it [00:00, ?it/s]\n",
      "Epoch 72: 0it [00:00, ?it/s]\n",
      "Epoch 73: 0it [00:00, ?it/s]\n",
      "Epoch 74: 0it [00:00, ?it/s]\n",
      "Epoch 75: 0it [00:00, ?it/s]\n",
      "Epoch 76: 0it [00:00, ?it/s]\n",
      "Epoch 77: 0it [00:00, ?it/s]\n",
      "Epoch 78: 0it [00:00, ?it/s]\n",
      "Epoch 79: 0it [00:00, ?it/s]\n",
      "Epoch 80: 0it [00:00, ?it/s]\n",
      "Epoch 81: 0it [00:00, ?it/s]\n",
      "Epoch 82: 0it [00:00, ?it/s]\n",
      "Epoch 83: 0it [00:00, ?it/s]\n",
      "Epoch 84: 0it [00:00, ?it/s]\n",
      "Epoch 85: 0it [00:00, ?it/s]\n",
      "Epoch 86: 0it [00:00, ?it/s]\n",
      "Epoch 87: 0it [00:00, ?it/s]\n",
      "Epoch 88: 0it [00:00, ?it/s]\n",
      "Epoch 89: 0it [00:00, ?it/s]\n",
      "Epoch 90: 0it [00:00, ?it/s]\n",
      "Epoch 91: 0it [00:00, ?it/s]\n",
      "Epoch 92: 0it [00:00, ?it/s]\n",
      "Epoch 93: 0it [00:00, ?it/s]\n",
      "Epoch 94: 0it [00:00, ?it/s]\n",
      "Epoch 95: 0it [00:00, ?it/s]\n",
      "Epoch 96: 0it [00:00, ?it/s]\n",
      "Epoch 97: 0it [00:00, ?it/s]\n",
      "Epoch 98: 0it [00:00, ?it/s]\n",
      "Epoch 99: 0it [00:00, ?it/s]\n",
      "Epoch 100: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrow, Rectangle\n",
    "import logging\n",
    "\n",
    "latent_dim = 128\n",
    "csi_channels = 8 \n",
    "audio_channels = 1\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "sample_rate = 16000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CSIAudioDataset(Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        logging.debug(f\"Processing index: {idx}\")\n",
    "        csi_path = self.data_df.iloc[idx]['csi_path']\n",
    "        csi_data = pd.read_csv(csi_path)\n",
    "        subcarrier_cols = [col for col in csi_data.columns if col.startswith('subcarrier_')]\n",
    "        csi_data = csi_data[subcarrier_cols].values.T  # Shape: (8, N)\n",
    "        target_len = 400\n",
    "        csi_data_resampled = np.array([\n",
    "            np.interp(\n",
    "                np.linspace(0, len(channel) - 1, target_len),\n",
    "                np.arange(len(channel)),\n",
    "                channel\n",
    "            ) for channel in csi_data\n",
    "        ])\n",
    "        csi_tensor = torch.tensor(csi_data_resampled, dtype=torch.float32)\n",
    "\n",
    "        audio_path = self.data_df.iloc[idx]['audio_path']\n",
    "        audio_data = pd.read_csv(audio_path, header=None).values.flatten()\n",
    "        audio_data_resampled = np.interp(\n",
    "            np.linspace(0, len(audio_data) - 1, target_len),\n",
    "            np.arange(len(audio_data)),\n",
    "            audio_data\n",
    "        )\n",
    "        audio_tensor = torch.tensor(audio_data_resampled, dtype=torch.float32)\n",
    "\n",
    "        min_len = min(csi_tensor.shape[1], len(audio_tensor))\n",
    "        if min_len == 0:\n",
    "            logging.warning(f\"No valid audio length for index: {idx}\")\n",
    "            return None\n",
    "\n",
    "        csi_tensor_trimmed = csi_tensor[:, :min_len]\n",
    "        audio_trimmed = audio_tensor[:min_len]\n",
    "        return csi_tensor_trimmed, audio_trimmed\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, csi_channels, audio_channels=1, csi_length=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.csi_length = csi_length  # Store csi_length as an attribute\n",
    "        input_dim = latent_dim + csi_channels * csi_length\n",
    "        T = csi_length // 4  # Adjusted for two ConvTranspose1d with stride=2\n",
    "        self.fc = nn.Linear(input_dim, 256 * T)\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, audio_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, z, csi_data):\n",
    "        csi_flat = csi_data.view(csi_data.size(0), -1)\n",
    "        x = torch.cat([z, csi_flat], dim=1)\n",
    "        T = self.csi_length // 4\n",
    "        x = self.fc(x).view(-1, 256, T)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return torch.sigmoid(x.view(-1, self.csi_length))  # Dynamic reshaping\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, csi_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, audio, csi):\n",
    "        csi_flat = csi.view(csi.size(0), -1)\n",
    "        x = torch.cat([audio, csi_flat], dim=1).unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "class WindowedTrainer:\n",
    "    def __init__(self, generator, discriminator, dataset, hyperparameters):\n",
    "        self.generator = generator.to(hyperparameters['device'])\n",
    "        self.discriminator = discriminator.to(hyperparameters['device'])\n",
    "        self.dataset = dataset\n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "        self.optimizer_g = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=hyperparameters['learning_rate'],\n",
    "            betas=(hyperparameters['beta1'], hyperparameters['beta2'])\n",
    "        )\n",
    "        self.optimizer_d = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=hyperparameters['learning_rate'],\n",
    "            betas=(hyperparameters['beta1'], hyperparameters['beta2'])\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=hyperparameters['batch_size'],\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "        \n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        if len(batch) == 0:\n",
    "            return None\n",
    "        csi_data = [item[0] for item in batch]\n",
    "        audio_data = [item[1] for item in batch]\n",
    "        csi_batch = torch.stack(csi_data)\n",
    "        audio_batch = torch.stack(audio_data)\n",
    "        return csi_batch, audio_batch\n",
    "    \n",
    "    def create_windows(self, data, window_size, stride):\n",
    "        windows = []\n",
    "        if len(data.shape) == 2:\n",
    "            data = data.unsqueeze(1)\n",
    "        for i in range(0, data.shape[2] - window_size + 1, stride):\n",
    "            window = data[:, :, i:i + window_size]\n",
    "            windows.append(window)\n",
    "        return windows\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        running_d_loss = 0.0\n",
    "        running_g_loss = 0.0\n",
    "        total_windows = 0\n",
    "        window_size = 64\n",
    "        stride = 32\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(tqdm(self.train_loader, desc=f\"Epoch {epoch + 1}\")):\n",
    "            if batch_data is None:\n",
    "                continue\n",
    "            csi_data, audio_data = batch_data\n",
    "            csi_data = csi_data.to(self.hyperparameters['device'])\n",
    "            audio_data = audio_data.to(self.hyperparameters['device'])\n",
    "            csi_windows = self.create_windows(csi_data, window_size, stride)\n",
    "            audio_windows = self.create_windows(audio_data, window_size, stride)\n",
    "            total_windows += len(csi_windows)\n",
    "            for csi_window, audio_window in zip(csi_windows, audio_windows):\n",
    "                if len(audio_window.shape) == 2:\n",
    "                    audio_window = audio_window.unsqueeze(1)\n",
    "                self.optimizer_d.zero_grad()\n",
    "                z = torch.randn(\n",
    "                    csi_window.size(0),\n",
    "                    self.hyperparameters['latent_dim'],\n",
    "                    device=self.hyperparameters['device']\n",
    "                )\n",
    "                fake_audio = self.generator(z, csi_window)\n",
    "                d_real = self.discriminator(audio_window.squeeze(1), csi_window)\n",
    "                d_fake = self.discriminator(fake_audio.detach(), csi_window)\n",
    "                d_loss_real = -torch.mean(d_real)\n",
    "                d_loss_fake = torch.mean(d_fake)\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "                gp = self.gradient_penalty(audio_window.squeeze(1), fake_audio.detach(), csi_window)\n",
    "                d_loss += 10.0 * gp\n",
    "                d_loss.backward()\n",
    "                self.optimizer_d.step()\n",
    "                self.optimizer_g.zero_grad()\n",
    "                fake_audio = self.generator(z, csi_window)\n",
    "                d_fake = self.discriminator(fake_audio, csi_window)\n",
    "                g_loss = -torch.mean(d_fake)\n",
    "                g_loss.backward()\n",
    "                self.optimizer_g.step()\n",
    "                running_d_loss += d_loss.item()\n",
    "                running_g_loss += g_loss.item()\n",
    "        avg_d_loss = running_d_loss / total_windows if total_windows > 0 else float('inf')\n",
    "        avg_g_loss = running_g_loss / total_windows if total_windows > 0 else float('inf')\n",
    "        self.d_losses.append(avg_d_loss)\n",
    "        self.g_losses.append(avg_g_loss)\n",
    "        return avg_d_loss, avg_g_loss\n",
    "    \n",
    "    def gradient_penalty(self, real_data, fake_data, condition):\n",
    "        batch_size = real_data.size(0)\n",
    "        alpha = torch.rand(batch_size, 1).to(self.hyperparameters['device'])\n",
    "        alpha = alpha.expand_as(real_data)\n",
    "        interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "        interpolated.requires_grad_(True)\n",
    "        d_interpolated = self.discriminator(interpolated, condition)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_interpolated),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_norm = gradients.norm(2, dim=1)\n",
    "        gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.hyperparameters['epochs']):\n",
    "            d_loss, g_loss = self.train_epoch(epoch)\n",
    "            logging.info(\n",
    "                f\"Epoch [{epoch + 1}/{self.hyperparameters['epochs']}] \"\n",
    "                f\"D_loss: {d_loss:.4f}, G_loss: {g_loss:.4f}\"\n",
    "            )\n",
    "        self.plot_training_progress()\n",
    "    \n",
    "    def plot_training_progress(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.d_losses, label='Discriminator Loss', color='red')\n",
    "        plt.plot(self.g_losses, label='Generator Loss', color='blue')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('training_progress.png')\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    hyperparameters = {\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"csi_channels\": csi_channels,\n",
    "        \"audio_channels\": audio_channels,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"beta1\": beta1,\n",
    "        \"beta2\": beta2,\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"device\": device\n",
    "    }\n",
    "    train_df = pd.read_csv(\"data_paths.csv\")  # Ensure this file exists\n",
    "    dataset = CSIAudioDataset(train_df)\n",
    "    generator = Generator(latent_dim, csi_channels, csi_length=64).to(device)\n",
    "    discriminator = Discriminator(csi_channels).to(device)\n",
    "    trainer = WindowedTrainer(generator, discriminator, dataset, hyperparameters)\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T20:14:44.581154Z",
     "iopub.status.busy": "2025-01-23T20:14:44.580691Z",
     "iopub.status.idle": "2025-01-23T20:14:45.019655Z",
     "shell.execute_reply": "2025-01-23T20:14:45.018367Z",
     "shell.execute_reply.started": "2025-01-23T20:14:44.581113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([25600, 528]) from checkpoint, the shape in current model is torch.Size([25600, 3328]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Initialize and load generator\u001b[39;00m\n\u001b[32m     37\u001b[39m generator = Generator(latent_dim=latent_dim, csi_channels=csi_channels)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgenerator.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m              \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m              \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m generator.to(device)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Process CSI data (top 8 subcarriers)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for Generator:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([25600, 528]) from checkpoint, the shape in current model is torch.Size([25600, 3328])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generator definition (MUST MATCH TRAINED ARCHITECTURE)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=128, csi_channels=8, audio_channels=1, csi_length=400):\n",
    "        super(Generator, self).__init__()\n",
    "        input_dim = latent_dim + csi_channels * csi_length\n",
    "        self.fc = nn.Linear(input_dim, 256 * 100)\n",
    "        self.conv1 = nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, audio_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, z, csi_data):\n",
    "        csi_flat = csi_data.view(csi_data.size(0), -1)\n",
    "        x = torch.cat([z, csi_flat], dim=1)\n",
    "        x = self.fc(x).view(-1, 256, 100)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return torch.sigmoid(x.view(-1, 400))\n",
    "\n",
    "# Model parameters (MUST MATCH TRAINING PARAMETERS)\n",
    "latent_dim = 128\n",
    "csi_channels = 8\n",
    "csi_length = 400\n",
    "\n",
    "generator = Generator(latent_dim=latent_dim, csi_channels=csi_channels)\n",
    "generator.load_state_dict(\n",
    "    torch.load('generator.pth', \n",
    "              map_location=device, \n",
    "              weights_only=True)\n",
    ")\n",
    "generator.to(device)\n",
    "\n",
    "csi_data_path = 'data_capture/csi/8_csi_filtered/top8_filtered_csi_data_2024-08-17_23-16-37.224.csv'  # <-- update filename as needed\n",
    "raw_csi_df = pd.read_csv(csi_data_path)\n",
    "subcarrier_cols = [col for col in raw_csi_df.columns if col.startswith('subcarrier_')]\n",
    "raw_csi = raw_csi_df[subcarrier_cols].values.T  # shape: (8, N)\n",
    "\n",
    "# Interpolate each subcarrier to 400 points\n",
    "target_len = 400\n",
    "processed_csi = np.array([\n",
    "    np.interp(\n",
    "        np.linspace(0, len(channel) - 1, target_len),\n",
    "        np.arange(len(channel)),\n",
    "        channel\n",
    "    ) for channel in raw_csi\n",
    "])  # shape: (8, 400)\n",
    "\n",
    "# Create CSI tensor\n",
    "sample_csi = torch.tensor(processed_csi, dtype=torch.float32)\n",
    "sample_csi = sample_csi.view(1, csi_channels, csi_length).to(device)\n",
    "\n",
    "# Verify dimensions\n",
    "if sample_csi.shape != (1, csi_channels, csi_length):\n",
    "    raise ValueError(f\"CSI tensor shape mismatch: {sample_csi.shape}\")\n",
    "\n",
    "def generate_synthetic_audio(generator, latent_dim, csi_data_sample, num_samples, \n",
    "                            output_dir, target_length=1600, sample_rate=16000):\n",
    "    generator.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        z = torch.randn(1, latent_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_audio = generator(z, csi_data_sample)\n",
    "        \n",
    "        audio_400 = generated_audio.squeeze().cpu().numpy()\n",
    "        x_original = np.linspace(0, 1, len(audio_400))\n",
    "        x_target = np.linspace(0, 1, target_length)\n",
    "        audio_interpolated = np.interp(x_target, x_original, audio_400)\n",
    "        audio_interpolated = np.clip(audio_interpolated * 2 - 1, -1.0, 1.0)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"generated_audio_{i+1}_{target_length}pts.wav\")\n",
    "        sf.write(output_path, audio_interpolated, sample_rate)\n",
    "        print(f\"Generated {target_length}-point audio at: {output_path}\")\n",
    "\n",
    "generate_synthetic_audio(\n",
    "    generator=generator,\n",
    "    latent_dim=latent_dim,\n",
    "    csi_data_sample=sample_csi,\n",
    "    num_samples=2,\n",
    "    output_dir=\"generated_audio\",\n",
    "    target_length=32000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Code - 30 july\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 11:56:46,353 - INFO - ================================================================================\n",
      "STARTING ENHANCED CSI-AUDIO GAN TRAINING\n",
      "================================================================================\n",
      "2025-09-16 11:56:46,363 - INFO - Found 75 CSI files and 70 audio files.\n",
      "2025-09-16 11:56:46,365 - INFO - Found 68 matching file pairs.\n",
      "2025-09-16 11:56:46,369 - INFO - Data split: 57 train, 11 test samples.\n",
      "2025-09-16 11:56:46,370 - INFO - CONFIGURATION:\n",
      "2025-09-16 11:56:46,371 - INFO -   latent_dim: 64\n",
      "2025-09-16 11:56:46,372 - INFO -   csi_channels: 8\n",
      "2025-09-16 11:56:46,372 - INFO -   epochs: 300\n",
      "2025-09-16 11:56:46,373 - INFO -   batch_size: 8\n",
      "2025-09-16 11:56:46,374 - INFO -   lr_generator: 0.0001\n",
      "2025-09-16 11:56:46,375 - INFO -   lr_discriminator: 5e-05\n",
      "2025-09-16 11:56:46,376 - INFO -   beta1: 0.5\n",
      "2025-09-16 11:56:46,377 - INFO -   beta2: 0.999\n",
      "2025-09-16 11:56:46,378 - INFO -   sample_rate: 44100\n",
      "2025-09-16 11:56:46,379 - INFO -   audio_duration: 10.0\n",
      "2025-09-16 11:56:46,379 - INFO -   sequence_length: 441000\n",
      "2025-09-16 11:56:46,380 - INFO -   chunk_size: 2048\n",
      "2025-09-16 11:56:46,381 - INFO -   lambda_reconstruction: 15.0\n",
      "2025-09-16 11:56:46,381 - INFO -   lambda_feature_matching: 3.0\n",
      "2025-09-16 11:56:46,382 - INFO -   lambda_gradient_penalty: 8.0\n",
      "2025-09-16 11:56:46,383 - INFO -   lambda_spectral: 2.0\n",
      "2025-09-16 11:56:46,384 - INFO -   lambda_drum_loss: 5.0\n",
      "2025-09-16 11:56:46,385 - INFO -   n_critic: 3\n",
      "2025-09-16 11:56:46,385 - INFO -   warmup_epochs: 15\n",
      "2025-09-16 11:56:46,386 - INFO -   train_test_split: 0.85\n",
      "2025-09-16 11:56:46,387 - INFO -   random_seed: 42\n",
      "2025-09-16 11:56:46,387 - INFO -   device: cuda\n",
      "2025-09-16 11:56:46,394 - INFO - Enhanced CSI preprocessor initialized.\n",
      "2025-09-16 11:56:46,396 - INFO - Dataset created with 57 samples.\n",
      "2025-09-16 11:56:46,432 - INFO - Generator params: 1,302,561\n",
      "2025-09-16 11:56:46,434 - INFO - Discriminator params: 607,905\n",
      "2025-09-16 11:56:46,435 - INFO - Starting training...\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\3804841665.py:274: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
      "Epoch 1/300:   0%|          | 0/7 [00:05<?, ?it/s]\n",
      "2025-09-16 11:56:52,414 - ERROR - Training pipeline failed: DataLoader worker (pid(s) 13792, 20840, 23540, 24884) exited unexpectedly\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1285, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Srach\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py\", line 114, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\3804841665.py\", line 440, in main\n",
      "    history = train_enhanced_gan(generator, discriminator, train_dataloader, HYPERPARAMETERS)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\3804841665.py\", line 283, in train_enhanced_gan\n",
      "    for csi_data, audio_data in progress_bar:\n",
      "                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\tqdm\\std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "               ^^^^^^^^\n",
      "  File \"c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 734, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1492, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1454, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1298, in _try_get_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: DataLoader worker (pid(s) 13792, 20840, 23540, 24884) exited unexpectedly\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 13792, 20840, 23540, 24884) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 459\u001b[39m\n\u001b[32m    456\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 440\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    437\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDiscriminator params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mdiscriminator.parameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m history = \u001b[43mtrain_enhanced_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHYPERPARAMETERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mTraining finished. Saving models and history...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    443\u001b[39m torch.save(generator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \u001b[33m\"\u001b[39m\u001b[33menhanced_generator.pth\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 283\u001b[39m, in \u001b[36mtrain_enhanced_gan\u001b[39m\u001b[34m(generator, discriminator, train_dataloader, hyperparams)\u001b[39m\n\u001b[32m    280\u001b[39m generator.train(); discriminator.train()\n\u001b[32m    281\u001b[39m progress_bar = tqdm(train_dataloader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparams[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcsi_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcsi_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsi_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsi_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1492\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1454\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1452\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1455\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1456\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1298\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1297\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1299\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1300\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 13792, 20840, 23540, 24884) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import spectral_norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import signal\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CSI_DIRECTORY = \"data/csi\"\n",
    "AUDIO_DIRECTORY = \"data/audio\"\n",
    "OUTPUT_DIRECTORY = \"output\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "HYPERPARAMETERS = {\n",
    "    \"latent_dim\": 64,\n",
    "    \"csi_channels\": 8,\n",
    "    \"epochs\": 300,\n",
    "    \"batch_size\": 8,\n",
    "    \"lr_generator\": 0.0001,\n",
    "    \"lr_discriminator\": 0.00005,\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"sample_rate\": None, # Auto-detected\n",
    "    \"audio_duration\": 10.0,\n",
    "    \"sequence_length\": None, # Auto-calculated\n",
    "    \"chunk_size\": 2048,\n",
    "    \"lambda_reconstruction\": 15.0,\n",
    "    \"lambda_feature_matching\": 3.0,\n",
    "    \"lambda_gradient_penalty\": 8.0,\n",
    "    \"lambda_spectral\": 2.0,\n",
    "    \"lambda_drum_loss\": 5.0,\n",
    "    \"n_critic\": 3,\n",
    "    \"warmup_epochs\": 15,\n",
    "    \"train_test_split\": 0.85,\n",
    "    \"random_seed\": 42,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPROCESSING CLASSES (CSI & Audio)\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedCSIPreprocessor:\n",
    "    \"\"\"Enhanced CSI preprocessor with filtering and feature extraction.\"\"\"\n",
    "    def __init__(self, sample_rate, csi_sample_rate=1000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.csi_sample_rate = csi_sample_rate\n",
    "        self.nyquist = csi_sample_rate / 2.0\n",
    "        self._init_butterworth_filter()\n",
    "        logging.info(\"Enhanced CSI preprocessor initialized.\")\n",
    "\n",
    "    def _init_butterworth_filter(self):\n",
    "        try:\n",
    "            cutoff_freq = 50  # Hz\n",
    "            order = 4\n",
    "            self.butterworth_sos = signal.butter(order, cutoff_freq, btype='low', fs=self.csi_sample_rate, output='sos')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Butterworth filter init failed: {e}\")\n",
    "            self.butterworth_sos = None\n",
    "\n",
    "    def apply_butterworth_filter(self, csi_signal):\n",
    "        if self.butterworth_sos is not None and len(csi_signal) > 20:\n",
    "            return signal.sosfilt(self.butterworth_sos, csi_signal).astype(np.float32)\n",
    "        return csi_signal\n",
    "\n",
    "    def extract_motion_features(self, csi_windows):\n",
    "        try:\n",
    "            features_list = []\n",
    "            for channel_idx in range(csi_windows.shape[0]):\n",
    "                channel_data = csi_windows[channel_idx]\n",
    "                # Extract 5 features: energy, phase diff, amplitude change, amp variance, phase variance\n",
    "                energy = np.abs(channel_data) ** 2\n",
    "                phase = np.unwrap(np.angle(signal.hilbert(channel_data)))\n",
    "                phase_diff = np.diff(phase, prepend=phase[0])\n",
    "                amplitude = np.abs(channel_data)\n",
    "                amp_change = np.diff(amplitude, prepend=amplitude[0])\n",
    "                amp_var = pd.Series(amplitude).rolling(32, center=True, min_periods=1).var().bfill().ffill().to_numpy()\n",
    "                phase_var = pd.Series(phase).rolling(32, center=True, min_periods=1).var().bfill().ffill().to_numpy()\n",
    "                features_list.append(np.stack([energy, phase_diff, amp_change, amp_var, phase_var], axis=0))\n",
    "            return np.concatenate(features_list, axis=0).astype(np.float32)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Motion feature extraction failed: {e}\")\n",
    "            return np.zeros((csi_windows.shape[0] * 5, csi_windows.shape[1]), dtype=np.float32)\n",
    "\n",
    "    def process_csi_complete(self, raw_csi_data):\n",
    "        filtered_csi = np.array([self.apply_butterworth_filter(ch) for ch in raw_csi_data])\n",
    "        return self.extract_motion_features(filtered_csi)\n",
    "\n",
    "class EnhancedAudioPreprocessor:\n",
    "    def __init__(self, sample_rate):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.csi_preprocessor = EnhancedCSIPreprocessor(sample_rate)\n",
    "\n",
    "    def process_audio_with_csi(self, audio, raw_csi_data):\n",
    "        processed_csi = self.csi_preprocessor.process_csi_complete(raw_csi_data)\n",
    "        if np.max(np.abs(audio)) > 0:\n",
    "            audio = audio / np.max(np.abs(audio)) * 0.95\n",
    "        return audio.astype(np.float32), processed_csi\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedCSIAudioDataset(Dataset):\n",
    "    def __init__(self, data_df, is_training=True):\n",
    "        self.data_df = data_df.reset_index(drop=True)\n",
    "        self.is_training = is_training\n",
    "        self.sample_rate = HYPERPARAMETERS['sample_rate']\n",
    "        self.chunk_size = HYPERPARAMETERS['chunk_size']\n",
    "        self.audio_preprocessor = EnhancedAudioPreprocessor(self.sample_rate)\n",
    "        logging.info(f\"Dataset created with {len(self.data_df)} samples.\")\n",
    "\n",
    "    def load_and_align_data(self, csi_path, audio_path):\n",
    "        try:\n",
    "            audio_data, file_sr = sf.read(audio_path, dtype='float32')\n",
    "            if file_sr != self.sample_rate:\n",
    "                audio_data = librosa.resample(audio_data, orig_sr=file_sr, target_sr=self.sample_rate)\n",
    "            if audio_data.ndim > 1:\n",
    "                audio_data = audio_data.mean(axis=1)\n",
    "\n",
    "            csi_df = pd.read_csv(csi_path)\n",
    "            sub_cols = [c for c in csi_df.columns if c.startswith('subcarrier_')]\n",
    "            csi_values = csi_df[sub_cols].values.T[:HYPERPARAMETERS['csi_channels']]\n",
    "\n",
    "            if csi_values.shape[1] == 0: return None, None\n",
    "\n",
    "            csi_aligned = librosa.resample(csi_values.astype(float), orig_sr=csi_values.shape[1], target_sr=len(audio_data))\n",
    "            return audio_data, csi_aligned\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load or align {os.path.basename(audio_path)}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_df.iloc[idx]\n",
    "        audio_data, csi_data = self.load_and_align_data(row['csi_path'], row['audio_path'])\n",
    "\n",
    "        if audio_data is None or len(audio_data) < self.chunk_size:\n",
    "            return None # Will be filtered by collate_fn\n",
    "\n",
    "        max_start = len(audio_data) - self.chunk_size\n",
    "        start_idx = random.randint(0, max_start) if self.is_training else 0\n",
    "        end_idx = start_idx + self.chunk_size\n",
    "        \n",
    "        audio_chunk = audio_data[start_idx:end_idx]\n",
    "        csi_chunk = csi_data[:, start_idx:end_idx]\n",
    "\n",
    "        processed_audio, processed_csi = self.audio_preprocessor.process_audio_with_csi(audio_chunk, csi_chunk)\n",
    "        \n",
    "        return torch.from_numpy(processed_csi), torch.from_numpy(processed_audio)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, csi_channels):\n",
    "        super().__init__()\n",
    "        self.chunk_size = HYPERPARAMETERS['chunk_size']\n",
    "        csi_feature_dim = csi_channels * 5 # 5 features per channel\n",
    "        \n",
    "        self.csi_conv = nn.Sequential(\n",
    "            nn.Conv1d(csi_feature_dim, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.latent_fc = nn.Linear(latent_dim, 128 * (self.chunk_size // 16))\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose1d(256, 128, 4, 2, 1), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 64, 4, 2, 1), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(64, 32, 4, 2, 1), nn.BatchNorm1d(32), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, 1, 4, 2, 1), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, csi_data):\n",
    "        csi_feat = self.csi_conv(csi_data)\n",
    "        latent_vec = self.latent_fc(noise).view(-1, 128, self.chunk_size // 16)\n",
    "        \n",
    "        csi_interp = F.interpolate(csi_feat, size=latent_vec.shape[2], mode='nearest')\n",
    "        \n",
    "        combined = torch.cat([latent_vec, csi_interp], dim=1)\n",
    "        return self.model(combined).squeeze(1)\n",
    "\n",
    "class EnhancedDiscriminator(nn.Module):\n",
    "    def __init__(self, csi_channels):\n",
    "        super().__init__()\n",
    "        csi_feature_dim = csi_channels * 5\n",
    "\n",
    "        self.audio_path = nn.Sequential(\n",
    "            spectral_norm(nn.Conv1d(1, 32, 15, 5, 7)), nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv1d(32, 64, 7, 3, 3)), nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.csi_path = nn.Sequential(\n",
    "            spectral_norm(nn.Conv1d(csi_feature_dim, 32, 15, 5, 7)), nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv1d(32, 64, 7, 3, 3)), nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.final_path = nn.Sequential(\n",
    "            spectral_norm(nn.Conv1d(128, 256, 5, 3, 2)), nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv1d(256, 512, 3, 2, 1)), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv1d(512, 1, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio, csi, return_features=False):\n",
    "        audio_feat = self.audio_path(audio.unsqueeze(1))\n",
    "        csi_feat = self.csi_path(csi)\n",
    "        \n",
    "        min_len = min(audio_feat.shape[2], csi_feat.shape[2])\n",
    "        combined = torch.cat([audio_feat[:,:,:min_len], csi_feat[:,:,:min_len]], dim=1)\n",
    "        \n",
    "        out = self.final_path(combined)\n",
    "        return out\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def feature_matching_loss(real_features, fake_features):\n",
    "    return sum(F.l1_loss(f_fake, f_real) for f_real, f_fake in zip(real_features, fake_features))\n",
    "\n",
    "def gradient_penalty(discriminator, real_data, fake_data, csi_data, device):\n",
    "    batch_size = real_data.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, device=device)\n",
    "    interpolated_audio = (alpha * real_data.unsqueeze(1) + (1 - alpha) * fake_data.unsqueeze(1)).requires_grad_(True)\n",
    "    d_interpolated = discriminator(interpolated_audio.squeeze(1), csi_data)\n",
    "    \n",
    "    grad_outputs = torch.ones_like(d_interpolated, requires_grad=False)\n",
    "    gradients = torch.autograd.grad(d_interpolated, interpolated_audio, grad_outputs, create_graph=True, retain_graph=True)[0]\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "def spectral_loss(gen, tar):\n",
    "    gen_fft = torch.fft.rfft(gen, dim=-1)\n",
    "    tar_fft = torch.fft.rfft(tar, dim=-1)\n",
    "    return F.l1_loss(torch.abs(gen_fft), torch.abs(tar_fft))\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_enhanced_gan(generator, discriminator, train_dataloader, hyperparams):\n",
    "    device = hyperparams['device']\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=hyperparams['lr_generator'], betas=(hyperparams['beta1'], hyperparams['beta2']))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=hyperparams['lr_discriminator'], betas=(hyperparams['beta1'], hyperparams['beta2']))\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
    "    \n",
    "    history = {k: [] for k in ['g_loss', 'd_loss', 'g_adv', 'g_recon', 'g_spectral', 'd_real', 'd_fake']}\n",
    "\n",
    "    for epoch in range(hyperparams['epochs']):\n",
    "        epoch_losses = {key: 0.0 for key in history}\n",
    "        generator.train(); discriminator.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{hyperparams['epochs']}\")\n",
    "\n",
    "        for csi_data, audio_data in progress_bar:\n",
    "            csi_data, audio_data = csi_data.to(device), audio_data.to(device)\n",
    "            batch_size = csi_data.size(0)\n",
    "            \n",
    "            optimizer_d.zero_grad(set_to_none=True)\n",
    "            with autocast(enabled=(device.type == 'cuda')):\n",
    "                real_output = discriminator(audio_data, csi_data)\n",
    "                d_loss_real = criterion(real_output, torch.ones_like(real_output) * 0.9) # Label smoothing\n",
    "\n",
    "                noise = torch.randn(batch_size, hyperparams['latent_dim'], device=device)\n",
    "                fake_audio = generator(noise, csi_data)\n",
    "                fake_output = discriminator(fake_audio.detach(), csi_data)\n",
    "                d_loss_fake = criterion(fake_output, torch.zeros_like(fake_output))\n",
    "                \n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "            \n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(optimizer_d)\n",
    "\n",
    "            if len(history['g_loss']) % hyperparams['n_critic'] == 0:\n",
    "                optimizer_g.zero_grad(set_to_none=True)\n",
    "                with autocast(enabled=(device.type == 'cuda')):\n",
    "                    gen_output = discriminator(fake_audio, csi_data)\n",
    "                    g_loss_adv = criterion(gen_output, torch.ones_like(gen_output))\n",
    "                    g_loss_recon = F.l1_loss(fake_audio, audio_data) * hyperparams['lambda_reconstruction']\n",
    "                    g_loss_spectral = spectral_loss(fake_audio, audio_data) * hyperparams['lambda_spectral']\n",
    "                    g_loss = g_loss_adv + g_loss_recon + g_loss_spectral\n",
    "                \n",
    "                scaler.scale(g_loss).backward()\n",
    "                scaler.step(optimizer_g)\n",
    "            \n",
    "            scaler.update()\n",
    "\n",
    "            epoch_losses['d_loss'] += d_loss.item()\n",
    "            epoch_losses['g_loss'] += g_loss.item()\n",
    "            epoch_losses['g_adv'] += g_loss_adv.item()\n",
    "            epoch_losses['g_recon'] += g_loss_recon.item()\n",
    "            epoch_losses['g_spectral'] += g_loss_spectral.item()\n",
    "            epoch_losses['d_real'] += d_loss_real.item()\n",
    "            epoch_losses['d_fake'] += d_loss_fake.item()\n",
    "            \n",
    "        for key in history:\n",
    "            history[key].append(epoch_losses[key] / len(train_dataloader))\n",
    "        logging.info(f\"Epoch {epoch+1} - G_Loss: {history['g_loss'][-1]:.4f}, D_Loss: {history['d_loss'][-1]:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# ============================================================================\n",
    "# PLOTTING AND MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_results(history, output_dir):\n",
    "    \"\"\"Saves plots of the training history to the output directory.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    epochs = range(1, len(history['g_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(epochs, history['g_loss'], label='Generator Total Loss', color='blue')\n",
    "    plt.plot(epochs, history['d_loss'], label='Discriminator Total Loss', color='red')\n",
    "    plt.title('Generator vs. Discriminator Loss', fontsize=16)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'total_losses.png')); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(epochs, history['g_adv'], label='Adversarial Loss', linestyle='--')\n",
    "    plt.plot(epochs, history['g_recon'], label='Reconstruction Loss', linestyle='--')\n",
    "    plt.plot(epochs, history['g_spectral'], label='Spectral Loss', linestyle='--')\n",
    "    plt.title('Generator Loss Components', fontsize=16)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'generator_components.png')); plt.close()\n",
    "\n",
    "    logging.info(f\"Training plots saved to {output_dir}\")\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    \"\"\"Extracts a timestamp from a filename.\"\"\"\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\\.\\d+)', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def determine_sample_rate(audio_dir):\n",
    "    \"\"\"Determines the sample rate from the first audio file.\"\"\"\n",
    "    try:\n",
    "        audio_file = [f for f in os.listdir(audio_dir) if f.endswith('.wav')][0]\n",
    "        with sf.SoundFile(os.path.join(audio_dir, audio_file)) as f:\n",
    "            return f.samplerate\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not determine sample rate from {audio_dir}. Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Loads, matches, and splits the CSI and audio data.\"\"\"\n",
    "    try:\n",
    "        sample_rate = determine_sample_rate(AUDIO_DIRECTORY)\n",
    "        HYPERPARAMETERS['sample_rate'] = sample_rate\n",
    "        HYPERPARAMETERS['sequence_length'] = int(sample_rate * HYPERPARAMETERS['audio_duration'])\n",
    "\n",
    "        csi_files = [f for f in os.listdir(CSI_DIRECTORY) if f.endswith('.csv')]\n",
    "        audio_files = [f for f in os.listdir(AUDIO_DIRECTORY) if f.endswith('.wav')]\n",
    "        logging.info(f\"Found {len(csi_files)} CSI files and {len(audio_files)} audio files.\")\n",
    "\n",
    "        csi_ts = {extract_timestamp(f): f for f in csi_files if extract_timestamp(f)}\n",
    "        audio_ts = {extract_timestamp(f): f for f in audio_files if extract_timestamp(f)}\n",
    "        \n",
    "        common_ts = sorted(list(set(csi_ts.keys()) & set(audio_ts.keys())))\n",
    "        logging.info(f\"Found {len(common_ts)} matching file pairs.\")\n",
    "\n",
    "        if not common_ts:\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        data_pairs = [{\"csi_path\": os.path.join(CSI_DIRECTORY, csi_ts[ts]),\n",
    "                       \"audio_path\": os.path.join(AUDIO_DIRECTORY, audio_ts[ts])} for ts in common_ts]\n",
    "\n",
    "        df = pd.DataFrame(data_pairs)\n",
    "        return train_test_split(df, test_size=1-HYPERPARAMETERS['train_test_split'], random_state=HYPERPARAMETERS['random_seed'])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Data preparation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Filters out None items and stacks the rest.\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch: return None\n",
    "    csi_data = torch.stack([item[0] for item in batch])\n",
    "    audio_data = torch.stack([item[1] for item in batch])\n",
    "    return csi_data, audio_data\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"=\" * 80 + \"\\nSTARTING ENHANCED CSI-AUDIO GAN TRAINING\\n\" + \"=\" * 80)\n",
    "        \n",
    "        train_df, test_df = load_and_prepare_data()\n",
    "        \n",
    "        if train_df.empty:\n",
    "            logging.error(\"Could not create a training set: no matching data files found.\")\n",
    "            logging.error(\"Please check that your CSI and audio filenames contain matching timestamps.\")\n",
    "            logging.info(\"Example format: '..._2025-09-16_11-30-00.123456.csv'\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Data split: {len(train_df)} train, {len(test_df)} test samples.\")\n",
    "        logging.info(\"CONFIGURATION:\")\n",
    "        for key, value in HYPERPARAMETERS.items(): logging.info(f\"  {key}: {value}\")\n",
    "\n",
    "        train_dataset = EnhancedCSIAudioDataset(train_df, is_training=True)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=HYPERPARAMETERS['batch_size'], shuffle=True, \n",
    "                                      collate_fn=collate_fn, drop_last=True, num_workers=os.cpu_count()//2)\n",
    "\n",
    "        generator = EnhancedGenerator(HYPERPARAMETERS['latent_dim'], HYPERPARAMETERS['csi_channels']).to(HYPERPARAMETERS['device'])\n",
    "        discriminator = EnhancedDiscriminator(HYPERPARAMETERS['csi_channels']).to(HYPERPARAMETERS['device'])\n",
    "\n",
    "        logging.info(f\"Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "        logging.info(f\"Discriminator params: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "        logging.info(\"Starting training...\")\n",
    "        \n",
    "        history = train_enhanced_gan(generator, discriminator, train_dataloader, HYPERPARAMETERS)\n",
    "        \n",
    "        logging.info(\"Training finished. Saving models and history...\")\n",
    "        torch.save(generator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \"enhanced_generator.pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \"enhanced_discriminator.pth\"))\n",
    "        \n",
    "        history_df = pd.DataFrame(history)\n",
    "        history_path = os.path.join(OUTPUT_DIRECTORY, 'training_history.csv')\n",
    "        history_df.to_csv(history_path, index=False)\n",
    "        \n",
    "        plot_results(history, OUTPUT_DIRECTORY)\n",
    "\n",
    "        logging.info(\"=\" * 80 + \"\\nTRAINING COMPLETED SUCCESSFULLY!\\n\" + \"=\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training pipeline failed: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 12:06:54,729 - INFO - ================================================================================\n",
      "2025-09-16 12:06:54,730 - INFO - STARTING OPTIMIZED CSI-AUDIO GAN TRAINING WITH BCE LOSS\n",
      "2025-09-16 12:06:54,731 - INFO - ================================================================================\n",
      "2025-09-16 12:06:54,737 - INFO - Determined sample rate: 44100Hz, sequence length: 441000\n",
      "2025-09-16 12:06:54,745 - INFO - Found 112 CSI files and 70 audio files\n",
      "2025-09-16 12:06:54,747 - INFO - Found 51 matching pairs\n",
      "2025-09-16 12:06:54,751 - INFO - Data split: 40 train, 11 test\n",
      "2025-09-16 12:06:54,752 - INFO - BCE OPTIMIZED CONFIGURATION:\n",
      "2025-09-16 12:06:54,754 - INFO -   latent_dim: 48\n",
      "2025-09-16 12:06:54,755 - INFO -   csi_channels: 8\n",
      "2025-09-16 12:06:54,756 - INFO -   audio_channels: 1\n",
      "2025-09-16 12:06:54,759 - INFO -   epochs: 200\n",
      "2025-09-16 12:06:54,760 - INFO -   batch_size: 8\n",
      "2025-09-16 12:06:54,761 - INFO -   lr_generator: 0.0002\n",
      "2025-09-16 12:06:54,762 - INFO -   lr_discriminator: 0.0001\n",
      "2025-09-16 12:06:54,763 - INFO -   lr_scheduler_gamma: 0.95\n",
      "2025-09-16 12:06:54,764 - INFO -   beta1: 0.5\n",
      "2025-09-16 12:06:54,765 - INFO -   beta2: 0.999\n",
      "2025-09-16 12:06:54,766 - INFO -   sample_rate: 44100\n",
      "2025-09-16 12:06:54,767 - INFO -   audio_duration: 10.0\n",
      "2025-09-16 12:06:54,768 - INFO -   sequence_length: 441000\n",
      "2025-09-16 12:06:54,768 - INFO -   chunk_size: 2048\n",
      "2025-09-16 12:06:54,769 - INFO -   overlap: 256\n",
      "2025-09-16 12:06:54,770 - INFO -   max_chunks_per_sample: 12\n",
      "2025-09-16 12:06:54,772 - INFO -   lambda_reconstruction: 10.0\n",
      "2025-09-16 12:06:54,775 - INFO -   lambda_feature_matching: 8.0\n",
      "2025-09-16 12:06:54,776 - INFO -   lambda_spectral: 2.0\n",
      "2025-09-16 12:06:54,778 - INFO -   lambda_drum_loss: 5.0\n",
      "2025-09-16 12:06:54,779 - INFO -   lambda_adversarial: 1.0\n",
      "2025-09-16 12:06:54,780 - INFO -   label_smoothing: 0.1\n",
      "2025-09-16 12:06:54,781 - INFO -   discriminator_steps: 1\n",
      "2025-09-16 12:06:54,782 - INFO -   generator_steps: 1\n",
      "2025-09-16 12:06:54,784 - INFO -   train_test_split: 0.8\n",
      "2025-09-16 12:06:54,786 - INFO -   random_seed: 42\n",
      "2025-09-16 12:06:54,788 - INFO -   enable_noise_reduction: True\n",
      "2025-09-16 12:06:54,790 - INFO -   preserve_drums: True\n",
      "2025-09-16 12:06:54,792 - INFO -   drum_freq_range: [40, 300]\n",
      "2025-09-16 12:06:54,793 - INFO -   kick_freq_range: [30, 120]\n",
      "2025-09-16 12:06:54,795 - INFO -   snare_freq_range: [120, 400]\n",
      "2025-09-16 12:06:54,796 - INFO -   drum_enhancement_factor: 1.5\n",
      "2025-09-16 12:06:54,798 - INFO -   enable_augmentation: False\n",
      "2025-09-16 12:06:54,799 - INFO -   use_mixed_precision: True\n",
      "2025-09-16 12:06:54,801 - INFO -   gradient_accumulation_steps: 2\n",
      "2025-09-16 12:06:54,803 - INFO -   device: cuda\n",
      "2025-09-16 12:06:54,806 - INFO - Lightweight CSI preprocessor initialized\n",
      "2025-09-16 12:06:54,809 - INFO - Lightweight audio preprocessor initialized\n",
      "2025-09-16 12:06:54,842 - INFO - Validated 40 samples\n",
      "2025-09-16 12:06:54,844 - INFO - Optimized dataset initialized: 40 samples\n",
      "2025-09-16 12:06:54,846 - INFO - Optimized Generator - total input: 32816\n",
      "2025-09-16 12:06:54,944 - INFO - Optimized Discriminator initialized for BCE loss\n",
      "2025-09-16 12:06:54,965 - INFO - BCE Generator parameters: 8,722,817\n",
      "2025-09-16 12:06:54,967 - INFO - BCE Discriminator parameters: 363,969\n",
      "2025-09-16 12:06:54,968 - INFO - Starting BCE GAN training...\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:514: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if hyperparams['use_mixed_precision'] else None\n",
      "Epoch 1/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 1/200:  20%|        | 1/5 [00:00<00:03,  1.15it/s, D_loss=1.3937, G_loss=61.1442, Cos_Sim=0.0011]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 1/200:  40%|      | 2/5 [00:01<00:02,  1.44it/s, D_loss=1.3971, G_loss=56.2312, Cos_Sim=0.0075]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 1/200:  60%|    | 3/5 [00:02<00:01,  1.54it/s, D_loss=1.3905, G_loss=56.9592, Cos_Sim=0.0081]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 1/200:  80%|  | 4/5 [00:02<00:00,  1.64it/s, D_loss=1.3787, G_loss=52.2350, Cos_Sim=0.0008]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 1/200: 100%|| 5/5 [00:03<00:00,  1.60it/s, D_loss=1.3732, G_loss=60.7849, Cos_Sim=-0.0172]\n",
      "2025-09-16 12:06:58,092 - INFO - Epoch 1: D_loss=1.3866, G_loss=57.4709, Avg Cos_Sim=0.0001\n",
      "Epoch 2/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 2/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=1.3611, G_loss=52.9362, Cos_Sim=-0.0069]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 2/200:  40%|      | 2/5 [00:01<00:02,  1.50it/s, D_loss=1.3573, G_loss=52.9992, Cos_Sim=-0.0168]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 2/200:  60%|    | 3/5 [00:01<00:01,  1.70it/s, D_loss=1.3438, G_loss=56.5841, Cos_Sim=0.0062] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 2/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=1.3346, G_loss=51.1653, Cos_Sim=0.0109]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 2/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=1.3254, G_loss=51.8128, Cos_Sim=0.0017]\n",
      "2025-09-16 12:07:01,318 - INFO - Epoch 2: D_loss=1.3444, G_loss=53.0995, Avg Cos_Sim=-0.0010\n",
      "Epoch 3/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 3/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=1.3166, G_loss=50.4565, Cos_Sim=-0.0207]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 3/200:  40%|      | 2/5 [00:01<00:02,  1.50it/s, D_loss=1.2994, G_loss=58.5000, Cos_Sim=0.0013] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 3/200:  60%|    | 3/5 [00:01<00:01,  1.58it/s, D_loss=1.3093, G_loss=46.3213, Cos_Sim=-0.0039]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 3/200:  80%|  | 4/5 [00:02<00:00,  1.71it/s, D_loss=1.2590, G_loss=54.9591, Cos_Sim=0.0041] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 3/200: 100%|| 5/5 [00:02<00:00,  1.67it/s, D_loss=1.2722, G_loss=60.6060, Cos_Sim=0.0080]\n",
      "2025-09-16 12:07:04,592 - INFO - Epoch 3: D_loss=1.2913, G_loss=54.1686, Avg Cos_Sim=-0.0023\n",
      "Epoch 4/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 4/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=1.2720, G_loss=59.8550, Cos_Sim=-0.0235]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 4/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=1.2433, G_loss=43.6713, Cos_Sim=0.0075] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 4/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=1.2119, G_loss=59.1073, Cos_Sim=-0.0202]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 4/200:  80%|  | 4/5 [00:02<00:00,  1.83it/s, D_loss=1.2175, G_loss=48.4104, Cos_Sim=-0.0007]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 4/200: 100%|| 5/5 [00:02<00:00,  1.76it/s, D_loss=1.2536, G_loss=48.5352, Cos_Sim=-0.0002]\n",
      "2025-09-16 12:07:07,708 - INFO - Epoch 4: D_loss=1.2397, G_loss=51.9158, Avg Cos_Sim=-0.0074\n",
      "Epoch 5/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 5/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=1.1959, G_loss=45.2555, Cos_Sim=-0.0011]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 5/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=1.2075, G_loss=53.6677, Cos_Sim=-0.0071]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 5/200:  60%|    | 3/5 [00:01<00:01,  1.81it/s, D_loss=1.2395, G_loss=53.7013, Cos_Sim=-0.0021]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 5/200:  80%|  | 4/5 [00:02<00:00,  1.87it/s, D_loss=1.1742, G_loss=53.1152, Cos_Sim=-0.0122]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 5/200: 100%|| 5/5 [00:02<00:00,  1.82it/s, D_loss=1.2063, G_loss=43.9891, Cos_Sim=-0.0022]\n",
      "2025-09-16 12:07:10,749 - INFO - Epoch 5: D_loss=1.2047, G_loss=49.9458, Avg Cos_Sim=-0.0050\n",
      "Epoch 6/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 6/200:  20%|        | 1/5 [00:00<00:02,  1.39it/s, D_loss=1.1568, G_loss=48.8633, Cos_Sim=-0.0013]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 6/200:  40%|      | 2/5 [00:01<00:01,  1.69it/s, D_loss=1.1829, G_loss=52.8226, Cos_Sim=-0.0183]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 6/200:  60%|    | 3/5 [00:01<00:01,  1.84it/s, D_loss=1.1664, G_loss=55.4161, Cos_Sim=0.0125] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 6/200:  80%|  | 4/5 [00:02<00:00,  1.91it/s, D_loss=1.1899, G_loss=52.7282, Cos_Sim=-0.0025]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 6/200: 100%|| 5/5 [00:02<00:00,  1.82it/s, D_loss=1.1736, G_loss=50.6362, Cos_Sim=0.0090] \n",
      "2025-09-16 12:07:13,826 - INFO - Epoch 6: D_loss=1.1739, G_loss=52.0933, Avg Cos_Sim=-0.0001\n",
      "Epoch 7/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 7/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=1.1528, G_loss=44.3249, Cos_Sim=0.0070]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 7/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=1.1383, G_loss=43.5868, Cos_Sim=-0.0010]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 7/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=1.1272, G_loss=42.5581, Cos_Sim=0.0111] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 7/200:  80%|  | 4/5 [00:02<00:00,  1.81it/s, D_loss=1.1830, G_loss=46.1403, Cos_Sim=-0.0067]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 7/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=1.1607, G_loss=56.9440, Cos_Sim=0.0018] \n",
      "2025-09-16 12:07:16,981 - INFO - Epoch 7: D_loss=1.1524, G_loss=46.7108, Avg Cos_Sim=0.0024\n",
      "Epoch 8/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 8/200:  20%|        | 1/5 [00:00<00:02,  1.35it/s, D_loss=1.1416, G_loss=45.0940, Cos_Sim=0.0246]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 8/200:  40%|      | 2/5 [00:01<00:01,  1.68it/s, D_loss=1.0970, G_loss=49.3556, Cos_Sim=0.0023]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 8/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=1.1240, G_loss=52.1802, Cos_Sim=0.0032]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 8/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=1.0792, G_loss=44.2343, Cos_Sim=-0.0029]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 8/200: 100%|| 5/5 [00:02<00:00,  1.80it/s, D_loss=1.1148, G_loss=49.3898, Cos_Sim=0.0113] \n",
      "2025-09-16 12:07:20,020 - INFO - Epoch 8: D_loss=1.1113, G_loss=48.0508, Avg Cos_Sim=0.0077\n",
      "Epoch 9/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 9/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=1.0678, G_loss=54.4411, Cos_Sim=-0.0036]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 9/200:  40%|      | 2/5 [00:01<00:01,  1.54it/s, D_loss=1.1249, G_loss=56.7458, Cos_Sim=0.0035] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 9/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=1.0424, G_loss=52.2824, Cos_Sim=0.0043]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 9/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=1.1794, G_loss=46.7279, Cos_Sim=-0.0005]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 9/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=1.0635, G_loss=44.7708, Cos_Sim=0.0015] \n",
      "2025-09-16 12:07:23,087 - INFO - Epoch 9: D_loss=1.0956, G_loss=50.9936, Avg Cos_Sim=0.0010\n",
      "Epoch 10/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 10/200:  20%|        | 1/5 [00:00<00:02,  1.42it/s, D_loss=1.1054, G_loss=50.1791, Cos_Sim=-0.0062]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 10/200:  40%|      | 2/5 [00:01<00:01,  1.73it/s, D_loss=1.0689, G_loss=49.4241, Cos_Sim=0.0054] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 10/200:  60%|    | 3/5 [00:01<00:01,  1.79it/s, D_loss=1.0490, G_loss=55.5051, Cos_Sim=-0.0042]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 10/200:  80%|  | 4/5 [00:02<00:00,  1.90it/s, D_loss=1.0296, G_loss=51.3579, Cos_Sim=0.0088] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 10/200: 100%|| 5/5 [00:02<00:00,  1.82it/s, D_loss=1.1096, G_loss=47.0937, Cos_Sim=-0.0118]\n",
      "2025-09-16 12:07:26,104 - INFO - Epoch 10: D_loss=1.0725, G_loss=50.7120, Avg Cos_Sim=-0.0016\n",
      "Epoch 11/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 11/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=1.0215, G_loss=52.9378, Cos_Sim=-0.0041]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 11/200:  40%|      | 2/5 [00:01<00:01,  1.70it/s, D_loss=1.0964, G_loss=48.7900, Cos_Sim=0.0042] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 11/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=1.0425, G_loss=48.4221, Cos_Sim=-0.0014]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 11/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=1.1616, G_loss=52.4881, Cos_Sim=-0.0188]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 11/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=1.0584, G_loss=50.3443, Cos_Sim=-0.0063]\n",
      "2025-09-16 12:07:29,204 - INFO - Epoch 11: D_loss=1.0761, G_loss=50.5965, Avg Cos_Sim=-0.0053\n",
      "Epoch 12/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 12/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=1.0630, G_loss=48.5394, Cos_Sim=-0.0017]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 12/200:  40%|      | 2/5 [00:01<00:01,  1.66it/s, D_loss=1.0663, G_loss=46.3010, Cos_Sim=0.0098] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 12/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=1.0909, G_loss=44.8686, Cos_Sim=0.0189]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 12/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=1.0499, G_loss=50.2093, Cos_Sim=-0.0140]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 12/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=1.0001, G_loss=51.5263, Cos_Sim=-0.0121]\n",
      "2025-09-16 12:07:32,289 - INFO - Epoch 12: D_loss=1.0540, G_loss=48.2889, Avg Cos_Sim=0.0002\n",
      "Epoch 13/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 13/200:  20%|        | 1/5 [00:00<00:02,  1.34it/s, D_loss=1.0290, G_loss=48.7435, Cos_Sim=-0.0081]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 13/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.9998, G_loss=48.0623, Cos_Sim=0.0045] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 13/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=1.0134, G_loss=52.9783, Cos_Sim=-0.0075]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 13/200:  80%|  | 4/5 [00:02<00:00,  1.85it/s, D_loss=1.0159, G_loss=50.3658, Cos_Sim=-0.0197]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 13/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=1.0281, G_loss=43.8141, Cos_Sim=-0.0159]\n",
      "2025-09-16 12:07:35,364 - INFO - Epoch 13: D_loss=1.0173, G_loss=48.7928, Avg Cos_Sim=-0.0093\n",
      "Epoch 14/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 14/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=1.0346, G_loss=51.0026, Cos_Sim=-0.0182]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 14/200:  40%|      | 2/5 [00:01<00:01,  1.65it/s, D_loss=1.0006, G_loss=49.7360, Cos_Sim=0.0073] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 14/200:  60%|    | 3/5 [00:01<00:01,  1.79it/s, D_loss=1.0110, G_loss=43.7321, Cos_Sim=0.0107]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 14/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=1.0066, G_loss=51.1883, Cos_Sim=-0.0090]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 14/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=1.0663, G_loss=42.0192, Cos_Sim=0.0049] \n",
      "2025-09-16 12:07:38,415 - INFO - Epoch 14: D_loss=1.0238, G_loss=47.5356, Avg Cos_Sim=-0.0009\n",
      "Epoch 15/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 15/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=1.0268, G_loss=47.4212, Cos_Sim=0.0012]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 15/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.9601, G_loss=45.1867, Cos_Sim=0.0073]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 15/200:  60%|    | 3/5 [00:01<00:01,  1.78it/s, D_loss=1.0372, G_loss=50.2133, Cos_Sim=-0.0273]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 15/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.9721, G_loss=50.9134, Cos_Sim=0.0159] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 15/200: 100%|| 5/5 [00:02<00:00,  1.74it/s, D_loss=1.0128, G_loss=39.0443, Cos_Sim=0.0122]\n",
      "2025-09-16 12:07:41,568 - INFO - Epoch 15: D_loss=1.0018, G_loss=46.5558, Avg Cos_Sim=0.0019\n",
      "Epoch 16/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 16/200:  20%|        | 1/5 [00:00<00:03,  1.33it/s, D_loss=0.9669, G_loss=50.3364, Cos_Sim=-0.0071]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 16/200:  40%|      | 2/5 [00:01<00:01,  1.69it/s, D_loss=0.9621, G_loss=43.7856, Cos_Sim=0.0051] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 16/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=1.1191, G_loss=50.5680, Cos_Sim=-0.0041]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 16/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.9794, G_loss=53.1071, Cos_Sim=-0.0117]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 16/200: 100%|| 5/5 [00:02<00:00,  1.74it/s, D_loss=0.9470, G_loss=46.8455, Cos_Sim=-0.0019]\n",
      "2025-09-16 12:07:44,720 - INFO - Epoch 16: D_loss=0.9949, G_loss=48.9285, Avg Cos_Sim=-0.0039\n",
      "Epoch 17/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 17/200:  20%|        | 1/5 [00:00<00:02,  1.39it/s, D_loss=0.9647, G_loss=50.2208, Cos_Sim=-0.0156]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 17/200:  40%|      | 2/5 [00:01<00:01,  1.71it/s, D_loss=1.0616, G_loss=49.5809, Cos_Sim=-0.0013]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 17/200:  60%|    | 3/5 [00:01<00:01,  1.85it/s, D_loss=0.9663, G_loss=48.3884, Cos_Sim=-0.0207]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 17/200:  80%|  | 4/5 [00:02<00:00,  1.96it/s, D_loss=1.0705, G_loss=54.5309, Cos_Sim=-0.0094]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 17/200: 100%|| 5/5 [00:02<00:00,  1.88it/s, D_loss=0.9673, G_loss=54.7520, Cos_Sim=0.0114] \n",
      "2025-09-16 12:07:47,646 - INFO - Epoch 17: D_loss=1.0061, G_loss=51.4946, Avg Cos_Sim=-0.0071\n",
      "Epoch 18/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 18/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.9185, G_loss=46.2876, Cos_Sim=-0.0042]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 18/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.9490, G_loss=41.1196, Cos_Sim=-0.0063]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 18/200:  60%|    | 3/5 [00:01<00:01,  1.79it/s, D_loss=0.9520, G_loss=45.2939, Cos_Sim=-0.0389]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 18/200:  80%|  | 4/5 [00:02<00:00,  1.78it/s, D_loss=1.0016, G_loss=42.2809, Cos_Sim=-0.0027]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 18/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.9066, G_loss=53.2837, Cos_Sim=-0.0017]\n",
      "2025-09-16 12:07:50,778 - INFO - Epoch 18: D_loss=0.9455, G_loss=45.6531, Avg Cos_Sim=-0.0108\n",
      "Epoch 19/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 19/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=0.9154, G_loss=49.6047, Cos_Sim=0.0118]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 19/200:  40%|      | 2/5 [00:01<00:01,  1.57it/s, D_loss=0.9054, G_loss=50.2951, Cos_Sim=-0.0063]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 19/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.9069, G_loss=47.0222, Cos_Sim=-0.0041]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 19/200:  80%|  | 4/5 [00:02<00:00,  1.84it/s, D_loss=0.9592, G_loss=46.8223, Cos_Sim=0.0012] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 19/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=0.9704, G_loss=52.3333, Cos_Sim=0.0073]\n",
      "2025-09-16 12:07:53,847 - INFO - Epoch 19: D_loss=0.9315, G_loss=49.2155, Avg Cos_Sim=0.0020\n",
      "Epoch 20/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 20/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.9308, G_loss=47.2592, Cos_Sim=0.0130]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 20/200:  40%|      | 2/5 [00:01<00:01,  1.71it/s, D_loss=0.8855, G_loss=46.8156, Cos_Sim=0.0070]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 20/200:  60%|    | 3/5 [00:01<00:01,  1.82it/s, D_loss=0.9832, G_loss=51.4785, Cos_Sim=0.0033]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 20/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.9085, G_loss=50.6907, Cos_Sim=0.0240]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 20/200: 100%|| 5/5 [00:02<00:00,  1.78it/s, D_loss=0.9679, G_loss=46.1417, Cos_Sim=0.0211]\n",
      "2025-09-16 12:07:56,918 - INFO - Epoch 20: D_loss=0.9352, G_loss=48.4771, Avg Cos_Sim=0.0137\n",
      "Epoch 21/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 21/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.9395, G_loss=44.6948, Cos_Sim=-0.0231]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 21/200:  40%|      | 2/5 [00:01<00:02,  1.42it/s, D_loss=1.0218, G_loss=45.0570, Cos_Sim=0.0155] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 21/200:  60%|    | 3/5 [00:02<00:01,  1.54it/s, D_loss=0.9882, G_loss=46.6290, Cos_Sim=-0.0051]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 21/200:  80%|  | 4/5 [00:02<00:00,  1.64it/s, D_loss=0.9238, G_loss=47.2312, Cos_Sim=-0.0012]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 21/200: 100%|| 5/5 [00:03<00:00,  1.62it/s, D_loss=0.9129, G_loss=49.1945, Cos_Sim=-0.0158]\n",
      "2025-09-16 12:08:00,339 - INFO - Epoch 21: D_loss=0.9572, G_loss=46.5613, Avg Cos_Sim=-0.0059\n",
      "Epoch 22/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 22/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.9539, G_loss=46.3995, Cos_Sim=0.0067]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 22/200:  40%|      | 2/5 [00:01<00:02,  1.50it/s, D_loss=0.9109, G_loss=39.9528, Cos_Sim=-0.0096]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 22/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.9766, G_loss=50.4039, Cos_Sim=-0.0114]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 22/200:  80%|  | 4/5 [00:02<00:00,  1.65it/s, D_loss=0.9908, G_loss=47.6286, Cos_Sim=0.0179] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 22/200: 100%|| 5/5 [00:03<00:00,  1.66it/s, D_loss=0.9345, G_loss=47.7558, Cos_Sim=0.0193]\n",
      "2025-09-16 12:08:03,620 - INFO - Epoch 22: D_loss=0.9533, G_loss=46.4281, Avg Cos_Sim=0.0046\n",
      "Epoch 23/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 23/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=1.0281, G_loss=40.1863, Cos_Sim=-0.0222]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 23/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.9007, G_loss=47.8132, Cos_Sim=-0.0155]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 23/200:  60%|    | 3/5 [00:01<00:01,  1.79it/s, D_loss=0.8958, G_loss=47.0469, Cos_Sim=0.0060] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 23/200:  80%|  | 4/5 [00:02<00:00,  1.84it/s, D_loss=0.9229, G_loss=46.4310, Cos_Sim=-0.0036]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 23/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.9463, G_loss=48.1562, Cos_Sim=0.0226] \n",
      "2025-09-16 12:08:06,724 - INFO - Epoch 23: D_loss=0.9388, G_loss=45.9267, Avg Cos_Sim=-0.0025\n",
      "Epoch 24/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 24/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.9262, G_loss=44.6097, Cos_Sim=-0.0232]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 24/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.9372, G_loss=57.0502, Cos_Sim=-0.0022]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 24/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.9308, G_loss=48.9296, Cos_Sim=-0.0016]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 24/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.9996, G_loss=47.4725, Cos_Sim=-0.0090]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 24/200: 100%|| 5/5 [00:02<00:00,  1.72it/s, D_loss=0.9200, G_loss=39.6226, Cos_Sim=-0.0029]\n",
      "2025-09-16 12:08:09,900 - INFO - Epoch 24: D_loss=0.9428, G_loss=47.5369, Avg Cos_Sim=-0.0078\n",
      "Epoch 25/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 25/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.9304, G_loss=53.0473, Cos_Sim=0.0130]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 25/200:  40%|      | 2/5 [00:01<00:01,  1.71it/s, D_loss=0.9532, G_loss=38.7818, Cos_Sim=-0.0166]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 25/200:  60%|    | 3/5 [00:01<00:01,  1.80it/s, D_loss=0.9382, G_loss=43.9865, Cos_Sim=-0.0014]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 25/200:  80%|  | 4/5 [00:02<00:00,  1.84it/s, D_loss=0.9149, G_loss=46.9984, Cos_Sim=-0.0075]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 25/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=0.9470, G_loss=47.0836, Cos_Sim=0.0118] \n",
      "2025-09-16 12:08:12,963 - INFO - Epoch 25: D_loss=0.9368, G_loss=45.9795, Avg Cos_Sim=-0.0001\n",
      "Epoch 26/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 26/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=0.9276, G_loss=54.4066, Cos_Sim=0.0077]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 26/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.9216, G_loss=42.1115, Cos_Sim=0.0033]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 26/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=0.9552, G_loss=40.6550, Cos_Sim=-0.0084]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 26/200:  80%|  | 4/5 [00:02<00:00,  1.83it/s, D_loss=0.9385, G_loss=44.4559, Cos_Sim=-0.0170]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 26/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.9245, G_loss=51.8925, Cos_Sim=0.0045] \n",
      "2025-09-16 12:08:16,098 - INFO - Epoch 26: D_loss=0.9335, G_loss=46.7043, Avg Cos_Sim=-0.0020\n",
      "Epoch 27/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 27/200:  20%|        | 1/5 [00:00<00:02,  1.38it/s, D_loss=0.9403, G_loss=47.0377, Cos_Sim=0.0175]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 27/200:  40%|      | 2/5 [00:01<00:01,  1.75it/s, D_loss=0.8967, G_loss=46.1416, Cos_Sim=-0.0082]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 27/200:  60%|    | 3/5 [00:01<00:01,  1.82it/s, D_loss=0.8887, G_loss=50.5518, Cos_Sim=0.0036] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 27/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.9081, G_loss=43.4823, Cos_Sim=0.0070]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 27/200: 100%|| 5/5 [00:02<00:00,  1.80it/s, D_loss=0.9594, G_loss=49.5697, Cos_Sim=0.0057]\n",
      "2025-09-16 12:08:19,123 - INFO - Epoch 27: D_loss=0.9186, G_loss=47.3566, Avg Cos_Sim=0.0051\n",
      "Epoch 28/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 28/200:  20%|        | 1/5 [00:00<00:03,  1.33it/s, D_loss=0.8868, G_loss=42.5124, Cos_Sim=-0.0121]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 28/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.8798, G_loss=44.3664, Cos_Sim=0.0148] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 28/200:  60%|    | 3/5 [00:01<00:01,  1.81it/s, D_loss=0.8548, G_loss=50.4826, Cos_Sim=-0.0154]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 28/200:  80%|  | 4/5 [00:02<00:00,  1.91it/s, D_loss=0.8905, G_loss=46.6548, Cos_Sim=-0.0211]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 28/200: 100%|| 5/5 [00:02<00:00,  1.82it/s, D_loss=0.9355, G_loss=50.4693, Cos_Sim=0.0090] \n",
      "2025-09-16 12:08:22,165 - INFO - Epoch 28: D_loss=0.8895, G_loss=46.8971, Avg Cos_Sim=-0.0050\n",
      "Epoch 29/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 29/200:  20%|        | 1/5 [00:00<00:02,  1.37it/s, D_loss=0.8665, G_loss=38.8295, Cos_Sim=0.0176]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 29/200:  40%|      | 2/5 [00:01<00:01,  1.70it/s, D_loss=0.9114, G_loss=40.1572, Cos_Sim=0.0057]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 29/200:  60%|    | 3/5 [00:01<00:01,  1.80it/s, D_loss=0.9644, G_loss=48.9145, Cos_Sim=-0.0030]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 29/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.8912, G_loss=51.7547, Cos_Sim=-0.0005]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 29/200: 100%|| 5/5 [00:02<00:00,  1.80it/s, D_loss=0.9147, G_loss=51.1390, Cos_Sim=-0.0118]\n",
      "2025-09-16 12:08:25,223 - INFO - Epoch 29: D_loss=0.9096, G_loss=46.1590, Avg Cos_Sim=0.0016\n",
      "Epoch 30/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 30/200:  20%|        | 1/5 [00:00<00:02,  1.38it/s, D_loss=0.9901, G_loss=41.5850, Cos_Sim=0.0082]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 30/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8984, G_loss=46.3815, Cos_Sim=-0.0004]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 30/200:  60%|    | 3/5 [00:01<00:01,  1.76it/s, D_loss=0.8519, G_loss=39.3414, Cos_Sim=-0.0019]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 30/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.9003, G_loss=56.6898, Cos_Sim=0.0024] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 30/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.8512, G_loss=46.0287, Cos_Sim=-0.0026]\n",
      "2025-09-16 12:08:28,305 - INFO - Epoch 30: D_loss=0.8984, G_loss=46.0053, Avg Cos_Sim=0.0011\n",
      "Epoch 31/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 31/200:  20%|        | 1/5 [00:00<00:02,  1.34it/s, D_loss=0.8617, G_loss=54.6596, Cos_Sim=-0.0122]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 31/200:  40%|      | 2/5 [00:01<00:01,  1.71it/s, D_loss=0.8468, G_loss=41.9731, Cos_Sim=0.0156] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 31/200:  60%|    | 3/5 [00:01<00:01,  1.83it/s, D_loss=1.0022, G_loss=40.9027, Cos_Sim=0.0309]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 31/200:  80%|  | 4/5 [00:02<00:00,  1.88it/s, D_loss=0.8385, G_loss=44.0776, Cos_Sim=-0.0050]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 31/200: 100%|| 5/5 [00:02<00:00,  1.78it/s, D_loss=0.8521, G_loss=46.7291, Cos_Sim=0.0037] \n",
      "2025-09-16 12:08:31,398 - INFO - Epoch 31: D_loss=0.8803, G_loss=45.6684, Avg Cos_Sim=0.0066\n",
      "Epoch 32/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 32/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.8890, G_loss=42.0940, Cos_Sim=0.0034]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 32/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.8337, G_loss=44.2371, Cos_Sim=-0.0140]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 32/200:  60%|    | 3/5 [00:01<00:01,  1.70it/s, D_loss=0.8990, G_loss=43.4684, Cos_Sim=0.0184] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 32/200:  80%|  | 4/5 [00:02<00:00,  1.83it/s, D_loss=0.8611, G_loss=40.1228, Cos_Sim=0.0162]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 32/200: 100%|| 5/5 [00:02<00:00,  1.74it/s, D_loss=0.8878, G_loss=47.4469, Cos_Sim=0.0011]\n",
      "2025-09-16 12:08:34,561 - INFO - Epoch 32: D_loss=0.8741, G_loss=43.4739, Avg Cos_Sim=0.0050\n",
      "Epoch 33/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 33/200:  20%|        | 1/5 [00:00<00:02,  1.34it/s, D_loss=0.8319, G_loss=41.6670, Cos_Sim=-0.0101]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 33/200:  40%|      | 2/5 [00:01<00:01,  1.67it/s, D_loss=0.8975, G_loss=43.6064, Cos_Sim=0.0203] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 33/200:  60%|    | 3/5 [00:01<00:01,  1.82it/s, D_loss=0.8570, G_loss=40.5285, Cos_Sim=-0.0147]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 33/200:  80%|  | 4/5 [00:02<00:00,  1.89it/s, D_loss=0.9127, G_loss=37.5692, Cos_Sim=0.0013] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 33/200: 100%|| 5/5 [00:02<00:00,  1.81it/s, D_loss=0.8686, G_loss=54.1681, Cos_Sim=-0.0462]\n",
      "2025-09-16 12:08:37,634 - INFO - Epoch 33: D_loss=0.8736, G_loss=43.5079, Avg Cos_Sim=-0.0099\n",
      "Epoch 34/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 34/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.8747, G_loss=43.5581, Cos_Sim=-0.0052]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 34/200:  40%|      | 2/5 [00:01<00:01,  1.63it/s, D_loss=0.8568, G_loss=42.1888, Cos_Sim=-0.0257]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 34/200:  60%|    | 3/5 [00:01<00:01,  1.76it/s, D_loss=0.9008, G_loss=40.7389, Cos_Sim=0.0248] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 34/200:  80%|  | 4/5 [00:02<00:00,  1.71it/s, D_loss=0.8630, G_loss=47.9071, Cos_Sim=0.0109]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 34/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.8689, G_loss=42.9663, Cos_Sim=0.0249]\n",
      "2025-09-16 12:08:40,839 - INFO - Epoch 34: D_loss=0.8728, G_loss=43.4718, Avg Cos_Sim=0.0059\n",
      "Epoch 35/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 35/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=0.9356, G_loss=52.1820, Cos_Sim=-0.0247]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 35/200:  40%|      | 2/5 [00:01<00:01,  1.53it/s, D_loss=0.9198, G_loss=46.6248, Cos_Sim=-0.0068]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 35/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=0.8485, G_loss=38.5338, Cos_Sim=-0.0196]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 35/200:  80%|  | 4/5 [00:02<00:00,  1.87it/s, D_loss=0.9056, G_loss=51.8603, Cos_Sim=0.0104] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 35/200: 100%|| 5/5 [00:02<00:00,  1.78it/s, D_loss=0.8523, G_loss=44.8816, Cos_Sim=0.0176]\n",
      "2025-09-16 12:08:43,916 - INFO - Epoch 35: D_loss=0.8924, G_loss=46.8165, Avg Cos_Sim=-0.0046\n",
      "Epoch 36/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 36/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.8677, G_loss=42.5751, Cos_Sim=0.0139]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 36/200:  40%|      | 2/5 [00:01<00:01,  1.68it/s, D_loss=0.9041, G_loss=47.5589, Cos_Sim=0.0021]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 36/200:  60%|    | 3/5 [00:01<00:01,  1.86it/s, D_loss=0.8776, G_loss=47.6580, Cos_Sim=0.0203]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 36/200:  80%|  | 4/5 [00:02<00:00,  1.97it/s, D_loss=0.8326, G_loss=43.9456, Cos_Sim=0.0070]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 36/200: 100%|| 5/5 [00:02<00:00,  1.86it/s, D_loss=0.8126, G_loss=47.1223, Cos_Sim=-0.0164]\n",
      "2025-09-16 12:08:46,879 - INFO - Epoch 36: D_loss=0.8589, G_loss=45.7720, Avg Cos_Sim=0.0054\n",
      "Epoch 37/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 37/200:  20%|        | 1/5 [00:00<00:02,  1.36it/s, D_loss=0.8242, G_loss=39.3491, Cos_Sim=0.0024]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 37/200:  40%|      | 2/5 [00:01<00:01,  1.67it/s, D_loss=0.9116, G_loss=48.7016, Cos_Sim=-0.0325]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 37/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.9056, G_loss=49.0846, Cos_Sim=-0.0224]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 37/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.8426, G_loss=41.9738, Cos_Sim=0.0093] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 37/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.8789, G_loss=41.8346, Cos_Sim=0.0038]\n",
      "2025-09-16 12:08:49,977 - INFO - Epoch 37: D_loss=0.8726, G_loss=44.1887, Avg Cos_Sim=-0.0079\n",
      "Epoch 38/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 38/200:  20%|        | 1/5 [00:00<00:02,  1.39it/s, D_loss=0.8623, G_loss=40.2117, Cos_Sim=0.0093]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 38/200:  40%|      | 2/5 [00:01<00:01,  1.67it/s, D_loss=0.9015, G_loss=47.5908, Cos_Sim=0.0280]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 38/200:  60%|    | 3/5 [00:01<00:01,  1.81it/s, D_loss=0.8238, G_loss=48.3823, Cos_Sim=-0.0072]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 38/200:  80%|  | 4/5 [00:02<00:00,  1.89it/s, D_loss=0.8669, G_loss=52.0226, Cos_Sim=-0.0148]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 38/200: 100%|| 5/5 [00:02<00:00,  1.84it/s, D_loss=0.8244, G_loss=57.4348, Cos_Sim=-0.0058]\n",
      "2025-09-16 12:08:52,952 - INFO - Epoch 38: D_loss=0.8558, G_loss=49.1284, Avg Cos_Sim=0.0019\n",
      "Epoch 39/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 39/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.8857, G_loss=45.3991, Cos_Sim=0.0070]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 39/200:  40%|      | 2/5 [00:01<00:01,  1.64it/s, D_loss=0.8551, G_loss=39.5718, Cos_Sim=-0.0392]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 39/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=0.8811, G_loss=48.2361, Cos_Sim=0.0157] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 39/200:  80%|  | 4/5 [00:02<00:00,  1.83it/s, D_loss=0.9581, G_loss=48.6834, Cos_Sim=-0.0178]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 39/200: 100%|| 5/5 [00:02<00:00,  1.74it/s, D_loss=0.8275, G_loss=53.4886, Cos_Sim=-0.0077]\n",
      "2025-09-16 12:08:56,075 - INFO - Epoch 39: D_loss=0.8815, G_loss=47.0758, Avg Cos_Sim=-0.0084\n",
      "Epoch 40/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 40/200:  20%|        | 1/5 [00:00<00:02,  1.45it/s, D_loss=0.9814, G_loss=47.5793, Cos_Sim=0.0064]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 40/200:  40%|      | 2/5 [00:01<00:01,  1.78it/s, D_loss=0.8568, G_loss=42.7274, Cos_Sim=0.0192]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 40/200:  60%|    | 3/5 [00:01<00:01,  1.93it/s, D_loss=0.8127, G_loss=42.5225, Cos_Sim=-0.0024]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 40/200:  80%|  | 4/5 [00:02<00:00,  1.97it/s, D_loss=0.8698, G_loss=47.3684, Cos_Sim=-0.0200]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 40/200: 100%|| 5/5 [00:02<00:00,  1.94it/s, D_loss=0.9248, G_loss=50.8969, Cos_Sim=0.0095] \n",
      "2025-09-16 12:08:58,896 - INFO - Epoch 40: D_loss=0.8891, G_loss=46.2189, Avg Cos_Sim=0.0025\n",
      "Epoch 41/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 41/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.8841, G_loss=42.6595, Cos_Sim=-0.0052]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 41/200:  40%|      | 2/5 [00:01<00:01,  1.63it/s, D_loss=0.9473, G_loss=45.3405, Cos_Sim=-0.0073]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 41/200:  60%|    | 3/5 [00:01<00:01,  1.81it/s, D_loss=0.8015, G_loss=43.7754, Cos_Sim=0.0133] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 41/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.8764, G_loss=47.8968, Cos_Sim=-0.0038]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 41/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=0.8837, G_loss=50.8544, Cos_Sim=0.0011] \n",
      "2025-09-16 12:09:01,930 - INFO - Epoch 41: D_loss=0.8786, G_loss=46.1053, Avg Cos_Sim=-0.0004\n",
      "Epoch 42/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 42/200:  20%|        | 1/5 [00:00<00:02,  1.38it/s, D_loss=0.8039, G_loss=48.4594, Cos_Sim=0.0165]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 42/200:  40%|      | 2/5 [00:01<00:01,  1.79it/s, D_loss=0.8695, G_loss=43.5795, Cos_Sim=-0.0074]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 42/200:  60%|    | 3/5 [00:01<00:01,  1.90it/s, D_loss=0.9500, G_loss=45.1463, Cos_Sim=-0.0036]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 42/200:  80%|  | 4/5 [00:02<00:00,  1.91it/s, D_loss=0.8512, G_loss=40.5720, Cos_Sim=0.0222] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 42/200: 100%|| 5/5 [00:02<00:00,  1.88it/s, D_loss=0.8201, G_loss=48.7592, Cos_Sim=0.0181]\n",
      "2025-09-16 12:09:04,842 - INFO - Epoch 42: D_loss=0.8589, G_loss=45.3033, Avg Cos_Sim=0.0092\n",
      "Epoch 43/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 43/200:  20%|        | 1/5 [00:00<00:02,  1.38it/s, D_loss=0.8260, G_loss=47.5929, Cos_Sim=-0.0077]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 43/200:  40%|      | 2/5 [00:01<00:01,  1.64it/s, D_loss=0.8513, G_loss=41.1092, Cos_Sim=0.0191] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 43/200:  60%|    | 3/5 [00:01<00:01,  1.81it/s, D_loss=0.8415, G_loss=46.6159, Cos_Sim=0.0138]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 43/200:  80%|  | 4/5 [00:02<00:00,  1.89it/s, D_loss=0.8739, G_loss=45.0252, Cos_Sim=0.0160]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 43/200: 100%|| 5/5 [00:02<00:00,  1.86it/s, D_loss=0.8813, G_loss=43.1965, Cos_Sim=0.0220]\n",
      "2025-09-16 12:09:07,782 - INFO - Epoch 43: D_loss=0.8548, G_loss=44.7079, Avg Cos_Sim=0.0126\n",
      "Epoch 44/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 44/200:  20%|        | 1/5 [00:00<00:03,  1.33it/s, D_loss=0.8607, G_loss=38.2464, Cos_Sim=0.0266]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 44/200:  40%|      | 2/5 [00:01<00:01,  1.68it/s, D_loss=0.9045, G_loss=40.7738, Cos_Sim=0.0096]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 44/200:  60%|    | 3/5 [00:01<00:01,  1.82it/s, D_loss=0.9376, G_loss=52.6525, Cos_Sim=-0.0245]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 44/200:  80%|  | 4/5 [00:02<00:00,  1.94it/s, D_loss=0.8221, G_loss=40.5543, Cos_Sim=-0.0064]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 44/200: 100%|| 5/5 [00:02<00:00,  1.84it/s, D_loss=0.8780, G_loss=50.9564, Cos_Sim=-0.0257]\n",
      "2025-09-16 12:09:10,751 - INFO - Epoch 44: D_loss=0.8806, G_loss=44.6367, Avg Cos_Sim=-0.0041\n",
      "Epoch 45/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 45/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.8406, G_loss=39.4825, Cos_Sim=-0.0259]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 45/200:  40%|      | 2/5 [00:01<00:01,  1.55it/s, D_loss=0.8128, G_loss=49.7901, Cos_Sim=-0.0062]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 45/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=0.9084, G_loss=42.6009, Cos_Sim=-0.0049]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 45/200:  80%|  | 4/5 [00:02<00:00,  1.87it/s, D_loss=0.8459, G_loss=45.7757, Cos_Sim=-0.0030]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 45/200: 100%|| 5/5 [00:02<00:00,  1.78it/s, D_loss=0.8340, G_loss=51.2634, Cos_Sim=0.0077] \n",
      "2025-09-16 12:09:13,814 - INFO - Epoch 45: D_loss=0.8484, G_loss=45.7825, Avg Cos_Sim=-0.0064\n",
      "Epoch 46/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 46/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.8783, G_loss=42.3788, Cos_Sim=-0.0507]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 46/200:  40%|      | 2/5 [00:01<00:01,  1.67it/s, D_loss=0.8143, G_loss=41.9180, Cos_Sim=0.0020] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 46/200:  60%|    | 3/5 [00:01<00:01,  1.81it/s, D_loss=0.8845, G_loss=44.0733, Cos_Sim=0.0354]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 46/200:  80%|  | 4/5 [00:02<00:00,  1.97it/s, D_loss=0.7976, G_loss=55.9205, Cos_Sim=-0.0069]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 46/200: 100%|| 5/5 [00:02<00:00,  1.88it/s, D_loss=0.8736, G_loss=40.6777, Cos_Sim=0.0139] \n",
      "2025-09-16 12:09:16,712 - INFO - Epoch 46: D_loss=0.8497, G_loss=44.9937, Avg Cos_Sim=-0.0013\n",
      "Epoch 47/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 47/200:  20%|        | 1/5 [00:00<00:02,  1.44it/s, D_loss=0.8402, G_loss=41.7792, Cos_Sim=0.0013]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 47/200:  40%|      | 2/5 [00:01<00:01,  1.74it/s, D_loss=0.8232, G_loss=47.5157, Cos_Sim=-0.0336]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 47/200:  60%|    | 3/5 [00:01<00:01,  1.90it/s, D_loss=0.8426, G_loss=48.7083, Cos_Sim=-0.0113]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 47/200:  80%|  | 4/5 [00:02<00:00,  2.00it/s, D_loss=0.8246, G_loss=45.6038, Cos_Sim=0.0101] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 47/200: 100%|| 5/5 [00:02<00:00,  1.92it/s, D_loss=0.9264, G_loss=45.4865, Cos_Sim=0.0162]\n",
      "2025-09-16 12:09:19,583 - INFO - Epoch 47: D_loss=0.8514, G_loss=45.8187, Avg Cos_Sim=-0.0035\n",
      "Epoch 48/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 48/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.8319, G_loss=38.0883, Cos_Sim=0.0156]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 48/200:  40%|      | 2/5 [00:01<00:01,  1.68it/s, D_loss=0.8417, G_loss=48.7963, Cos_Sim=0.0024]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 48/200:  60%|    | 3/5 [00:01<00:01,  1.80it/s, D_loss=0.9106, G_loss=46.0046, Cos_Sim=-0.0004]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 48/200:  80%|  | 4/5 [00:02<00:00,  1.90it/s, D_loss=0.8775, G_loss=42.6976, Cos_Sim=0.0028] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 48/200: 100%|| 5/5 [00:02<00:00,  1.85it/s, D_loss=0.8240, G_loss=37.9669, Cos_Sim=0.0175]\n",
      "2025-09-16 12:09:22,530 - INFO - Epoch 48: D_loss=0.8571, G_loss=42.7107, Avg Cos_Sim=0.0076\n",
      "Epoch 49/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 49/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.8765, G_loss=50.0450, Cos_Sim=0.0133]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 49/200:  40%|      | 2/5 [00:01<00:01,  1.68it/s, D_loss=0.8320, G_loss=41.1251, Cos_Sim=-0.0092]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 49/200:  60%|    | 3/5 [00:01<00:01,  1.80it/s, D_loss=0.8915, G_loss=39.6396, Cos_Sim=-0.0101]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 49/200:  80%|  | 4/5 [00:02<00:00,  1.92it/s, D_loss=0.8118, G_loss=43.9009, Cos_Sim=0.0024] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 49/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=0.7927, G_loss=44.2175, Cos_Sim=-0.0018]\n",
      "2025-09-16 12:09:25,582 - INFO - Epoch 49: D_loss=0.8409, G_loss=43.7856, Avg Cos_Sim=-0.0011\n",
      "Epoch 50/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 50/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.8574, G_loss=46.0978, Cos_Sim=0.0107]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 50/200:  40%|      | 2/5 [00:01<00:01,  1.67it/s, D_loss=0.8137, G_loss=45.6716, Cos_Sim=0.0077]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 50/200:  60%|    | 3/5 [00:01<00:01,  1.83it/s, D_loss=0.8110, G_loss=43.4894, Cos_Sim=0.0163]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 50/200:  80%|  | 4/5 [00:02<00:00,  1.93it/s, D_loss=0.8499, G_loss=54.9406, Cos_Sim=0.0143]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 50/200: 100%|| 5/5 [00:02<00:00,  1.88it/s, D_loss=0.9552, G_loss=46.0368, Cos_Sim=0.0268]\n",
      "2025-09-16 12:09:28,485 - INFO - Epoch 50: D_loss=0.8574, G_loss=47.2472, Avg Cos_Sim=0.0152\n",
      "Epoch 51/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 51/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.8976, G_loss=47.9923, Cos_Sim=0.0101]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 51/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.8354, G_loss=51.1044, Cos_Sim=0.0005]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 51/200:  60%|    | 3/5 [00:01<00:01,  1.80it/s, D_loss=0.8208, G_loss=45.5568, Cos_Sim=0.0034]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 51/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.8284, G_loss=43.3337, Cos_Sim=-0.0221]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 51/200: 100%|| 5/5 [00:02<00:00,  1.81it/s, D_loss=0.8627, G_loss=49.0009, Cos_Sim=0.0314] \n",
      "2025-09-16 12:09:31,497 - INFO - Epoch 51: D_loss=0.8490, G_loss=47.3976, Avg Cos_Sim=0.0047\n",
      "Epoch 52/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 52/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.8615, G_loss=53.3959, Cos_Sim=0.0018]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 52/200:  40%|      | 2/5 [00:01<00:01,  1.74it/s, D_loss=0.8589, G_loss=45.6590, Cos_Sim=-0.0171]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 52/200:  60%|    | 3/5 [00:01<00:01,  1.91it/s, D_loss=0.8177, G_loss=42.7232, Cos_Sim=0.0112] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 52/200:  80%|  | 4/5 [00:02<00:00,  1.99it/s, D_loss=0.9200, G_loss=43.1853, Cos_Sim=0.0131]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 52/200: 100%|| 5/5 [00:02<00:00,  1.89it/s, D_loss=0.8375, G_loss=42.1742, Cos_Sim=-0.0138]\n",
      "2025-09-16 12:09:34,406 - INFO - Epoch 52: D_loss=0.8591, G_loss=45.4275, Avg Cos_Sim=-0.0010\n",
      "Epoch 53/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 53/200:  20%|        | 1/5 [00:00<00:02,  1.46it/s, D_loss=0.8310, G_loss=50.6707, Cos_Sim=0.0060]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 53/200:  40%|      | 2/5 [00:01<00:01,  1.73it/s, D_loss=0.8118, G_loss=41.3700, Cos_Sim=-0.0289]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 53/200:  60%|    | 3/5 [00:01<00:01,  1.87it/s, D_loss=0.8784, G_loss=41.7963, Cos_Sim=0.0317] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 53/200:  80%|  | 4/5 [00:02<00:00,  1.94it/s, D_loss=0.9197, G_loss=48.0314, Cos_Sim=0.0346]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 53/200: 100%|| 5/5 [00:02<00:00,  1.91it/s, D_loss=0.8283, G_loss=45.9899, Cos_Sim=-0.0086]\n",
      "2025-09-16 12:09:37,282 - INFO - Epoch 53: D_loss=0.8538, G_loss=45.5717, Avg Cos_Sim=0.0070\n",
      "Epoch 54/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 54/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.9003, G_loss=44.7555, Cos_Sim=0.0122]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 54/200:  40%|      | 2/5 [00:01<00:01,  1.72it/s, D_loss=0.8397, G_loss=41.7121, Cos_Sim=0.0070]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 54/200:  60%|    | 3/5 [00:01<00:01,  1.93it/s, D_loss=0.8630, G_loss=45.9176, Cos_Sim=0.0155]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 54/200:  80%|  | 4/5 [00:02<00:00,  1.99it/s, D_loss=0.8949, G_loss=45.4837, Cos_Sim=0.0430]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 54/200: 100%|| 5/5 [00:02<00:00,  1.88it/s, D_loss=0.8898, G_loss=43.7828, Cos_Sim=0.0049]\n",
      "2025-09-16 12:09:40,195 - INFO - Epoch 54: D_loss=0.8776, G_loss=44.3303, Avg Cos_Sim=0.0165\n",
      "Epoch 55/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 55/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.8477, G_loss=46.0942, Cos_Sim=-0.0088]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 55/200:  40%|      | 2/5 [00:01<00:01,  1.53it/s, D_loss=0.8681, G_loss=41.8904, Cos_Sim=-0.0127]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 55/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.8234, G_loss=42.1591, Cos_Sim=-0.0001]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 55/200:  80%|  | 4/5 [00:02<00:00,  1.68it/s, D_loss=0.9156, G_loss=44.1930, Cos_Sim=-0.0317]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 55/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8198, G_loss=43.4054, Cos_Sim=0.0021] \n",
      "2025-09-16 12:09:43,426 - INFO - Epoch 55: D_loss=0.8549, G_loss=43.5484, Avg Cos_Sim=-0.0102\n",
      "Epoch 56/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 56/200:  20%|        | 1/5 [00:00<00:02,  1.36it/s, D_loss=0.8416, G_loss=48.0327, Cos_Sim=-0.0521]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 56/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8201, G_loss=42.8875, Cos_Sim=0.0355] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 56/200:  60%|    | 3/5 [00:01<00:01,  1.70it/s, D_loss=0.8237, G_loss=49.3371, Cos_Sim=-0.0061]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 56/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8503, G_loss=37.1629, Cos_Sim=-0.0258]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 56/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8423, G_loss=44.0499, Cos_Sim=-0.0101]\n",
      "2025-09-16 12:09:46,552 - INFO - Epoch 56: D_loss=0.8356, G_loss=44.2940, Avg Cos_Sim=-0.0117\n",
      "Epoch 57/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 57/200:  20%|        | 1/5 [00:00<00:03,  1.10it/s, D_loss=0.8642, G_loss=42.4817, Cos_Sim=-0.0203]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 57/200:  40%|      | 2/5 [00:01<00:02,  1.48it/s, D_loss=0.8356, G_loss=42.1661, Cos_Sim=0.0068] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 57/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.8467, G_loss=42.3370, Cos_Sim=0.0266]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 57/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8330, G_loss=52.6368, Cos_Sim=-0.0156]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 57/200: 100%|| 5/5 [00:03<00:00,  1.63it/s, D_loss=0.8067, G_loss=51.3755, Cos_Sim=-0.0012]\n",
      "2025-09-16 12:09:49,937 - INFO - Epoch 57: D_loss=0.8373, G_loss=46.1994, Avg Cos_Sim=-0.0007\n",
      "Epoch 58/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 58/200:  20%|        | 1/5 [00:00<00:03,  1.31it/s, D_loss=0.8937, G_loss=41.4435, Cos_Sim=0.0149]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 58/200:  40%|      | 2/5 [00:01<00:01,  1.67it/s, D_loss=0.8843, G_loss=43.3939, Cos_Sim=-0.0051]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 58/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8496, G_loss=51.2689, Cos_Sim=-0.0347]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 58/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.8783, G_loss=50.0573, Cos_Sim=-0.0034]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 58/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.8503, G_loss=50.5426, Cos_Sim=-0.0334]\n",
      "2025-09-16 12:09:53,123 - INFO - Epoch 58: D_loss=0.8712, G_loss=47.3412, Avg Cos_Sim=-0.0123\n",
      "Epoch 59/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 59/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.8533, G_loss=46.6354, Cos_Sim=-0.0164]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 59/200:  40%|      | 2/5 [00:01<00:01,  1.52it/s, D_loss=0.8455, G_loss=51.9653, Cos_Sim=-0.0213]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 59/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.8499, G_loss=49.1774, Cos_Sim=0.0311] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 59/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.8004, G_loss=47.4782, Cos_Sim=0.0184]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 59/200: 100%|| 5/5 [00:02<00:00,  1.72it/s, D_loss=0.7970, G_loss=44.8936, Cos_Sim=-0.0022]\n",
      "2025-09-16 12:09:56,320 - INFO - Epoch 59: D_loss=0.8292, G_loss=48.0300, Avg Cos_Sim=0.0019\n",
      "Epoch 60/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 60/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=0.8274, G_loss=51.3715, Cos_Sim=-0.0128]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 60/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8275, G_loss=50.4787, Cos_Sim=-0.0109]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 60/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.9100, G_loss=39.1766, Cos_Sim=-0.0078]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 60/200:  80%|  | 4/5 [00:02<00:00,  1.76it/s, D_loss=0.8527, G_loss=44.0015, Cos_Sim=0.0325] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 60/200: 100%|| 5/5 [00:02<00:00,  1.72it/s, D_loss=0.9011, G_loss=46.1371, Cos_Sim=-0.0012]\n",
      "2025-09-16 12:09:59,545 - INFO - Epoch 60: D_loss=0.8637, G_loss=46.2331, Avg Cos_Sim=-0.0000\n",
      "Epoch 61/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 61/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8209, G_loss=43.7636, Cos_Sim=-0.0158]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 61/200:  40%|      | 2/5 [00:01<00:01,  1.57it/s, D_loss=0.8341, G_loss=54.1160, Cos_Sim=-0.0166]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 61/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.9156, G_loss=51.4533, Cos_Sim=-0.0124]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 61/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.8235, G_loss=46.9185, Cos_Sim=-0.0051]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 61/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8348, G_loss=54.0034, Cos_Sim=0.0320] \n",
      "2025-09-16 12:10:02,655 - INFO - Epoch 61: D_loss=0.8458, G_loss=50.0510, Avg Cos_Sim=-0.0036\n",
      "Epoch 62/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 62/200:  20%|        | 1/5 [00:00<00:02,  1.34it/s, D_loss=0.7866, G_loss=42.9823, Cos_Sim=0.0334]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 62/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8433, G_loss=52.9485, Cos_Sim=-0.0036]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 62/200:  60%|    | 3/5 [00:01<00:01,  1.75it/s, D_loss=0.8176, G_loss=44.1777, Cos_Sim=-0.0202]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 62/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.8311, G_loss=49.6425, Cos_Sim=0.0106] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 62/200: 100%|| 5/5 [00:02<00:00,  1.76it/s, D_loss=0.8682, G_loss=45.1227, Cos_Sim=-0.0284]\n",
      "2025-09-16 12:10:05,792 - INFO - Epoch 62: D_loss=0.8293, G_loss=46.9747, Avg Cos_Sim=-0.0016\n",
      "Epoch 63/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 63/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=0.8259, G_loss=53.5624, Cos_Sim=0.0009]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 63/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.9169, G_loss=51.5877, Cos_Sim=-0.0204]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 63/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.8585, G_loss=40.5172, Cos_Sim=0.0608] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 63/200:  80%|  | 4/5 [00:02<00:00,  1.76it/s, D_loss=0.8964, G_loss=50.2811, Cos_Sim=0.0045]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 63/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.7948, G_loss=46.3245, Cos_Sim=-0.0027]\n",
      "2025-09-16 12:10:09,016 - INFO - Epoch 63: D_loss=0.8585, G_loss=48.4546, Avg Cos_Sim=0.0086\n",
      "Epoch 64/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 64/200:  20%|        | 1/5 [00:00<00:03,  1.17it/s, D_loss=0.8493, G_loss=50.7767, Cos_Sim=0.0208]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 64/200:  40%|      | 2/5 [00:01<00:01,  1.55it/s, D_loss=0.8590, G_loss=43.2615, Cos_Sim=0.0278]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 64/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.8160, G_loss=53.0896, Cos_Sim=0.0052]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 64/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.8501, G_loss=43.6094, Cos_Sim=0.0156]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 64/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8093, G_loss=46.8850, Cos_Sim=0.0126]\n",
      "2025-09-16 12:10:12,164 - INFO - Epoch 64: D_loss=0.8367, G_loss=47.5244, Avg Cos_Sim=0.0164\n",
      "Epoch 65/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 65/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=0.8038, G_loss=46.5369, Cos_Sim=-0.0056]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 65/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8526, G_loss=49.8552, Cos_Sim=-0.0186]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 65/200:  60%|    | 3/5 [00:01<00:01,  1.60it/s, D_loss=0.8060, G_loss=43.6727, Cos_Sim=0.0001] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 65/200:  80%|  | 4/5 [00:02<00:00,  1.70it/s, D_loss=0.8385, G_loss=43.2283, Cos_Sim=-0.0287]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 65/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8767, G_loss=46.6216, Cos_Sim=0.0313] \n",
      "2025-09-16 12:10:15,374 - INFO - Epoch 65: D_loss=0.8355, G_loss=45.9829, Avg Cos_Sim=-0.0043\n",
      "Epoch 66/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 66/200:  20%|        | 1/5 [00:00<00:02,  1.37it/s, D_loss=0.8528, G_loss=43.7284, Cos_Sim=0.0068]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 66/200:  40%|      | 2/5 [00:01<00:01,  1.66it/s, D_loss=0.8708, G_loss=51.1790, Cos_Sim=-0.0115]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 66/200:  60%|    | 3/5 [00:01<00:01,  1.64it/s, D_loss=0.8107, G_loss=40.3347, Cos_Sim=-0.0201]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 66/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8588, G_loss=48.9813, Cos_Sim=-0.0062]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 66/200: 100%|| 5/5 [00:02<00:00,  1.72it/s, D_loss=0.8066, G_loss=43.4042, Cos_Sim=-0.0282]\n",
      "2025-09-16 12:10:18,584 - INFO - Epoch 66: D_loss=0.8399, G_loss=45.5255, Avg Cos_Sim=-0.0118\n",
      "Epoch 67/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 67/200:  20%|        | 1/5 [00:00<00:03,  1.22it/s, D_loss=0.8228, G_loss=42.6493, Cos_Sim=0.0083]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 67/200:  40%|      | 2/5 [00:01<00:01,  1.58it/s, D_loss=0.8883, G_loss=35.7715, Cos_Sim=-0.0253]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 67/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.8164, G_loss=44.8474, Cos_Sim=-0.0181]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 67/200:  80%|  | 4/5 [00:02<00:00,  1.73it/s, D_loss=0.9420, G_loss=42.7430, Cos_Sim=0.0322] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 67/200: 100%|| 5/5 [00:02<00:00,  1.67it/s, D_loss=0.8515, G_loss=51.0103, Cos_Sim=0.0015]\n",
      "2025-09-16 12:10:21,853 - INFO - Epoch 67: D_loss=0.8642, G_loss=43.4043, Avg Cos_Sim=-0.0003\n",
      "Epoch 68/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 68/200:  20%|        | 1/5 [00:00<00:02,  1.33it/s, D_loss=0.9177, G_loss=45.2985, Cos_Sim=0.0415]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 68/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8699, G_loss=45.2285, Cos_Sim=-0.0005]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 68/200:  60%|    | 3/5 [00:01<00:01,  1.71it/s, D_loss=0.8342, G_loss=51.7160, Cos_Sim=0.0059] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 68/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8062, G_loss=41.2773, Cos_Sim=-0.0046]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 68/200: 100%|| 5/5 [00:02<00:00,  1.72it/s, D_loss=0.8786, G_loss=42.0652, Cos_Sim=0.0012] \n",
      "2025-09-16 12:10:25,027 - INFO - Epoch 68: D_loss=0.8613, G_loss=45.1171, Avg Cos_Sim=0.0087\n",
      "Epoch 69/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 69/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8846, G_loss=46.7278, Cos_Sim=-0.0176]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 69/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8052, G_loss=52.7920, Cos_Sim=0.0005] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 69/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=0.9077, G_loss=39.8117, Cos_Sim=0.0026]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 69/200:  80%|  | 4/5 [00:02<00:00,  1.81it/s, D_loss=0.8169, G_loss=55.3972, Cos_Sim=-0.0096]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 69/200: 100%|| 5/5 [00:02<00:00,  1.74it/s, D_loss=0.8291, G_loss=46.9360, Cos_Sim=0.0284] \n",
      "2025-09-16 12:10:28,189 - INFO - Epoch 69: D_loss=0.8487, G_loss=48.3329, Avg Cos_Sim=0.0008\n",
      "Epoch 70/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 70/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.8479, G_loss=37.4637, Cos_Sim=-0.0067]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 70/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=0.8851, G_loss=39.4344, Cos_Sim=-0.0191]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 70/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.8543, G_loss=47.0350, Cos_Sim=0.0265] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 70/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.8488, G_loss=42.3054, Cos_Sim=0.0195]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 70/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8207, G_loss=50.7855, Cos_Sim=-0.0012]\n",
      "2025-09-16 12:10:31,347 - INFO - Epoch 70: D_loss=0.8514, G_loss=43.4048, Avg Cos_Sim=0.0038\n",
      "Epoch 71/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 71/200:  20%|        | 1/5 [00:00<00:03,  1.13it/s, D_loss=0.8192, G_loss=37.2660, Cos_Sim=0.0378]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 71/200:  40%|      | 2/5 [00:01<00:01,  1.54it/s, D_loss=0.8127, G_loss=46.0133, Cos_Sim=-0.0035]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 71/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.8730, G_loss=45.8610, Cos_Sim=-0.0051]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 71/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8164, G_loss=49.5989, Cos_Sim=0.0073] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 71/200: 100%|| 5/5 [00:03<00:00,  1.65it/s, D_loss=0.8044, G_loss=45.9795, Cos_Sim=-0.0010]\n",
      "2025-09-16 12:10:34,636 - INFO - Epoch 71: D_loss=0.8251, G_loss=44.9437, Avg Cos_Sim=0.0071\n",
      "Epoch 72/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 72/200:  20%|        | 1/5 [00:00<00:02,  1.33it/s, D_loss=0.8145, G_loss=46.9599, Cos_Sim=-0.0065]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 72/200:  40%|      | 2/5 [00:01<00:01,  1.69it/s, D_loss=0.7988, G_loss=45.9430, Cos_Sim=-0.0102]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 72/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8069, G_loss=50.8318, Cos_Sim=-0.0201]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 72/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.8233, G_loss=41.8137, Cos_Sim=0.0289] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 72/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8256, G_loss=44.8364, Cos_Sim=-0.0040]\n",
      "2025-09-16 12:10:37,830 - INFO - Epoch 72: D_loss=0.8138, G_loss=46.0770, Avg Cos_Sim=-0.0024\n",
      "Epoch 73/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 73/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8183, G_loss=51.6326, Cos_Sim=0.0062]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 73/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8544, G_loss=47.6742, Cos_Sim=-0.0138]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 73/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=0.8454, G_loss=47.0518, Cos_Sim=0.0140] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 73/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.8872, G_loss=48.5609, Cos_Sim=0.0185]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 73/200: 100%|| 5/5 [00:02<00:00,  1.74it/s, D_loss=0.8175, G_loss=42.9906, Cos_Sim=0.0049]\n",
      "2025-09-16 12:10:40,982 - INFO - Epoch 73: D_loss=0.8446, G_loss=47.5820, Avg Cos_Sim=0.0060\n",
      "Epoch 74/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 74/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.8491, G_loss=41.5367, Cos_Sim=-0.0102]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 74/200:  40%|      | 2/5 [00:01<00:01,  1.53it/s, D_loss=0.8185, G_loss=48.2576, Cos_Sim=0.0110] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 74/200:  60%|    | 3/5 [00:01<00:01,  1.71it/s, D_loss=0.8816, G_loss=50.3957, Cos_Sim=0.0020]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 74/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.8010, G_loss=51.8067, Cos_Sim=-0.0001]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 74/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=0.8441, G_loss=48.0586, Cos_Sim=-0.0079]\n",
      "2025-09-16 12:10:44,113 - INFO - Epoch 74: D_loss=0.8389, G_loss=48.0111, Avg Cos_Sim=-0.0010\n",
      "Epoch 75/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 75/200:  20%|        | 1/5 [00:00<00:02,  1.35it/s, D_loss=0.8364, G_loss=53.1944, Cos_Sim=-0.0166]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 75/200:  40%|      | 2/5 [00:01<00:01,  1.55it/s, D_loss=0.8089, G_loss=45.6964, Cos_Sim=-0.0098]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 75/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.8071, G_loss=45.6753, Cos_Sim=0.0080] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 75/200:  80%|  | 4/5 [00:02<00:00,  1.83it/s, D_loss=0.8204, G_loss=47.6856, Cos_Sim=0.0063]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 75/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.7807, G_loss=45.5949, Cos_Sim=0.0102]\n",
      "2025-09-16 12:10:47,200 - INFO - Epoch 75: D_loss=0.8107, G_loss=47.5693, Avg Cos_Sim=-0.0004\n",
      "Epoch 76/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 76/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.7930, G_loss=52.4925, Cos_Sim=-0.0268]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 76/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=0.7894, G_loss=45.4982, Cos_Sim=-0.0449]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 76/200:  60%|    | 3/5 [00:01<00:01,  1.76it/s, D_loss=0.8564, G_loss=49.5579, Cos_Sim=-0.0198]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 76/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.9368, G_loss=46.9057, Cos_Sim=0.0283] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 76/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8236, G_loss=50.4038, Cos_Sim=0.0044]\n",
      "2025-09-16 12:10:50,331 - INFO - Epoch 76: D_loss=0.8398, G_loss=48.9716, Avg Cos_Sim=-0.0118\n",
      "Epoch 77/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 77/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=0.8413, G_loss=44.5352, Cos_Sim=0.0253]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 77/200:  40%|      | 2/5 [00:01<00:01,  1.64it/s, D_loss=0.8126, G_loss=38.6275, Cos_Sim=0.0368]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 77/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.8271, G_loss=49.7253, Cos_Sim=0.0212]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 77/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.7971, G_loss=37.7411, Cos_Sim=-0.0105]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 77/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8432, G_loss=56.4563, Cos_Sim=0.0163] \n",
      "2025-09-16 12:10:53,559 - INFO - Epoch 77: D_loss=0.8243, G_loss=45.4170, Avg Cos_Sim=0.0178\n",
      "Epoch 78/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 78/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.7961, G_loss=40.8013, Cos_Sim=-0.0246]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 78/200:  40%|      | 2/5 [00:01<00:01,  1.57it/s, D_loss=0.8287, G_loss=42.2524, Cos_Sim=-0.0038]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 78/200:  60%|    | 3/5 [00:01<00:01,  1.72it/s, D_loss=0.8548, G_loss=50.1462, Cos_Sim=0.0006] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 78/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8393, G_loss=44.6712, Cos_Sim=0.0194]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 78/200: 100%|| 5/5 [00:03<00:00,  1.65it/s, D_loss=0.8633, G_loss=51.6653, Cos_Sim=-0.0253]\n",
      "2025-09-16 12:10:56,847 - INFO - Epoch 78: D_loss=0.8364, G_loss=45.9073, Avg Cos_Sim=-0.0067\n",
      "Epoch 79/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 79/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=0.8415, G_loss=38.0916, Cos_Sim=0.0444]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 79/200:  40%|      | 2/5 [00:01<00:01,  1.64it/s, D_loss=0.8108, G_loss=47.5210, Cos_Sim=0.0018]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 79/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8357, G_loss=43.4890, Cos_Sim=-0.0123]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 79/200:  80%|  | 4/5 [00:02<00:00,  1.81it/s, D_loss=0.8194, G_loss=47.8562, Cos_Sim=-0.0112]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 79/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8395, G_loss=48.9077, Cos_Sim=-0.0158]\n",
      "2025-09-16 12:10:59,976 - INFO - Epoch 79: D_loss=0.8294, G_loss=45.1731, Avg Cos_Sim=0.0014\n",
      "Epoch 80/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 80/200:  20%|        | 1/5 [00:00<00:03,  1.17it/s, D_loss=0.8881, G_loss=50.8014, Cos_Sim=-0.0163]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 80/200:  40%|      | 2/5 [00:01<00:01,  1.57it/s, D_loss=0.8662, G_loss=48.5209, Cos_Sim=-0.0032]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 80/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.8065, G_loss=51.4438, Cos_Sim=-0.0203]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 80/200:  80%|  | 4/5 [00:02<00:00,  1.89it/s, D_loss=0.8574, G_loss=51.3442, Cos_Sim=0.0131] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 80/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8219, G_loss=43.3723, Cos_Sim=0.0117]\n",
      "2025-09-16 12:11:03,123 - INFO - Epoch 80: D_loss=0.8480, G_loss=49.0965, Avg Cos_Sim=-0.0030\n",
      "Epoch 81/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 81/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=0.8045, G_loss=47.3778, Cos_Sim=0.0068]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 81/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8838, G_loss=46.5984, Cos_Sim=-0.0086]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 81/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.8950, G_loss=55.6206, Cos_Sim=-0.0122]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 81/200:  80%|  | 4/5 [00:02<00:00,  1.74it/s, D_loss=0.8308, G_loss=49.1494, Cos_Sim=0.0125] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 81/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.7692, G_loss=41.0757, Cos_Sim=0.0099]\n",
      "2025-09-16 12:11:06,327 - INFO - Epoch 81: D_loss=0.8367, G_loss=47.9644, Avg Cos_Sim=0.0017\n",
      "Epoch 82/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 82/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.8479, G_loss=44.4478, Cos_Sim=-0.0249]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 82/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.7990, G_loss=41.0724, Cos_Sim=-0.0057]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 82/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.8488, G_loss=57.6126, Cos_Sim=-0.0093]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 82/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.8058, G_loss=48.0363, Cos_Sim=-0.0031]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 82/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8281, G_loss=46.6919, Cos_Sim=-0.0156]\n",
      "2025-09-16 12:11:09,557 - INFO - Epoch 82: D_loss=0.8260, G_loss=47.5722, Avg Cos_Sim=-0.0117\n",
      "Epoch 83/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 83/200:  20%|        | 1/5 [00:00<00:03,  1.19it/s, D_loss=0.8036, G_loss=49.6881, Cos_Sim=0.0136]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 83/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.8123, G_loss=53.0990, Cos_Sim=-0.0277]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 83/200:  60%|    | 3/5 [00:01<00:01,  1.72it/s, D_loss=0.8624, G_loss=53.0412, Cos_Sim=-0.0269]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 83/200:  80%|  | 4/5 [00:02<00:00,  1.81it/s, D_loss=0.8396, G_loss=42.6668, Cos_Sim=-0.0037]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 83/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8165, G_loss=41.1117, Cos_Sim=0.0610] \n",
      "2025-09-16 12:11:12,767 - INFO - Epoch 83: D_loss=0.8269, G_loss=47.9214, Avg Cos_Sim=0.0033\n",
      "Epoch 84/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 84/200:  20%|        | 1/5 [00:00<00:03,  1.31it/s, D_loss=0.9064, G_loss=45.8481, Cos_Sim=0.0224]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 84/200:  40%|      | 2/5 [00:01<00:01,  1.66it/s, D_loss=0.8415, G_loss=42.6240, Cos_Sim=0.0148]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 84/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.7934, G_loss=46.9940, Cos_Sim=0.0042]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 84/200:  80%|  | 4/5 [00:02<00:00,  1.83it/s, D_loss=0.8486, G_loss=42.6472, Cos_Sim=-0.0058]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 84/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.9008, G_loss=48.5893, Cos_Sim=0.0155] \n",
      "2025-09-16 12:11:15,841 - INFO - Epoch 84: D_loss=0.8581, G_loss=45.3405, Avg Cos_Sim=0.0102\n",
      "Epoch 85/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 85/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.8397, G_loss=42.4143, Cos_Sim=0.0157]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 85/200:  40%|      | 2/5 [00:01<00:01,  1.65it/s, D_loss=0.8070, G_loss=57.3208, Cos_Sim=-0.0037]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 85/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.7924, G_loss=43.7438, Cos_Sim=0.0091] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 85/200:  80%|  | 4/5 [00:02<00:00,  1.74it/s, D_loss=0.8026, G_loss=45.7422, Cos_Sim=0.0138]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 85/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8090, G_loss=44.3695, Cos_Sim=-0.0268]\n",
      "2025-09-16 12:11:19,075 - INFO - Epoch 85: D_loss=0.8101, G_loss=46.7181, Avg Cos_Sim=0.0016\n",
      "Epoch 86/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 86/200:  20%|        | 1/5 [00:00<00:02,  1.33it/s, D_loss=0.8084, G_loss=42.1778, Cos_Sim=0.0070]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 86/200:  40%|      | 2/5 [00:01<00:01,  1.66it/s, D_loss=0.8527, G_loss=54.2992, Cos_Sim=-0.0001]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 86/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=0.8944, G_loss=46.2846, Cos_Sim=0.0131] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 86/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.8285, G_loss=41.1149, Cos_Sim=0.0050]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 86/200: 100%|| 5/5 [00:02<00:00,  1.76it/s, D_loss=0.8455, G_loss=43.7219, Cos_Sim=0.0084]\n",
      "2025-09-16 12:11:22,199 - INFO - Epoch 86: D_loss=0.8459, G_loss=45.5197, Avg Cos_Sim=0.0067\n",
      "Epoch 87/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 87/200:  20%|        | 1/5 [00:00<00:03,  1.15it/s, D_loss=0.9168, G_loss=41.3119, Cos_Sim=-0.0165]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 87/200:  40%|      | 2/5 [00:01<00:01,  1.53it/s, D_loss=0.8188, G_loss=45.6033, Cos_Sim=-0.0041]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 87/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8588, G_loss=47.9368, Cos_Sim=0.0252] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 87/200:  80%|  | 4/5 [00:02<00:00,  1.81it/s, D_loss=0.8426, G_loss=41.2315, Cos_Sim=-0.0108]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 87/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8897, G_loss=51.3738, Cos_Sim=-0.0073]\n",
      "2025-09-16 12:11:25,364 - INFO - Epoch 87: D_loss=0.8653, G_loss=45.4915, Avg Cos_Sim=-0.0027\n",
      "Epoch 88/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 88/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8004, G_loss=51.0209, Cos_Sim=0.0133]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 88/200:  40%|      | 2/5 [00:01<00:01,  1.52it/s, D_loss=0.8297, G_loss=42.4451, Cos_Sim=0.0001]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 88/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.7904, G_loss=46.2096, Cos_Sim=0.0504]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 88/200:  80%|  | 4/5 [00:02<00:00,  1.72it/s, D_loss=0.8240, G_loss=46.2694, Cos_Sim=-0.0174]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 88/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.8411, G_loss=57.2575, Cos_Sim=-0.0004]\n",
      "2025-09-16 12:11:28,584 - INFO - Epoch 88: D_loss=0.8171, G_loss=48.6405, Avg Cos_Sim=0.0092\n",
      "Epoch 89/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 89/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=0.8519, G_loss=45.3177, Cos_Sim=-0.0168]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 89/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.8853, G_loss=46.0658, Cos_Sim=0.0117] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 89/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=0.8417, G_loss=41.1885, Cos_Sim=0.0033]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 89/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.8237, G_loss=42.0524, Cos_Sim=-0.0053]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 89/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=0.8373, G_loss=43.1927, Cos_Sim=0.0077] \n",
      "2025-09-16 12:11:31,667 - INFO - Epoch 89: D_loss=0.8480, G_loss=43.5634, Avg Cos_Sim=0.0001\n",
      "Epoch 90/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 90/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.7937, G_loss=51.1317, Cos_Sim=-0.0039]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 90/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8595, G_loss=51.9207, Cos_Sim=0.0145] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 90/200:  60%|    | 3/5 [00:01<00:01,  1.71it/s, D_loss=0.7918, G_loss=50.6597, Cos_Sim=0.0026]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 90/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8210, G_loss=45.6288, Cos_Sim=0.0362]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 90/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8949, G_loss=43.3568, Cos_Sim=0.0194]\n",
      "2025-09-16 12:11:34,896 - INFO - Epoch 90: D_loss=0.8322, G_loss=48.5395, Avg Cos_Sim=0.0138\n",
      "Epoch 91/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 91/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8525, G_loss=53.7104, Cos_Sim=-0.0171]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 91/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.7907, G_loss=46.3008, Cos_Sim=-0.0045]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 91/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.8492, G_loss=50.8099, Cos_Sim=0.0020] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 91/200:  80%|  | 4/5 [00:02<00:00,  1.78it/s, D_loss=0.9083, G_loss=44.9156, Cos_Sim=0.0045]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 91/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.9904, G_loss=45.9917, Cos_Sim=-0.0616]\n",
      "2025-09-16 12:11:38,094 - INFO - Epoch 91: D_loss=0.8782, G_loss=48.3457, Avg Cos_Sim=-0.0154\n",
      "Epoch 92/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 92/200:  20%|        | 1/5 [00:00<00:03,  1.22it/s, D_loss=0.8545, G_loss=40.4431, Cos_Sim=0.0096]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 92/200:  40%|      | 2/5 [00:01<00:01,  1.57it/s, D_loss=0.8075, G_loss=51.6366, Cos_Sim=0.0140]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 92/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8740, G_loss=39.8584, Cos_Sim=-0.0354]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 92/200:  80%|  | 4/5 [00:02<00:00,  1.83it/s, D_loss=0.8448, G_loss=53.0105, Cos_Sim=0.0211] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 92/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8906, G_loss=48.2097, Cos_Sim=0.0317]\n",
      "2025-09-16 12:11:41,280 - INFO - Epoch 92: D_loss=0.8543, G_loss=46.6317, Avg Cos_Sim=0.0082\n",
      "Epoch 93/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 93/200:  20%|        | 1/5 [00:00<00:02,  1.35it/s, D_loss=0.7929, G_loss=48.8808, Cos_Sim=0.0160]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 93/200:  40%|      | 2/5 [00:01<00:01,  1.68it/s, D_loss=0.8569, G_loss=53.5554, Cos_Sim=0.0421]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 93/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.8300, G_loss=38.6260, Cos_Sim=-0.0397]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 93/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.8382, G_loss=46.2486, Cos_Sim=0.0041] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 93/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8235, G_loss=40.7664, Cos_Sim=0.0078]\n",
      "2025-09-16 12:11:44,418 - INFO - Epoch 93: D_loss=0.8283, G_loss=45.6154, Avg Cos_Sim=0.0060\n",
      "Epoch 94/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 94/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.7706, G_loss=49.0269, Cos_Sim=-0.0223]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 94/200:  40%|      | 2/5 [00:01<00:02,  1.45it/s, D_loss=0.8545, G_loss=45.4500, Cos_Sim=0.0213] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 94/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.8457, G_loss=41.4069, Cos_Sim=-0.0054]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 94/200:  80%|  | 4/5 [00:02<00:00,  1.74it/s, D_loss=0.8378, G_loss=40.1466, Cos_Sim=0.0378] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 94/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8474, G_loss=49.1755, Cos_Sim=0.0081]\n",
      "2025-09-16 12:11:47,638 - INFO - Epoch 94: D_loss=0.8312, G_loss=45.0412, Avg Cos_Sim=0.0079\n",
      "Epoch 95/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 95/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8407, G_loss=40.0497, Cos_Sim=0.0286]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 95/200:  40%|      | 2/5 [00:01<00:01,  1.54it/s, D_loss=0.8253, G_loss=46.8268, Cos_Sim=-0.0139]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 95/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.8178, G_loss=39.5961, Cos_Sim=0.0078] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 95/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8132, G_loss=41.3362, Cos_Sim=-0.0310]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 95/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8514, G_loss=37.4617, Cos_Sim=0.0056] \n",
      "2025-09-16 12:11:50,947 - INFO - Epoch 95: D_loss=0.8297, G_loss=41.0541, Avg Cos_Sim=-0.0006\n",
      "Epoch 96/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 96/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.8341, G_loss=45.7212, Cos_Sim=0.0046]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 96/200:  40%|      | 2/5 [00:01<00:01,  1.51it/s, D_loss=0.8308, G_loss=50.4025, Cos_Sim=0.0048]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 96/200:  60%|    | 3/5 [00:01<00:01,  1.59it/s, D_loss=0.8302, G_loss=40.3019, Cos_Sim=0.0166]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 96/200:  80%|  | 4/5 [00:02<00:00,  1.72it/s, D_loss=0.8415, G_loss=48.0961, Cos_Sim=-0.0255]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 96/200: 100%|| 5/5 [00:03<00:00,  1.65it/s, D_loss=0.8123, G_loss=44.1609, Cos_Sim=-0.0121]\n",
      "2025-09-16 12:11:54,241 - INFO - Epoch 96: D_loss=0.8298, G_loss=45.7365, Avg Cos_Sim=-0.0023\n",
      "Epoch 97/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 97/200:  20%|        | 1/5 [00:00<00:03,  1.09it/s, D_loss=0.9006, G_loss=43.7711, Cos_Sim=-0.0171]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 97/200:  40%|      | 2/5 [00:01<00:02,  1.44it/s, D_loss=0.8422, G_loss=52.9236, Cos_Sim=-0.0010]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 97/200:  60%|    | 3/5 [00:02<00:01,  1.56it/s, D_loss=0.8693, G_loss=41.9603, Cos_Sim=0.0112] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 97/200:  80%|  | 4/5 [00:02<00:00,  1.67it/s, D_loss=0.8523, G_loss=44.3444, Cos_Sim=-0.0243]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 97/200: 100%|| 5/5 [00:03<00:00,  1.59it/s, D_loss=0.7882, G_loss=50.4042, Cos_Sim=-0.0242]\n",
      "2025-09-16 12:11:57,647 - INFO - Epoch 97: D_loss=0.8505, G_loss=46.6807, Avg Cos_Sim=-0.0111\n",
      "Epoch 98/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 98/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.7900, G_loss=48.3580, Cos_Sim=0.0016]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 98/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.9569, G_loss=47.6164, Cos_Sim=0.0117]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 98/200:  60%|    | 3/5 [00:01<00:01,  1.64it/s, D_loss=0.8155, G_loss=46.3436, Cos_Sim=-0.0135]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 98/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8503, G_loss=45.7013, Cos_Sim=-0.0068]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 98/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8658, G_loss=47.4673, Cos_Sim=-0.0066]\n",
      "2025-09-16 12:12:00,871 - INFO - Epoch 98: D_loss=0.8557, G_loss=47.0973, Avg Cos_Sim=-0.0027\n",
      "Epoch 99/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 99/200:  20%|        | 1/5 [00:00<00:03,  1.13it/s, D_loss=0.9344, G_loss=39.2366, Cos_Sim=0.0085]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 99/200:  40%|      | 2/5 [00:01<00:01,  1.54it/s, D_loss=0.8085, G_loss=53.0472, Cos_Sim=-0.0066]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 99/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8205, G_loss=43.4488, Cos_Sim=0.0338] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 99/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.8255, G_loss=45.0784, Cos_Sim=0.0356]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 99/200: 100%|| 5/5 [00:03<00:00,  1.66it/s, D_loss=0.8223, G_loss=58.4124, Cos_Sim=0.0142]\n",
      "2025-09-16 12:12:04,162 - INFO - Epoch 99: D_loss=0.8422, G_loss=47.8447, Avg Cos_Sim=0.0171\n",
      "Epoch 100/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 100/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8033, G_loss=47.7507, Cos_Sim=-0.0054]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 100/200:  40%|      | 2/5 [00:01<00:01,  1.54it/s, D_loss=0.8123, G_loss=45.8143, Cos_Sim=0.0160] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 100/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.7943, G_loss=40.1304, Cos_Sim=0.0066]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 100/200:  80%|  | 4/5 [00:02<00:00,  1.81it/s, D_loss=0.8507, G_loss=43.4773, Cos_Sim=0.0151]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 100/200: 100%|| 5/5 [00:02<00:00,  1.72it/s, D_loss=0.8524, G_loss=49.0094, Cos_Sim=-0.0034]\n",
      "2025-09-16 12:12:07,357 - INFO - Epoch 100: D_loss=0.8226, G_loss=45.2364, Avg Cos_Sim=0.0058\n",
      "Epoch 101/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 101/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.8599, G_loss=48.0761, Cos_Sim=0.0126]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 101/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8407, G_loss=40.2249, Cos_Sim=-0.0200]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 101/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.8022, G_loss=41.6813, Cos_Sim=-0.0076]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 101/200:  80%|  | 4/5 [00:02<00:00,  1.76it/s, D_loss=0.8048, G_loss=50.6569, Cos_Sim=-0.0269]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 101/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8109, G_loss=46.6119, Cos_Sim=0.0474] \n",
      "2025-09-16 12:12:10,606 - INFO - Epoch 101: D_loss=0.8237, G_loss=45.4502, Avg Cos_Sim=0.0011\n",
      "Epoch 102/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 102/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.8327, G_loss=44.9024, Cos_Sim=-0.0348]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 102/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8170, G_loss=45.3976, Cos_Sim=0.0102] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 102/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.8874, G_loss=50.7491, Cos_Sim=0.0032]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 102/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8878, G_loss=37.8003, Cos_Sim=-0.0227]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 102/200: 100%|| 5/5 [00:02<00:00,  1.78it/s, D_loss=0.9138, G_loss=44.5640, Cos_Sim=-0.0220]\n",
      "2025-09-16 12:12:13,687 - INFO - Epoch 102: D_loss=0.8677, G_loss=44.6827, Avg Cos_Sim=-0.0132\n",
      "Epoch 103/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 103/200:  20%|        | 1/5 [00:00<00:02,  1.35it/s, D_loss=0.8300, G_loss=43.0928, Cos_Sim=-0.0067]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 103/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=0.8190, G_loss=50.7615, Cos_Sim=-0.0194]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 103/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=0.8928, G_loss=49.8870, Cos_Sim=0.0011] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 103/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8653, G_loss=48.8173, Cos_Sim=-0.0048]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 103/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.8199, G_loss=40.9629, Cos_Sim=0.0014] \n",
      "2025-09-16 12:12:16,883 - INFO - Epoch 103: D_loss=0.8454, G_loss=46.7043, Avg Cos_Sim=-0.0057\n",
      "Epoch 104/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 104/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.7972, G_loss=41.3270, Cos_Sim=0.0027]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 104/200:  40%|      | 2/5 [00:01<00:01,  1.66it/s, D_loss=0.8423, G_loss=47.3649, Cos_Sim=-0.0112]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 104/200:  60%|    | 3/5 [00:01<00:01,  1.79it/s, D_loss=0.8322, G_loss=39.1717, Cos_Sim=-0.0354]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 104/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.8498, G_loss=47.7489, Cos_Sim=-0.0116]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 104/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8594, G_loss=41.1720, Cos_Sim=0.0333] \n",
      "2025-09-16 12:12:20,046 - INFO - Epoch 104: D_loss=0.8362, G_loss=43.3569, Avg Cos_Sim=-0.0044\n",
      "Epoch 105/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 105/200:  20%|        | 1/5 [00:00<00:02,  1.39it/s, D_loss=0.8069, G_loss=54.0399, Cos_Sim=0.0024]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 105/200:  40%|      | 2/5 [00:01<00:01,  1.70it/s, D_loss=0.8201, G_loss=47.0508, Cos_Sim=0.0355]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 105/200:  60%|    | 3/5 [00:01<00:01,  1.78it/s, D_loss=0.7952, G_loss=42.5411, Cos_Sim=-0.0085]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 105/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8387, G_loss=47.1281, Cos_Sim=-0.0111]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 105/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8789, G_loss=45.9619, Cos_Sim=-0.0432]\n",
      "2025-09-16 12:12:23,191 - INFO - Epoch 105: D_loss=0.8280, G_loss=47.3444, Avg Cos_Sim=-0.0050\n",
      "Epoch 106/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 106/200:  20%|        | 1/5 [00:00<00:03,  1.13it/s, D_loss=0.8142, G_loss=40.8876, Cos_Sim=-0.0247]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 106/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.8341, G_loss=40.3690, Cos_Sim=-0.0046]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 106/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.7915, G_loss=44.6950, Cos_Sim=-0.0092]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 106/200:  80%|  | 4/5 [00:02<00:00,  1.76it/s, D_loss=0.8353, G_loss=38.0527, Cos_Sim=0.0286] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 106/200: 100%|| 5/5 [00:03<00:00,  1.63it/s, D_loss=0.9241, G_loss=47.4905, Cos_Sim=0.0177]\n",
      "2025-09-16 12:12:26,536 - INFO - Epoch 106: D_loss=0.8398, G_loss=42.2990, Avg Cos_Sim=0.0016\n",
      "Epoch 107/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 107/200:  20%|        | 1/5 [00:00<00:03,  1.22it/s, D_loss=0.8273, G_loss=48.2615, Cos_Sim=-0.0048]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 107/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8030, G_loss=47.7112, Cos_Sim=-0.0036]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 107/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8280, G_loss=47.9252, Cos_Sim=0.0331] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 107/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.8326, G_loss=44.8170, Cos_Sim=0.0307]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 107/200: 100%|| 5/5 [00:02<00:00,  1.67it/s, D_loss=0.7937, G_loss=37.0812, Cos_Sim=0.0086]\n",
      "2025-09-16 12:12:29,821 - INFO - Epoch 107: D_loss=0.8169, G_loss=45.1592, Avg Cos_Sim=0.0128\n",
      "Epoch 108/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 108/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.8486, G_loss=47.0344, Cos_Sim=-0.0038]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 108/200:  40%|      | 2/5 [00:01<00:01,  1.64it/s, D_loss=0.8141, G_loss=52.7189, Cos_Sim=0.0356] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 108/200:  60%|    | 3/5 [00:01<00:01,  1.80it/s, D_loss=0.8067, G_loss=45.4763, Cos_Sim=0.0002]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 108/200:  80%|  | 4/5 [00:02<00:00,  1.85it/s, D_loss=0.8557, G_loss=50.1466, Cos_Sim=-0.0146]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 108/200: 100%|| 5/5 [00:02<00:00,  1.72it/s, D_loss=0.7845, G_loss=47.1040, Cos_Sim=0.0028] \n",
      "2025-09-16 12:12:32,994 - INFO - Epoch 108: D_loss=0.8219, G_loss=48.4960, Avg Cos_Sim=0.0041\n",
      "Epoch 109/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 109/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.9199, G_loss=45.5734, Cos_Sim=0.0373]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 109/200:  40%|      | 2/5 [00:01<00:01,  1.64it/s, D_loss=0.8802, G_loss=43.4131, Cos_Sim=0.0301]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 109/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8637, G_loss=53.7347, Cos_Sim=0.0081]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 109/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.7832, G_loss=47.8007, Cos_Sim=-0.0074]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 109/200: 100%|| 5/5 [00:02<00:00,  1.74it/s, D_loss=0.8008, G_loss=37.6516, Cos_Sim=0.0040] \n",
      "2025-09-16 12:12:36,141 - INFO - Epoch 109: D_loss=0.8495, G_loss=45.6347, Avg Cos_Sim=0.0144\n",
      "Epoch 110/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 110/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=0.8392, G_loss=41.6122, Cos_Sim=0.0290]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 110/200:  40%|      | 2/5 [00:01<00:01,  1.58it/s, D_loss=0.8762, G_loss=47.8690, Cos_Sim=0.0107]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 110/200:  60%|    | 3/5 [00:01<00:01,  1.65it/s, D_loss=0.8510, G_loss=49.9735, Cos_Sim=-0.0228]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 110/200:  80%|  | 4/5 [00:02<00:00,  1.73it/s, D_loss=0.8101, G_loss=45.2634, Cos_Sim=-0.0291]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 110/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8266, G_loss=53.3624, Cos_Sim=-0.0109]\n",
      "2025-09-16 12:12:39,387 - INFO - Epoch 110: D_loss=0.8406, G_loss=47.6161, Avg Cos_Sim=-0.0046\n",
      "Epoch 111/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 111/200:  20%|        | 1/5 [00:00<00:03,  1.22it/s, D_loss=0.8275, G_loss=60.1245, Cos_Sim=0.0086]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 111/200:  40%|      | 2/5 [00:01<00:01,  1.63it/s, D_loss=0.8308, G_loss=45.5173, Cos_Sim=0.0103]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 111/200:  60%|    | 3/5 [00:01<00:01,  1.78it/s, D_loss=0.8162, G_loss=45.9932, Cos_Sim=0.0072]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 111/200:  80%|  | 4/5 [00:02<00:00,  1.83it/s, D_loss=0.8758, G_loss=45.3531, Cos_Sim=0.0156]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 111/200: 100%|| 5/5 [00:02<00:00,  1.76it/s, D_loss=0.8446, G_loss=44.1654, Cos_Sim=-0.0149]\n",
      "2025-09-16 12:12:42,519 - INFO - Epoch 111: D_loss=0.8390, G_loss=48.2307, Avg Cos_Sim=0.0053\n",
      "Epoch 112/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 112/200:  20%|        | 1/5 [00:00<00:02,  1.36it/s, D_loss=0.8358, G_loss=49.7360, Cos_Sim=-0.0196]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 112/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8070, G_loss=34.2747, Cos_Sim=-0.0195]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 112/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8132, G_loss=45.7448, Cos_Sim=0.0120] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 112/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.8207, G_loss=47.1742, Cos_Sim=-0.0004]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 112/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.7942, G_loss=43.5912, Cos_Sim=-0.0145]\n",
      "2025-09-16 12:12:45,709 - INFO - Epoch 112: D_loss=0.8142, G_loss=44.1042, Avg Cos_Sim=-0.0084\n",
      "Epoch 113/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 113/200:  20%|        | 1/5 [00:00<00:03,  1.22it/s, D_loss=0.8357, G_loss=50.4118, Cos_Sim=-0.0138]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 113/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.8861, G_loss=45.7994, Cos_Sim=0.0151] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 113/200:  60%|    | 3/5 [00:01<00:01,  1.80it/s, D_loss=0.9135, G_loss=47.5809, Cos_Sim=0.0083]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 113/200:  80%|  | 4/5 [00:02<00:00,  1.84it/s, D_loss=0.8150, G_loss=40.4706, Cos_Sim=-0.0020]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 113/200: 100%|| 5/5 [00:02<00:00,  1.76it/s, D_loss=0.7822, G_loss=39.8246, Cos_Sim=-0.0269]\n",
      "2025-09-16 12:12:48,885 - INFO - Epoch 113: D_loss=0.8465, G_loss=44.8175, Avg Cos_Sim=-0.0039\n",
      "Epoch 114/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 114/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.8124, G_loss=41.1755, Cos_Sim=0.0304]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 114/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.8494, G_loss=42.6489, Cos_Sim=0.0051]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 114/200:  60%|    | 3/5 [00:01<00:01,  1.60it/s, D_loss=0.8362, G_loss=46.0302, Cos_Sim=-0.0371]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 114/200:  80%|  | 4/5 [00:02<00:00,  1.65it/s, D_loss=0.8001, G_loss=47.1791, Cos_Sim=-0.0206]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 114/200: 100%|| 5/5 [00:03<00:00,  1.64it/s, D_loss=0.8224, G_loss=56.1292, Cos_Sim=0.0009] \n",
      "2025-09-16 12:12:52,197 - INFO - Epoch 114: D_loss=0.8241, G_loss=46.6326, Avg Cos_Sim=-0.0043\n",
      "Epoch 115/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 115/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.8037, G_loss=48.6634, Cos_Sim=0.0386]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 115/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.8033, G_loss=52.2649, Cos_Sim=0.0199]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 115/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.8348, G_loss=46.5743, Cos_Sim=-0.0189]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 115/200:  80%|  | 4/5 [00:02<00:00,  1.72it/s, D_loss=0.8347, G_loss=49.4910, Cos_Sim=0.0103] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 115/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8333, G_loss=39.3300, Cos_Sim=-0.0486]\n",
      "2025-09-16 12:12:55,459 - INFO - Epoch 115: D_loss=0.8220, G_loss=47.2647, Avg Cos_Sim=0.0003\n",
      "Epoch 116/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 116/200:  20%|        | 1/5 [00:00<00:03,  1.19it/s, D_loss=0.8431, G_loss=37.4601, Cos_Sim=-0.0004]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 116/200:  40%|      | 2/5 [00:01<00:01,  1.55it/s, D_loss=0.8453, G_loss=46.6859, Cos_Sim=0.0154] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 116/200:  60%|    | 3/5 [00:01<00:01,  1.65it/s, D_loss=0.8782, G_loss=48.2164, Cos_Sim=0.0091]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 116/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8579, G_loss=40.3923, Cos_Sim=0.0104]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 116/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8426, G_loss=46.2059, Cos_Sim=0.0099]\n",
      "2025-09-16 12:12:58,785 - INFO - Epoch 116: D_loss=0.8534, G_loss=43.7921, Avg Cos_Sim=0.0089\n",
      "Epoch 117/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 117/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=0.8321, G_loss=47.2152, Cos_Sim=0.0340]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 117/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8072, G_loss=46.7700, Cos_Sim=-0.0201]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 117/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.9011, G_loss=48.3595, Cos_Sim=0.0171] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 117/200:  80%|  | 4/5 [00:02<00:00,  1.78it/s, D_loss=0.8100, G_loss=43.8323, Cos_Sim=-0.0047]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 117/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8181, G_loss=43.2224, Cos_Sim=-0.0140]\n",
      "2025-09-16 12:13:01,990 - INFO - Epoch 117: D_loss=0.8337, G_loss=45.8799, Avg Cos_Sim=0.0025\n",
      "Epoch 118/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 118/200:  20%|        | 1/5 [00:00<00:03,  1.20it/s, D_loss=0.7945, G_loss=42.5414, Cos_Sim=0.0253]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 118/200:  40%|      | 2/5 [00:01<00:01,  1.58it/s, D_loss=0.8190, G_loss=52.7828, Cos_Sim=-0.0187]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 118/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.8216, G_loss=46.4672, Cos_Sim=0.0113] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 118/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8574, G_loss=43.3236, Cos_Sim=0.0039]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 118/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.9000, G_loss=50.1021, Cos_Sim=-0.0042]\n",
      "2025-09-16 12:13:05,256 - INFO - Epoch 118: D_loss=0.8385, G_loss=47.0434, Avg Cos_Sim=0.0035\n",
      "Epoch 119/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 119/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.8628, G_loss=41.5918, Cos_Sim=0.0069]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 119/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.8362, G_loss=51.1994, Cos_Sim=0.0236]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 119/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.7833, G_loss=50.2720, Cos_Sim=-0.0084]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 119/200:  80%|  | 4/5 [00:02<00:00,  1.78it/s, D_loss=0.8541, G_loss=44.1483, Cos_Sim=-0.0285]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 119/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8125, G_loss=47.8401, Cos_Sim=0.0243] \n",
      "2025-09-16 12:13:08,496 - INFO - Epoch 119: D_loss=0.8298, G_loss=47.0103, Avg Cos_Sim=0.0036\n",
      "Epoch 120/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 120/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=0.8333, G_loss=40.3770, Cos_Sim=0.0137]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 120/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.8308, G_loss=45.1341, Cos_Sim=0.0059]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 120/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.8310, G_loss=44.9820, Cos_Sim=0.0309]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 120/200:  80%|  | 4/5 [00:02<00:00,  1.74it/s, D_loss=0.8309, G_loss=48.2361, Cos_Sim=0.0024]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 120/200: 100%|| 5/5 [00:02<00:00,  1.67it/s, D_loss=0.8630, G_loss=42.2008, Cos_Sim=0.0053]\n",
      "2025-09-16 12:13:11,797 - INFO - Epoch 120: D_loss=0.8378, G_loss=44.1860, Avg Cos_Sim=0.0116\n",
      "Epoch 121/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 121/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=0.9088, G_loss=48.7743, Cos_Sim=-0.0054]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 121/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8521, G_loss=38.1675, Cos_Sim=-0.0023]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 121/200:  60%|    | 3/5 [00:01<00:01,  1.71it/s, D_loss=0.8311, G_loss=46.0995, Cos_Sim=-0.0050]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 121/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.9100, G_loss=42.2179, Cos_Sim=-0.0052]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 121/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8456, G_loss=47.4699, Cos_Sim=-0.0167]\n",
      "2025-09-16 12:13:14,967 - INFO - Epoch 121: D_loss=0.8695, G_loss=44.5458, Avg Cos_Sim=-0.0069\n",
      "Epoch 122/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 122/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.8592, G_loss=39.8769, Cos_Sim=-0.0214]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 122/200:  40%|      | 2/5 [00:01<00:01,  1.57it/s, D_loss=0.8506, G_loss=46.4884, Cos_Sim=-0.0165]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 122/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8189, G_loss=47.5621, Cos_Sim=-0.0294]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 122/200:  80%|  | 4/5 [00:02<00:00,  1.68it/s, D_loss=0.7894, G_loss=52.0412, Cos_Sim=-0.0119]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 122/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8263, G_loss=49.4925, Cos_Sim=0.0097] \n",
      "2025-09-16 12:13:18,261 - INFO - Epoch 122: D_loss=0.8289, G_loss=47.0922, Avg Cos_Sim=-0.0139\n",
      "Epoch 123/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 123/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.8417, G_loss=42.5827, Cos_Sim=-0.0002]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 123/200:  40%|      | 2/5 [00:01<00:02,  1.42it/s, D_loss=0.7845, G_loss=46.3761, Cos_Sim=-0.0343]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 123/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.9163, G_loss=46.3066, Cos_Sim=-0.0478]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 123/200:  80%|  | 4/5 [00:02<00:00,  1.68it/s, D_loss=0.8658, G_loss=45.7171, Cos_Sim=-0.0074]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 123/200: 100%|| 5/5 [00:02<00:00,  1.67it/s, D_loss=0.7977, G_loss=40.1283, Cos_Sim=0.0060] \n",
      "2025-09-16 12:13:21,549 - INFO - Epoch 123: D_loss=0.8412, G_loss=44.2222, Avg Cos_Sim=-0.0167\n",
      "Epoch 124/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 124/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8387, G_loss=37.2704, Cos_Sim=0.0366]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 124/200:  40%|      | 2/5 [00:01<00:01,  1.52it/s, D_loss=0.8859, G_loss=44.6736, Cos_Sim=0.0072]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 124/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.8318, G_loss=42.6043, Cos_Sim=-0.0064]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 124/200:  80%|  | 4/5 [00:02<00:00,  1.78it/s, D_loss=0.8827, G_loss=49.7829, Cos_Sim=-0.0154]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 124/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8649, G_loss=54.6357, Cos_Sim=0.0016] \n",
      "2025-09-16 12:13:24,753 - INFO - Epoch 124: D_loss=0.8608, G_loss=45.7934, Avg Cos_Sim=0.0047\n",
      "Epoch 125/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 125/200:  20%|        | 1/5 [00:00<00:03,  1.31it/s, D_loss=0.9256, G_loss=44.6290, Cos_Sim=0.0105]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 125/200:  40%|      | 2/5 [00:01<00:02,  1.50it/s, D_loss=0.8239, G_loss=49.5599, Cos_Sim=-0.0134]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 125/200:  60%|    | 3/5 [00:01<00:01,  1.71it/s, D_loss=0.9101, G_loss=45.7717, Cos_Sim=0.0119] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 125/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.7904, G_loss=47.1007, Cos_Sim=-0.0009]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 125/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8607, G_loss=37.9160, Cos_Sim=0.0046] \n",
      "2025-09-16 12:13:27,896 - INFO - Epoch 125: D_loss=0.8621, G_loss=44.9954, Avg Cos_Sim=0.0025\n",
      "Epoch 126/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 126/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8765, G_loss=46.8878, Cos_Sim=-0.0480]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 126/200:  40%|      | 2/5 [00:01<00:01,  1.64it/s, D_loss=0.9251, G_loss=46.1346, Cos_Sim=0.0220] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 126/200:  60%|    | 3/5 [00:01<00:01,  1.75it/s, D_loss=0.8212, G_loss=49.7320, Cos_Sim=0.0006]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 126/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8119, G_loss=39.7260, Cos_Sim=0.0081]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 126/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8236, G_loss=44.2772, Cos_Sim=-0.0011]\n",
      "2025-09-16 12:13:31,095 - INFO - Epoch 126: D_loss=0.8517, G_loss=45.3515, Avg Cos_Sim=-0.0037\n",
      "Epoch 127/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 127/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=0.8151, G_loss=41.6498, Cos_Sim=0.0259]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 127/200:  40%|      | 2/5 [00:01<00:01,  1.52it/s, D_loss=0.8202, G_loss=47.8491, Cos_Sim=-0.0390]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 127/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.8406, G_loss=52.4902, Cos_Sim=-0.0338]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 127/200:  80%|  | 4/5 [00:02<00:00,  1.62it/s, D_loss=0.8130, G_loss=43.2385, Cos_Sim=-0.0034]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 127/200: 100%|| 5/5 [00:03<00:00,  1.64it/s, D_loss=0.9290, G_loss=38.8747, Cos_Sim=-0.0217]\n",
      "2025-09-16 12:13:34,455 - INFO - Epoch 127: D_loss=0.8436, G_loss=44.8205, Avg Cos_Sim=-0.0144\n",
      "Epoch 128/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 128/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.8995, G_loss=45.5532, Cos_Sim=0.0143]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 128/200:  40%|      | 2/5 [00:01<00:02,  1.46it/s, D_loss=0.8372, G_loss=52.5143, Cos_Sim=-0.0351]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 128/200:  60%|    | 3/5 [00:01<00:01,  1.58it/s, D_loss=0.8804, G_loss=44.0571, Cos_Sim=0.0019] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 128/200:  80%|  | 4/5 [00:02<00:00,  1.71it/s, D_loss=0.8025, G_loss=42.4842, Cos_Sim=-0.0163]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 128/200: 100%|| 5/5 [00:03<00:00,  1.64it/s, D_loss=0.9171, G_loss=45.5442, Cos_Sim=0.0191] \n",
      "2025-09-16 12:13:37,792 - INFO - Epoch 128: D_loss=0.8674, G_loss=46.0306, Avg Cos_Sim=-0.0032\n",
      "Epoch 129/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 129/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.8163, G_loss=42.1919, Cos_Sim=-0.0205]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 129/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.8148, G_loss=50.6103, Cos_Sim=0.0148] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 129/200:  60%|    | 3/5 [00:01<00:01,  1.70it/s, D_loss=0.8380, G_loss=39.6225, Cos_Sim=0.0026]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 129/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.8087, G_loss=41.9917, Cos_Sim=0.0048]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 129/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.8209, G_loss=42.9710, Cos_Sim=0.0379]\n",
      "2025-09-16 12:13:41,010 - INFO - Epoch 129: D_loss=0.8197, G_loss=43.4775, Avg Cos_Sim=0.0079\n",
      "Epoch 130/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 130/200:  20%|        | 1/5 [00:00<00:03,  1.22it/s, D_loss=0.8334, G_loss=46.2575, Cos_Sim=0.0073]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 130/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8825, G_loss=50.6155, Cos_Sim=0.0513]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 130/200:  60%|    | 3/5 [00:01<00:01,  1.76it/s, D_loss=0.8302, G_loss=47.9549, Cos_Sim=-0.0183]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 130/200:  80%|  | 4/5 [00:02<00:00,  1.87it/s, D_loss=0.8100, G_loss=40.2100, Cos_Sim=-0.0088]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 130/200: 100%|| 5/5 [00:02<00:00,  1.78it/s, D_loss=0.8330, G_loss=45.1151, Cos_Sim=0.0292] \n",
      "2025-09-16 12:13:44,089 - INFO - Epoch 130: D_loss=0.8378, G_loss=46.0306, Avg Cos_Sim=0.0121\n",
      "Epoch 131/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 131/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.8883, G_loss=46.2139, Cos_Sim=0.0189]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 131/200:  40%|      | 2/5 [00:01<00:01,  1.53it/s, D_loss=0.8176, G_loss=50.7285, Cos_Sim=-0.0120]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 131/200:  60%|    | 3/5 [00:01<00:01,  1.71it/s, D_loss=0.8221, G_loss=42.8719, Cos_Sim=0.0221] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 131/200:  80%|  | 4/5 [00:02<00:00,  1.70it/s, D_loss=0.8314, G_loss=47.9931, Cos_Sim=-0.0506]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 131/200: 100%|| 5/5 [00:03<00:00,  1.66it/s, D_loss=0.7872, G_loss=47.3171, Cos_Sim=-0.0059]\n",
      "2025-09-16 12:13:47,403 - INFO - Epoch 131: D_loss=0.8293, G_loss=47.0249, Avg Cos_Sim=-0.0055\n",
      "Epoch 132/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 132/200:  20%|        | 1/5 [00:00<00:02,  1.37it/s, D_loss=0.8361, G_loss=43.7277, Cos_Sim=0.0147]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 132/200:  40%|      | 2/5 [00:01<00:01,  1.65it/s, D_loss=0.8124, G_loss=42.5593, Cos_Sim=-0.0004]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 132/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=0.8286, G_loss=42.5638, Cos_Sim=0.0249] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 132/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8124, G_loss=49.0510, Cos_Sim=-0.0000]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 132/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.8636, G_loss=47.6632, Cos_Sim=0.0449] \n",
      "2025-09-16 12:13:50,518 - INFO - Epoch 132: D_loss=0.8306, G_loss=45.1130, Avg Cos_Sim=0.0168\n",
      "Epoch 133/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 133/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=0.8173, G_loss=41.4345, Cos_Sim=0.0052]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 133/200:  40%|      | 2/5 [00:01<00:02,  1.48it/s, D_loss=0.7876, G_loss=47.7249, Cos_Sim=-0.0111]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 133/200:  60%|    | 3/5 [00:01<00:01,  1.71it/s, D_loss=0.8367, G_loss=41.2003, Cos_Sim=-0.0027]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 133/200:  80%|  | 4/5 [00:02<00:00,  1.76it/s, D_loss=0.8223, G_loss=43.6519, Cos_Sim=0.0089] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 133/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.8004, G_loss=42.0125, Cos_Sim=0.0118]\n",
      "2025-09-16 12:13:53,717 - INFO - Epoch 133: D_loss=0.8129, G_loss=43.2048, Avg Cos_Sim=0.0024\n",
      "Epoch 134/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 134/200:  20%|        | 1/5 [00:00<00:02,  1.35it/s, D_loss=0.7988, G_loss=39.8768, Cos_Sim=0.0370]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 134/200:  40%|      | 2/5 [00:01<00:01,  1.64it/s, D_loss=0.8669, G_loss=43.0260, Cos_Sim=0.0003]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 134/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.7985, G_loss=39.4366, Cos_Sim=0.0263]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 134/200:  80%|  | 4/5 [00:02<00:00,  1.78it/s, D_loss=0.8265, G_loss=43.7661, Cos_Sim=0.0086]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 134/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8075, G_loss=55.0048, Cos_Sim=0.0085]\n",
      "2025-09-16 12:13:56,882 - INFO - Epoch 134: D_loss=0.8196, G_loss=44.2221, Avg Cos_Sim=0.0161\n",
      "Epoch 135/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 135/200:  20%|        | 1/5 [00:00<00:03,  1.17it/s, D_loss=0.9076, G_loss=42.3400, Cos_Sim=-0.0318]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 135/200:  40%|      | 2/5 [00:01<00:01,  1.54it/s, D_loss=0.7814, G_loss=49.5677, Cos_Sim=-0.0183]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 135/200:  60%|    | 3/5 [00:01<00:01,  1.72it/s, D_loss=0.8040, G_loss=45.3292, Cos_Sim=-0.0002]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 135/200:  80%|  | 4/5 [00:02<00:00,  1.78it/s, D_loss=0.8470, G_loss=50.7000, Cos_Sim=-0.0107]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 135/200: 100%|| 5/5 [00:03<00:00,  1.65it/s, D_loss=0.8579, G_loss=52.4508, Cos_Sim=0.0085] \n",
      "2025-09-16 12:14:00,221 - INFO - Epoch 135: D_loss=0.8396, G_loss=48.0776, Avg Cos_Sim=-0.0105\n",
      "Epoch 136/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 136/200:  20%|        | 1/5 [00:00<00:02,  1.36it/s, D_loss=0.7950, G_loss=39.5378, Cos_Sim=0.0020]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 136/200:  40%|      | 2/5 [00:01<00:01,  1.70it/s, D_loss=0.8308, G_loss=46.0913, Cos_Sim=0.0013]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 136/200:  60%|    | 3/5 [00:01<00:01,  1.73it/s, D_loss=0.8079, G_loss=44.6559, Cos_Sim=0.0242]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 136/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.7912, G_loss=49.7304, Cos_Sim=-0.0155]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 136/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.8307, G_loss=38.3185, Cos_Sim=-0.0018]\n",
      "2025-09-16 12:14:03,317 - INFO - Epoch 136: D_loss=0.8111, G_loss=43.6668, Avg Cos_Sim=0.0020\n",
      "Epoch 137/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 137/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=0.8410, G_loss=45.6121, Cos_Sim=0.0141]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 137/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.8091, G_loss=45.6213, Cos_Sim=-0.0292]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 137/200:  60%|    | 3/5 [00:01<00:01,  1.59it/s, D_loss=0.7815, G_loss=47.8817, Cos_Sim=-0.0033]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 137/200:  80%|  | 4/5 [00:02<00:00,  1.71it/s, D_loss=0.8140, G_loss=48.2406, Cos_Sim=0.0242] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 137/200: 100%|| 5/5 [00:03<00:00,  1.66it/s, D_loss=0.8143, G_loss=52.9558, Cos_Sim=-0.0171]\n",
      "2025-09-16 12:14:06,586 - INFO - Epoch 137: D_loss=0.8120, G_loss=48.0623, Avg Cos_Sim=-0.0023\n",
      "Epoch 138/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 138/200:  20%|        | 1/5 [00:00<00:02,  1.33it/s, D_loss=0.8035, G_loss=35.9122, Cos_Sim=-0.0018]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 138/200:  40%|      | 2/5 [00:01<00:01,  1.65it/s, D_loss=0.9191, G_loss=43.6388, Cos_Sim=-0.0052]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 138/200:  60%|    | 3/5 [00:01<00:01,  1.76it/s, D_loss=0.8578, G_loss=54.9541, Cos_Sim=0.0185] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 138/200:  80%|  | 4/5 [00:02<00:00,  1.66it/s, D_loss=0.7999, G_loss=39.3990, Cos_Sim=-0.0177]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 138/200: 100%|| 5/5 [00:03<00:00,  1.65it/s, D_loss=0.8346, G_loss=42.6693, Cos_Sim=0.0062] \n",
      "2025-09-16 12:14:09,926 - INFO - Epoch 138: D_loss=0.8430, G_loss=43.3147, Avg Cos_Sim=-0.0000\n",
      "Epoch 139/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 139/200:  20%|        | 1/5 [00:00<00:03,  1.33it/s, D_loss=0.8785, G_loss=49.7491, Cos_Sim=-0.0094]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 139/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.7852, G_loss=41.5877, Cos_Sim=0.0241] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 139/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.8343, G_loss=47.2462, Cos_Sim=0.0118]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 139/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.8593, G_loss=45.4014, Cos_Sim=0.0404]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 139/200: 100%|| 5/5 [00:02<00:00,  1.74it/s, D_loss=0.8172, G_loss=48.3818, Cos_Sim=0.0301]\n",
      "2025-09-16 12:14:13,096 - INFO - Epoch 139: D_loss=0.8349, G_loss=46.4732, Avg Cos_Sim=0.0194\n",
      "Epoch 140/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 140/200:  20%|        | 1/5 [00:00<00:03,  1.20it/s, D_loss=0.8337, G_loss=48.8755, Cos_Sim=0.0137]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 140/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=0.8422, G_loss=48.7125, Cos_Sim=0.0079]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 140/200:  60%|    | 3/5 [00:01<00:01,  1.79it/s, D_loss=0.9243, G_loss=41.5500, Cos_Sim=0.0270]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 140/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.8348, G_loss=44.2559, Cos_Sim=-0.0072]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 140/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8521, G_loss=48.5698, Cos_Sim=-0.0072]\n",
      "2025-09-16 12:14:16,291 - INFO - Epoch 140: D_loss=0.8574, G_loss=46.3927, Avg Cos_Sim=0.0068\n",
      "Epoch 141/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 141/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.8355, G_loss=40.2717, Cos_Sim=0.0121]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 141/200:  40%|      | 2/5 [00:01<00:01,  1.51it/s, D_loss=0.8182, G_loss=48.0207, Cos_Sim=-0.0042]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 141/200:  60%|    | 3/5 [00:01<00:01,  1.58it/s, D_loss=0.8717, G_loss=46.4141, Cos_Sim=0.0087] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 141/200:  80%|  | 4/5 [00:02<00:00,  1.72it/s, D_loss=0.8042, G_loss=43.6749, Cos_Sim=0.0052]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 141/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8209, G_loss=49.7362, Cos_Sim=0.0026]\n",
      "2025-09-16 12:14:19,545 - INFO - Epoch 141: D_loss=0.8301, G_loss=45.6235, Avg Cos_Sim=0.0049\n",
      "Epoch 142/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 142/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.7917, G_loss=45.4548, Cos_Sim=0.0103]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 142/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.8522, G_loss=46.6165, Cos_Sim=0.0120]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 142/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.8024, G_loss=44.3330, Cos_Sim=0.0118]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 142/200:  80%|  | 4/5 [00:02<00:00,  1.76it/s, D_loss=0.8425, G_loss=50.9398, Cos_Sim=0.0230]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 142/200: 100%|| 5/5 [00:02<00:00,  1.67it/s, D_loss=0.8252, G_loss=42.2725, Cos_Sim=0.0064]\n",
      "2025-09-16 12:14:22,836 - INFO - Epoch 142: D_loss=0.8228, G_loss=45.9233, Avg Cos_Sim=0.0127\n",
      "Epoch 143/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 143/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.8357, G_loss=51.4406, Cos_Sim=0.0134]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 143/200:  40%|      | 2/5 [00:01<00:02,  1.48it/s, D_loss=0.8152, G_loss=46.4721, Cos_Sim=0.0026]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 143/200:  60%|    | 3/5 [00:01<00:01,  1.59it/s, D_loss=0.8197, G_loss=46.4671, Cos_Sim=0.0025]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 143/200:  80%|  | 4/5 [00:02<00:00,  1.73it/s, D_loss=0.8490, G_loss=38.6107, Cos_Sim=-0.0011]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 143/200: 100%|| 5/5 [00:02<00:00,  1.67it/s, D_loss=0.7867, G_loss=48.0558, Cos_Sim=0.0178] \n",
      "2025-09-16 12:14:26,148 - INFO - Epoch 143: D_loss=0.8213, G_loss=46.2093, Avg Cos_Sim=0.0071\n",
      "Epoch 144/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 144/200:  20%|        | 1/5 [00:00<00:03,  1.17it/s, D_loss=0.8664, G_loss=45.8820, Cos_Sim=-0.0424]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 144/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.7969, G_loss=42.6959, Cos_Sim=-0.0143]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 144/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.9052, G_loss=45.1458, Cos_Sim=0.0157] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 144/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.8770, G_loss=41.9029, Cos_Sim=0.0337]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 144/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8551, G_loss=45.2239, Cos_Sim=0.0021]\n",
      "2025-09-16 12:14:29,405 - INFO - Epoch 144: D_loss=0.8601, G_loss=44.1701, Avg Cos_Sim=-0.0010\n",
      "Epoch 145/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 145/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=0.7958, G_loss=56.2481, Cos_Sim=0.0106]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 145/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.8385, G_loss=41.6748, Cos_Sim=0.0534]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 145/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.8605, G_loss=41.4433, Cos_Sim=0.0058]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 145/200:  80%|  | 4/5 [00:02<00:00,  1.72it/s, D_loss=0.8106, G_loss=43.7527, Cos_Sim=0.0014]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 145/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8597, G_loss=39.7661, Cos_Sim=-0.0084]\n",
      "2025-09-16 12:14:32,625 - INFO - Epoch 145: D_loss=0.8330, G_loss=44.5770, Avg Cos_Sim=0.0126\n",
      "Epoch 146/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 146/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8103, G_loss=42.4896, Cos_Sim=-0.0423]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 146/200:  40%|      | 2/5 [00:01<00:02,  1.45it/s, D_loss=0.8165, G_loss=49.1072, Cos_Sim=0.0054] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 146/200:  60%|    | 3/5 [00:02<00:01,  1.54it/s, D_loss=0.8359, G_loss=40.4300, Cos_Sim=-0.0175]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 146/200:  80%|  | 4/5 [00:02<00:00,  1.68it/s, D_loss=0.8418, G_loss=47.9155, Cos_Sim=-0.0029]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 146/200: 100%|| 5/5 [00:03<00:00,  1.61it/s, D_loss=1.0077, G_loss=40.9578, Cos_Sim=-0.0468]\n",
      "2025-09-16 12:14:36,029 - INFO - Epoch 146: D_loss=0.8624, G_loss=44.1800, Avg Cos_Sim=-0.0208\n",
      "Epoch 147/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 147/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.8577, G_loss=42.8937, Cos_Sim=0.0200]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 147/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.7919, G_loss=50.0421, Cos_Sim=-0.0391]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 147/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.8245, G_loss=43.3810, Cos_Sim=-0.0128]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 147/200:  80%|  | 4/5 [00:02<00:00,  1.74it/s, D_loss=0.8056, G_loss=51.8867, Cos_Sim=0.0188] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 147/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8602, G_loss=52.4303, Cos_Sim=0.0076]\n",
      "2025-09-16 12:14:39,285 - INFO - Epoch 147: D_loss=0.8280, G_loss=48.1268, Avg Cos_Sim=-0.0011\n",
      "Epoch 148/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 148/200:  20%|        | 1/5 [00:00<00:03,  1.19it/s, D_loss=0.8751, G_loss=45.2916, Cos_Sim=-0.0100]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 148/200:  40%|      | 2/5 [00:01<00:01,  1.54it/s, D_loss=0.8540, G_loss=54.4139, Cos_Sim=-0.0200]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 148/200:  60%|    | 3/5 [00:01<00:01,  1.71it/s, D_loss=0.8787, G_loss=44.3211, Cos_Sim=-0.0111]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 148/200:  80%|  | 4/5 [00:02<00:00,  1.80it/s, D_loss=0.8064, G_loss=44.4292, Cos_Sim=0.0018] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 148/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.8925, G_loss=40.1870, Cos_Sim=-0.0317]\n",
      "2025-09-16 12:14:42,528 - INFO - Epoch 148: D_loss=0.8614, G_loss=45.7286, Avg Cos_Sim=-0.0142\n",
      "Epoch 149/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 149/200:  20%|        | 1/5 [00:00<00:03,  1.32it/s, D_loss=0.8446, G_loss=50.5126, Cos_Sim=0.0245]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 149/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=0.8531, G_loss=51.9308, Cos_Sim=-0.0145]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 149/200:  60%|    | 3/5 [00:01<00:01,  1.62it/s, D_loss=0.8628, G_loss=43.9892, Cos_Sim=0.0172] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 149/200:  80%|  | 4/5 [00:02<00:00,  1.69it/s, D_loss=0.8039, G_loss=45.8763, Cos_Sim=-0.0223]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 149/200: 100%|| 5/5 [00:03<00:00,  1.65it/s, D_loss=0.8206, G_loss=46.9935, Cos_Sim=-0.0157]\n",
      "2025-09-16 12:14:45,894 - INFO - Epoch 149: D_loss=0.8370, G_loss=47.8605, Avg Cos_Sim=-0.0022\n",
      "Epoch 150/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 150/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.7911, G_loss=50.1484, Cos_Sim=-0.0234]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 150/200:  40%|      | 2/5 [00:01<00:01,  1.53it/s, D_loss=0.8096, G_loss=45.0174, Cos_Sim=0.0123] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 150/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.8199, G_loss=50.6923, Cos_Sim=0.0299]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 150/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.9049, G_loss=44.9787, Cos_Sim=0.0001]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 150/200: 100%|| 5/5 [00:03<00:00,  1.64it/s, D_loss=0.8108, G_loss=44.2509, Cos_Sim=-0.0107]\n",
      "2025-09-16 12:14:49,230 - INFO - Epoch 150: D_loss=0.8273, G_loss=47.0175, Avg Cos_Sim=0.0016\n",
      "Epoch 151/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 151/200:  20%|        | 1/5 [00:00<00:03,  1.19it/s, D_loss=0.8500, G_loss=58.1883, Cos_Sim=-0.0006]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 151/200:  40%|      | 2/5 [00:01<00:01,  1.58it/s, D_loss=0.8333, G_loss=42.2833, Cos_Sim=-0.0478]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 151/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.8170, G_loss=42.8867, Cos_Sim=-0.0420]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 151/200:  80%|  | 4/5 [00:02<00:00,  1.72it/s, D_loss=0.8621, G_loss=45.0090, Cos_Sim=-0.0062]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 151/200: 100%|| 5/5 [00:03<00:00,  1.66it/s, D_loss=0.8506, G_loss=46.5494, Cos_Sim=-0.0038]\n",
      "2025-09-16 12:14:52,549 - INFO - Epoch 151: D_loss=0.8426, G_loss=46.9833, Avg Cos_Sim=-0.0201\n",
      "Epoch 152/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 152/200:  20%|        | 1/5 [00:00<00:03,  1.13it/s, D_loss=0.8061, G_loss=45.2729, Cos_Sim=0.0148]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 152/200:  40%|      | 2/5 [00:01<00:02,  1.43it/s, D_loss=0.8269, G_loss=40.9008, Cos_Sim=-0.0269]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 152/200:  60%|    | 3/5 [00:01<00:01,  1.61it/s, D_loss=0.8502, G_loss=44.3883, Cos_Sim=0.0078] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 152/200:  80%|  | 4/5 [00:02<00:00,  1.65it/s, D_loss=0.8208, G_loss=46.5803, Cos_Sim=-0.0007]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 152/200: 100%|| 5/5 [00:03<00:00,  1.62it/s, D_loss=0.8298, G_loss=45.1842, Cos_Sim=-0.0021]\n",
      "2025-09-16 12:14:55,906 - INFO - Epoch 152: D_loss=0.8267, G_loss=44.4653, Avg Cos_Sim=-0.0014\n",
      "Epoch 153/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 153/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.8290, G_loss=43.9302, Cos_Sim=0.0086]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 153/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.7783, G_loss=40.5136, Cos_Sim=0.0072]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 153/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=0.8113, G_loss=46.9106, Cos_Sim=0.0346]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 153/200:  80%|  | 4/5 [00:02<00:00,  1.73it/s, D_loss=0.8500, G_loss=39.5795, Cos_Sim=-0.0275]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 153/200: 100%|| 5/5 [00:02<00:00,  1.71it/s, D_loss=0.8612, G_loss=52.7256, Cos_Sim=0.0054] \n",
      "2025-09-16 12:14:59,126 - INFO - Epoch 153: D_loss=0.8260, G_loss=44.7319, Avg Cos_Sim=0.0056\n",
      "Epoch 154/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 154/200:  20%|        | 1/5 [00:00<00:03,  1.19it/s, D_loss=0.8999, G_loss=42.2127, Cos_Sim=0.0452]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 154/200:  40%|      | 2/5 [00:01<00:01,  1.52it/s, D_loss=0.8228, G_loss=49.2060, Cos_Sim=-0.0273]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 154/200:  60%|    | 3/5 [00:01<00:01,  1.70it/s, D_loss=0.8693, G_loss=47.6455, Cos_Sim=-0.0418]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 154/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8832, G_loss=39.8354, Cos_Sim=-0.0087]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 154/200: 100%|| 5/5 [00:02<00:00,  1.75it/s, D_loss=0.8364, G_loss=50.9345, Cos_Sim=-0.0049]\n",
      "2025-09-16 12:15:02,262 - INFO - Epoch 154: D_loss=0.8623, G_loss=45.9668, Avg Cos_Sim=-0.0075\n",
      "Epoch 155/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 155/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.8211, G_loss=45.1794, Cos_Sim=-0.0103]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 155/200:  40%|      | 2/5 [00:01<00:02,  1.49it/s, D_loss=0.8635, G_loss=47.9437, Cos_Sim=0.0105] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 155/200:  60%|    | 3/5 [00:01<00:01,  1.64it/s, D_loss=0.8430, G_loss=43.0693, Cos_Sim=-0.0067]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 155/200:  80%|  | 4/5 [00:02<00:00,  1.73it/s, D_loss=0.7830, G_loss=45.3300, Cos_Sim=-0.0029]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 155/200: 100%|| 5/5 [00:03<00:00,  1.62it/s, D_loss=0.8469, G_loss=39.7340, Cos_Sim=0.0489] \n",
      "2025-09-16 12:15:05,638 - INFO - Epoch 155: D_loss=0.8315, G_loss=44.2513, Avg Cos_Sim=0.0079\n",
      "Epoch 156/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 156/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.7983, G_loss=49.1151, Cos_Sim=0.0150]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 156/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.7857, G_loss=49.0769, Cos_Sim=0.0196]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 156/200:  60%|    | 3/5 [00:01<00:01,  1.59it/s, D_loss=0.8072, G_loss=46.6654, Cos_Sim=-0.0026]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 156/200:  80%|  | 4/5 [00:02<00:00,  1.68it/s, D_loss=0.8392, G_loss=41.9851, Cos_Sim=0.0474] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 156/200: 100%|| 5/5 [00:03<00:00,  1.64it/s, D_loss=0.8493, G_loss=50.8011, Cos_Sim=0.0504]\n",
      "2025-09-16 12:15:08,981 - INFO - Epoch 156: D_loss=0.8159, G_loss=47.5287, Avg Cos_Sim=0.0260\n",
      "Epoch 157/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 157/200:  20%|        | 1/5 [00:00<00:03,  1.17it/s, D_loss=0.7931, G_loss=42.6380, Cos_Sim=0.0047]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 157/200:  40%|      | 2/5 [00:01<00:02,  1.40it/s, D_loss=0.8065, G_loss=52.9554, Cos_Sim=0.0291]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 157/200:  60%|    | 3/5 [00:02<00:01,  1.52it/s, D_loss=0.8370, G_loss=42.1583, Cos_Sim=0.0126]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 157/200:  80%|  | 4/5 [00:02<00:00,  1.61it/s, D_loss=0.8129, G_loss=42.8970, Cos_Sim=0.0356]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 157/200: 100%|| 5/5 [00:03<00:00,  1.55it/s, D_loss=0.9217, G_loss=44.6847, Cos_Sim=0.0161]\n",
      "2025-09-16 12:15:12,475 - INFO - Epoch 157: D_loss=0.8342, G_loss=45.0667, Avg Cos_Sim=0.0196\n",
      "Epoch 158/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 158/200:  20%|        | 1/5 [00:00<00:03,  1.20it/s, D_loss=0.8081, G_loss=42.1585, Cos_Sim=-0.0291]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 158/200:  40%|      | 2/5 [00:01<00:01,  1.50it/s, D_loss=0.8035, G_loss=48.6733, Cos_Sim=0.0118] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 158/200:  60%|    | 3/5 [00:01<00:01,  1.59it/s, D_loss=0.8923, G_loss=42.9435, Cos_Sim=-0.0055]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 158/200:  80%|  | 4/5 [00:02<00:00,  1.66it/s, D_loss=0.8034, G_loss=42.1308, Cos_Sim=0.0000] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 158/200: 100%|| 5/5 [00:03<00:00,  1.60it/s, D_loss=0.8689, G_loss=40.8766, Cos_Sim=0.0236]\n",
      "2025-09-16 12:15:15,874 - INFO - Epoch 158: D_loss=0.8352, G_loss=43.3565, Avg Cos_Sim=0.0002\n",
      "Epoch 159/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 159/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8421, G_loss=44.6663, Cos_Sim=0.0217]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 159/200:  40%|      | 2/5 [00:01<00:01,  1.54it/s, D_loss=0.8683, G_loss=50.0612, Cos_Sim=-0.0228]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 159/200:  60%|    | 3/5 [00:01<00:01,  1.72it/s, D_loss=0.9041, G_loss=47.5716, Cos_Sim=-0.0293]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 159/200:  80%|  | 4/5 [00:02<00:00,  1.79it/s, D_loss=0.8144, G_loss=43.8800, Cos_Sim=-0.0132]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 159/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8525, G_loss=48.9421, Cos_Sim=-0.0068]\n",
      "2025-09-16 12:15:19,163 - INFO - Epoch 159: D_loss=0.8563, G_loss=47.0242, Avg Cos_Sim=-0.0101\n",
      "Epoch 160/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 160/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8088, G_loss=49.1632, Cos_Sim=0.0150]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 160/200:  40%|      | 2/5 [00:01<00:01,  1.51it/s, D_loss=0.8341, G_loss=44.2254, Cos_Sim=0.0017]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 160/200:  60%|    | 3/5 [00:02<00:01,  1.22it/s, D_loss=0.8333, G_loss=46.2943, Cos_Sim=0.0123]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 160/200:  80%|  | 4/5 [00:02<00:00,  1.43it/s, D_loss=0.7992, G_loss=47.8232, Cos_Sim=0.0202]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 160/200: 100%|| 5/5 [00:03<00:00,  1.46it/s, D_loss=0.8286, G_loss=47.3082, Cos_Sim=-0.0136]\n",
      "2025-09-16 12:15:22,854 - INFO - Epoch 160: D_loss=0.8208, G_loss=46.9629, Avg Cos_Sim=0.0071\n",
      "Epoch 161/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 161/200:  20%|        | 1/5 [00:00<00:03,  1.24it/s, D_loss=0.8328, G_loss=37.0588, Cos_Sim=-0.0315]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 161/200:  40%|      | 2/5 [00:01<00:01,  1.67it/s, D_loss=0.8178, G_loss=51.4174, Cos_Sim=0.0166] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 161/200:  60%|    | 3/5 [00:01<00:01,  1.70it/s, D_loss=0.9089, G_loss=47.7321, Cos_Sim=-0.0333]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 161/200:  80%|  | 4/5 [00:02<00:00,  1.73it/s, D_loss=0.8445, G_loss=48.2662, Cos_Sim=-0.0038]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 161/200: 100%|| 5/5 [00:03<00:00,  1.65it/s, D_loss=0.8217, G_loss=50.8136, Cos_Sim=-0.0179]\n",
      "2025-09-16 12:15:26,168 - INFO - Epoch 161: D_loss=0.8451, G_loss=47.0576, Avg Cos_Sim=-0.0140\n",
      "Epoch 162/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 162/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8272, G_loss=44.1388, Cos_Sim=-0.0189]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 162/200:  40%|      | 2/5 [00:01<00:02,  1.45it/s, D_loss=0.9225, G_loss=49.4819, Cos_Sim=-0.0115]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 162/200:  60%|    | 3/5 [00:01<00:01,  1.59it/s, D_loss=0.8835, G_loss=47.8057, Cos_Sim=-0.0051]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 162/200:  80%|  | 4/5 [00:02<00:00,  1.57it/s, D_loss=0.7787, G_loss=42.3638, Cos_Sim=-0.0201]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 162/200: 100%|| 5/5 [00:03<00:00,  1.52it/s, D_loss=0.8298, G_loss=48.0316, Cos_Sim=-0.0025]\n",
      "2025-09-16 12:15:29,717 - INFO - Epoch 162: D_loss=0.8483, G_loss=46.3643, Avg Cos_Sim=-0.0116\n",
      "Epoch 163/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 163/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.7825, G_loss=45.1045, Cos_Sim=-0.0247]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 163/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=0.8596, G_loss=56.7948, Cos_Sim=-0.0181]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 163/200:  60%|    | 3/5 [00:01<00:01,  1.68it/s, D_loss=0.8141, G_loss=41.2829, Cos_Sim=-0.0314]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 163/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.7805, G_loss=44.8692, Cos_Sim=0.0082] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 163/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8086, G_loss=43.5165, Cos_Sim=0.0186]\n",
      "2025-09-16 12:15:32,976 - INFO - Epoch 163: D_loss=0.8091, G_loss=46.3136, Avg Cos_Sim=-0.0095\n",
      "Epoch 164/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 164/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.8200, G_loss=43.2731, Cos_Sim=-0.0025]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 164/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=0.8217, G_loss=43.6934, Cos_Sim=0.0108] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 164/200:  60%|    | 3/5 [00:01<00:01,  1.76it/s, D_loss=0.9143, G_loss=39.1535, Cos_Sim=-0.0421]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 164/200:  80%|  | 4/5 [00:02<00:00,  1.87it/s, D_loss=0.8969, G_loss=51.4754, Cos_Sim=-0.0037]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 164/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.7882, G_loss=52.6387, Cos_Sim=0.0022] \n",
      "2025-09-16 12:15:36,084 - INFO - Epoch 164: D_loss=0.8482, G_loss=46.0468, Avg Cos_Sim=-0.0071\n",
      "Epoch 165/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 165/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.8405, G_loss=43.9980, Cos_Sim=-0.0220]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 165/200:  40%|      | 2/5 [00:01<00:01,  1.51it/s, D_loss=0.8388, G_loss=42.9495, Cos_Sim=-0.0254]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 165/200:  60%|    | 3/5 [00:01<00:01,  1.60it/s, D_loss=0.8129, G_loss=43.6516, Cos_Sim=0.0109] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 165/200:  80%|  | 4/5 [00:02<00:00,  1.70it/s, D_loss=0.8799, G_loss=48.9138, Cos_Sim=-0.0125]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 165/200: 100%|| 5/5 [00:03<00:00,  1.66it/s, D_loss=0.8142, G_loss=43.1349, Cos_Sim=-0.0005]\n",
      "2025-09-16 12:15:39,403 - INFO - Epoch 165: D_loss=0.8372, G_loss=44.5296, Avg Cos_Sim=-0.0099\n",
      "Epoch 166/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 166/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.8927, G_loss=42.7093, Cos_Sim=0.0208]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 166/200:  40%|      | 2/5 [00:01<00:01,  1.59it/s, D_loss=0.9083, G_loss=50.0751, Cos_Sim=-0.0198]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 166/200:  60%|    | 3/5 [00:01<00:01,  1.65it/s, D_loss=0.7821, G_loss=44.5507, Cos_Sim=0.0177] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 166/200:  80%|  | 4/5 [00:02<00:00,  1.76it/s, D_loss=0.7991, G_loss=41.3055, Cos_Sim=0.0144]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 166/200: 100%|| 5/5 [00:02<00:00,  1.73it/s, D_loss=0.7793, G_loss=39.5794, Cos_Sim=-0.0180]\n",
      "2025-09-16 12:15:42,574 - INFO - Epoch 166: D_loss=0.8323, G_loss=43.6440, Avg Cos_Sim=0.0030\n",
      "Epoch 167/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 167/200:  20%|        | 1/5 [00:00<00:03,  1.18it/s, D_loss=0.8117, G_loss=42.4364, Cos_Sim=-0.0015]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 167/200:  40%|      | 2/5 [00:01<00:02,  1.50it/s, D_loss=0.8208, G_loss=52.2444, Cos_Sim=-0.0095]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 167/200:  60%|    | 3/5 [00:01<00:01,  1.57it/s, D_loss=0.8814, G_loss=44.6897, Cos_Sim=0.0114] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 167/200:  80%|  | 4/5 [00:02<00:00,  1.65it/s, D_loss=0.8126, G_loss=51.7970, Cos_Sim=0.0185]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 167/200: 100%|| 5/5 [00:03<00:00,  1.62it/s, D_loss=0.7970, G_loss=51.2124, Cos_Sim=-0.0399]\n",
      "2025-09-16 12:15:45,972 - INFO - Epoch 167: D_loss=0.8247, G_loss=48.4760, Avg Cos_Sim=-0.0042\n",
      "Epoch 168/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 168/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=0.7713, G_loss=45.5269, Cos_Sim=0.0014]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 168/200:  40%|      | 2/5 [00:01<00:02,  1.47it/s, D_loss=0.8157, G_loss=44.5147, Cos_Sim=0.0501]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 168/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.8320, G_loss=45.7057, Cos_Sim=-0.0193]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 168/200:  80%|  | 4/5 [00:02<00:00,  1.69it/s, D_loss=0.8262, G_loss=42.7996, Cos_Sim=0.0283] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 168/200: 100%|| 5/5 [00:03<00:00,  1.67it/s, D_loss=0.7997, G_loss=47.4410, Cos_Sim=-0.0045]\n",
      "2025-09-16 12:15:49,249 - INFO - Epoch 168: D_loss=0.8090, G_loss=45.1976, Avg Cos_Sim=0.0112\n",
      "Epoch 169/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 169/200:  20%|        | 1/5 [00:00<00:03,  1.05it/s, D_loss=0.8582, G_loss=44.4133, Cos_Sim=-0.0069]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 169/200:  40%|      | 2/5 [00:01<00:02,  1.44it/s, D_loss=0.8176, G_loss=51.5063, Cos_Sim=-0.0220]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 169/200:  60%|    | 3/5 [00:01<00:01,  1.63it/s, D_loss=0.7827, G_loss=48.2413, Cos_Sim=0.0282] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 169/200:  80%|  | 4/5 [00:02<00:00,  1.67it/s, D_loss=0.8764, G_loss=49.1485, Cos_Sim=0.0035]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 169/200: 100%|| 5/5 [00:03<00:00,  1.62it/s, D_loss=0.8725, G_loss=43.6964, Cos_Sim=0.0286]\n",
      "2025-09-16 12:15:52,620 - INFO - Epoch 169: D_loss=0.8415, G_loss=47.4012, Avg Cos_Sim=0.0063\n",
      "Epoch 170/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 170/200:  20%|        | 1/5 [00:00<00:03,  1.19it/s, D_loss=0.8190, G_loss=51.3517, Cos_Sim=-0.0067]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 170/200:  40%|      | 2/5 [00:01<00:02,  1.46it/s, D_loss=0.7883, G_loss=40.4532, Cos_Sim=-0.0173]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 170/200:  60%|    | 3/5 [00:01<00:01,  1.58it/s, D_loss=0.7825, G_loss=47.0074, Cos_Sim=-0.0021]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 170/200:  80%|  | 4/5 [00:02<00:00,  1.70it/s, D_loss=0.7996, G_loss=39.1414, Cos_Sim=0.0127] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 170/200: 100%|| 5/5 [00:03<00:00,  1.64it/s, D_loss=0.8780, G_loss=50.7850, Cos_Sim=0.0040]\n",
      "2025-09-16 12:15:55,965 - INFO - Epoch 170: D_loss=0.8135, G_loss=45.7478, Avg Cos_Sim=-0.0019\n",
      "Epoch 171/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 171/200:  20%|        | 1/5 [00:00<00:03,  1.10it/s, D_loss=0.9344, G_loss=50.3343, Cos_Sim=0.0342]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 171/200:  40%|      | 2/5 [00:01<00:02,  1.41it/s, D_loss=0.8640, G_loss=51.1405, Cos_Sim=-0.0205]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 171/200:  60%|    | 3/5 [00:02<00:01,  1.49it/s, D_loss=0.8348, G_loss=37.1455, Cos_Sim=0.0217] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 171/200:  80%|  | 4/5 [00:02<00:00,  1.63it/s, D_loss=0.8971, G_loss=44.8447, Cos_Sim=0.0106]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 171/200: 100%|| 5/5 [00:03<00:00,  1.54it/s, D_loss=0.8427, G_loss=43.4367, Cos_Sim=-0.0125]\n",
      "2025-09-16 12:15:59,485 - INFO - Epoch 171: D_loss=0.8746, G_loss=45.3804, Avg Cos_Sim=0.0067\n",
      "Epoch 172/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 172/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.8194, G_loss=44.3839, Cos_Sim=0.0170]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 172/200:  40%|      | 2/5 [00:01<00:02,  1.47it/s, D_loss=0.8403, G_loss=42.3079, Cos_Sim=-0.0020]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 172/200:  60%|    | 3/5 [00:01<00:01,  1.62it/s, D_loss=0.8913, G_loss=43.6613, Cos_Sim=-0.0317]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 172/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8171, G_loss=41.2389, Cos_Sim=0.0200] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 172/200: 100%|| 5/5 [00:02<00:00,  1.69it/s, D_loss=0.8368, G_loss=41.8975, Cos_Sim=-0.0240]\n",
      "2025-09-16 12:16:02,701 - INFO - Epoch 172: D_loss=0.8410, G_loss=42.6979, Avg Cos_Sim=-0.0041\n",
      "Epoch 173/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 173/200:  20%|        | 1/5 [00:00<00:03,  1.15it/s, D_loss=0.8232, G_loss=46.8494, Cos_Sim=0.0154]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 173/200:  40%|      | 2/5 [00:01<00:01,  1.51it/s, D_loss=0.8761, G_loss=48.2748, Cos_Sim=-0.0028]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 173/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.8469, G_loss=48.9040, Cos_Sim=-0.0290]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 173/200:  80%|  | 4/5 [00:02<00:00,  1.75it/s, D_loss=0.8459, G_loss=40.6454, Cos_Sim=0.0031] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 173/200: 100%|| 5/5 [00:02<00:00,  1.70it/s, D_loss=0.8469, G_loss=38.7856, Cos_Sim=0.0527]\n",
      "2025-09-16 12:16:05,941 - INFO - Epoch 173: D_loss=0.8478, G_loss=44.6918, Avg Cos_Sim=0.0079\n",
      "Epoch 174/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 174/200:  20%|        | 1/5 [00:00<00:03,  1.19it/s, D_loss=0.7844, G_loss=58.4090, Cos_Sim=-0.0137]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 174/200:  40%|      | 2/5 [00:01<00:01,  1.53it/s, D_loss=0.8617, G_loss=45.5427, Cos_Sim=-0.0239]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 174/200:  60%|    | 3/5 [00:01<00:01,  1.66it/s, D_loss=0.7972, G_loss=46.0515, Cos_Sim=-0.0063]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 174/200:  80%|  | 4/5 [00:02<00:00,  1.70it/s, D_loss=0.8618, G_loss=46.9604, Cos_Sim=0.0418] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 174/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8069, G_loss=41.8410, Cos_Sim=0.0242]\n",
      "2025-09-16 12:16:09,213 - INFO - Epoch 174: D_loss=0.8224, G_loss=47.7609, Avg Cos_Sim=0.0044\n",
      "Epoch 175/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 175/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.8231, G_loss=46.6995, Cos_Sim=-0.0114]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 175/200:  40%|      | 2/5 [00:01<00:01,  1.52it/s, D_loss=0.9339, G_loss=49.3528, Cos_Sim=0.0249] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 175/200:  60%|    | 3/5 [00:01<00:01,  1.64it/s, D_loss=0.7959, G_loss=44.1278, Cos_Sim=0.0259]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 175/200:  80%|  | 4/5 [00:02<00:00,  1.69it/s, D_loss=0.7883, G_loss=41.1744, Cos_Sim=0.0077]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 175/200: 100%|| 5/5 [00:03<00:00,  1.67it/s, D_loss=0.8038, G_loss=48.0479, Cos_Sim=-0.0463]\n",
      "2025-09-16 12:16:12,508 - INFO - Epoch 175: D_loss=0.8290, G_loss=45.8805, Avg Cos_Sim=0.0002\n",
      "Epoch 176/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 176/200:  20%|        | 1/5 [00:00<00:03,  1.21it/s, D_loss=0.8348, G_loss=47.8848, Cos_Sim=-0.0008]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 176/200:  40%|      | 2/5 [00:01<00:01,  1.60it/s, D_loss=0.8699, G_loss=43.7009, Cos_Sim=0.0169] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 176/200:  60%|    | 3/5 [00:01<00:01,  1.69it/s, D_loss=0.8327, G_loss=51.8612, Cos_Sim=-0.0136]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 176/200:  80%|  | 4/5 [00:02<00:00,  1.72it/s, D_loss=0.8368, G_loss=37.0448, Cos_Sim=0.0139] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 176/200: 100%|| 5/5 [00:02<00:00,  1.72it/s, D_loss=0.8019, G_loss=45.1553, Cos_Sim=0.0225]\n",
      "2025-09-16 12:16:15,731 - INFO - Epoch 176: D_loss=0.8352, G_loss=45.1294, Avg Cos_Sim=0.0078\n",
      "Epoch 177/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 177/200:  20%|        | 1/5 [00:00<00:02,  1.35it/s, D_loss=0.7776, G_loss=49.8273, Cos_Sim=-0.0015]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 177/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8025, G_loss=44.1659, Cos_Sim=0.0085] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 177/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.8264, G_loss=38.4887, Cos_Sim=-0.0309]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 177/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.7811, G_loss=42.4873, Cos_Sim=0.0029] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 177/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8354, G_loss=44.7571, Cos_Sim=-0.0189]\n",
      "2025-09-16 12:16:19,007 - INFO - Epoch 177: D_loss=0.8046, G_loss=43.9452, Avg Cos_Sim=-0.0080\n",
      "Epoch 178/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 178/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8177, G_loss=40.7496, Cos_Sim=0.0168]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 178/200:  40%|      | 2/5 [00:01<00:01,  1.56it/s, D_loss=0.8333, G_loss=48.4139, Cos_Sim=0.0026]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 178/200:  60%|    | 3/5 [00:01<00:01,  1.64it/s, D_loss=0.8245, G_loss=46.9329, Cos_Sim=-0.0314]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 178/200:  80%|  | 4/5 [00:02<00:00,  1.69it/s, D_loss=0.8672, G_loss=47.3805, Cos_Sim=0.0104] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 178/200: 100%|| 5/5 [00:03<00:00,  1.61it/s, D_loss=0.7937, G_loss=42.0455, Cos_Sim=-0.0075]\n",
      "2025-09-16 12:16:22,398 - INFO - Epoch 178: D_loss=0.8273, G_loss=45.1045, Avg Cos_Sim=-0.0018\n",
      "Epoch 179/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 179/200:  20%|        | 1/5 [00:00<00:03,  1.17it/s, D_loss=0.8164, G_loss=51.0842, Cos_Sim=-0.0129]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 179/200:  40%|      | 2/5 [00:01<00:01,  1.53it/s, D_loss=0.7984, G_loss=42.7644, Cos_Sim=-0.0235]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 179/200:  60%|    | 3/5 [00:01<00:01,  1.60it/s, D_loss=0.8305, G_loss=41.5011, Cos_Sim=-0.0309]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 179/200:  80%|  | 4/5 [00:02<00:00,  1.72it/s, D_loss=0.8519, G_loss=45.0855, Cos_Sim=-0.0215]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 179/200: 100%|| 5/5 [00:03<00:00,  1.64it/s, D_loss=0.8242, G_loss=45.2958, Cos_Sim=0.0244] \n",
      "2025-09-16 12:16:25,727 - INFO - Epoch 179: D_loss=0.8243, G_loss=45.1462, Avg Cos_Sim=-0.0129\n",
      "Epoch 180/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 180/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8380, G_loss=43.9206, Cos_Sim=0.0134]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 180/200:  40%|      | 2/5 [00:01<00:01,  1.51it/s, D_loss=0.8143, G_loss=44.8397, Cos_Sim=0.0551]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 180/200:  60%|    | 3/5 [00:01<00:01,  1.62it/s, D_loss=0.8355, G_loss=49.0279, Cos_Sim=-0.0240]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 180/200:  80%|  | 4/5 [00:02<00:00,  1.64it/s, D_loss=0.9116, G_loss=40.9659, Cos_Sim=0.0077] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 180/200: 100%|| 5/5 [00:03<00:00,  1.64it/s, D_loss=0.8460, G_loss=50.7112, Cos_Sim=-0.0007]\n",
      "2025-09-16 12:16:29,064 - INFO - Epoch 180: D_loss=0.8491, G_loss=45.8930, Avg Cos_Sim=0.0103\n",
      "Epoch 181/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 181/200:  20%|        | 1/5 [00:00<00:03,  1.28it/s, D_loss=0.8349, G_loss=47.0462, Cos_Sim=0.0245]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 181/200:  40%|      | 2/5 [00:01<00:01,  1.61it/s, D_loss=0.8695, G_loss=42.0579, Cos_Sim=0.0083]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 181/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.7833, G_loss=42.2348, Cos_Sim=-0.0048]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 181/200:  80%|  | 4/5 [00:02<00:00,  1.68it/s, D_loss=0.8813, G_loss=44.8442, Cos_Sim=0.0097] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 181/200: 100%|| 5/5 [00:03<00:00,  1.65it/s, D_loss=0.8442, G_loss=40.4073, Cos_Sim=0.0110]\n",
      "2025-09-16 12:16:32,372 - INFO - Epoch 181: D_loss=0.8426, G_loss=43.3181, Avg Cos_Sim=0.0097\n",
      "Epoch 182/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 182/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.7959, G_loss=44.6936, Cos_Sim=-0.0217]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 182/200:  40%|      | 2/5 [00:01<00:01,  1.58it/s, D_loss=0.8459, G_loss=46.1279, Cos_Sim=0.0009] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 182/200:  60%|    | 3/5 [00:01<00:01,  1.67it/s, D_loss=0.8795, G_loss=45.6299, Cos_Sim=0.0129]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 182/200:  80%|  | 4/5 [00:02<00:00,  1.70it/s, D_loss=0.8523, G_loss=36.4023, Cos_Sim=0.0431]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 182/200: 100%|| 5/5 [00:02<00:00,  1.68it/s, D_loss=0.8044, G_loss=49.7678, Cos_Sim=0.0368]\n",
      "2025-09-16 12:16:35,617 - INFO - Epoch 182: D_loss=0.8356, G_loss=44.5243, Avg Cos_Sim=0.0144\n",
      "Epoch 183/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 183/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8252, G_loss=44.3588, Cos_Sim=-0.0032]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 183/200:  40%|      | 2/5 [00:01<00:02,  1.50it/s, D_loss=0.8998, G_loss=48.7661, Cos_Sim=-0.0012]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 183/200:  60%|    | 3/5 [00:01<00:01,  1.64it/s, D_loss=0.7801, G_loss=46.7109, Cos_Sim=0.0091] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 183/200:  80%|  | 4/5 [00:02<00:00,  1.78it/s, D_loss=0.7928, G_loss=52.3311, Cos_Sim=0.0133]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 183/200: 100%|| 5/5 [00:03<00:00,  1.62it/s, D_loss=0.8210, G_loss=42.4069, Cos_Sim=0.0277]\n",
      "2025-09-16 12:16:38,994 - INFO - Epoch 183: D_loss=0.8238, G_loss=46.9148, Avg Cos_Sim=0.0091\n",
      "Epoch 184/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 184/200:  20%|        | 1/5 [00:00<00:03,  1.16it/s, D_loss=0.8512, G_loss=42.2190, Cos_Sim=0.0007]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 184/200:  40%|      | 2/5 [00:01<00:01,  1.52it/s, D_loss=0.8261, G_loss=44.5251, Cos_Sim=0.0266]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 184/200:  60%|    | 3/5 [00:01<00:01,  1.57it/s, D_loss=0.8735, G_loss=55.2093, Cos_Sim=-0.0115]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 184/200:  80%|  | 4/5 [00:02<00:00,  1.63it/s, D_loss=0.8028, G_loss=47.1852, Cos_Sim=-0.0149]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 184/200: 100%|| 5/5 [00:03<00:00,  1.51it/s, D_loss=0.8696, G_loss=38.9387, Cos_Sim=-0.0207]\n",
      "2025-09-16 12:16:42,613 - INFO - Epoch 184: D_loss=0.8446, G_loss=45.6155, Avg Cos_Sim=-0.0040\n",
      "Epoch 185/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 185/200:  20%|        | 1/5 [00:00<00:03,  1.12it/s, D_loss=0.8628, G_loss=54.8129, Cos_Sim=0.0239]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 185/200:  40%|      | 2/5 [00:01<00:02,  1.33it/s, D_loss=0.9836, G_loss=46.8284, Cos_Sim=0.0020]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 185/200:  60%|    | 3/5 [00:02<00:01,  1.44it/s, D_loss=0.8509, G_loss=43.3745, Cos_Sim=-0.0096]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 185/200:  80%|  | 4/5 [00:02<00:00,  1.44it/s, D_loss=0.8249, G_loss=49.4240, Cos_Sim=0.0066] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 185/200: 100%|| 5/5 [00:03<00:00,  1.48it/s, D_loss=0.8037, G_loss=46.0201, Cos_Sim=-0.0294]\n",
      "2025-09-16 12:16:46,302 - INFO - Epoch 185: D_loss=0.8652, G_loss=48.0920, Avg Cos_Sim=-0.0013\n",
      "Epoch 186/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 186/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.7924, G_loss=48.9056, Cos_Sim=-0.0035]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 186/200:  40%|      | 2/5 [00:01<00:01,  1.62it/s, D_loss=0.8292, G_loss=46.3514, Cos_Sim=0.0159] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 186/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=0.8383, G_loss=43.0443, Cos_Sim=0.0237]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 186/200:  80%|  | 4/5 [00:02<00:00,  1.82it/s, D_loss=0.8424, G_loss=46.6187, Cos_Sim=-0.0109]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 186/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.8235, G_loss=53.5699, Cos_Sim=0.0327] \n",
      "2025-09-16 12:16:49,429 - INFO - Epoch 186: D_loss=0.8252, G_loss=47.6980, Avg Cos_Sim=0.0116\n",
      "Epoch 187/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 187/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.8228, G_loss=43.7468, Cos_Sim=0.0086]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 187/200:  40%|      | 2/5 [00:01<00:01,  1.72it/s, D_loss=0.8178, G_loss=50.1158, Cos_Sim=0.0202]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 187/200:  60%|    | 3/5 [00:01<00:01,  1.82it/s, D_loss=0.8178, G_loss=47.2954, Cos_Sim=0.0122]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 187/200:  80%|  | 4/5 [00:02<00:00,  1.93it/s, D_loss=0.8356, G_loss=45.1653, Cos_Sim=0.0027]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 187/200: 100%|| 5/5 [00:02<00:00,  1.84it/s, D_loss=0.8741, G_loss=45.9642, Cos_Sim=-0.0076]\n",
      "2025-09-16 12:16:52,454 - INFO - Epoch 187: D_loss=0.8336, G_loss=46.4575, Avg Cos_Sim=0.0072\n",
      "Epoch 188/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 188/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=0.9147, G_loss=43.0301, Cos_Sim=0.0095]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 188/200:  40%|      | 2/5 [00:01<00:01,  1.71it/s, D_loss=0.8187, G_loss=46.3788, Cos_Sim=-0.0250]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 188/200:  60%|    | 3/5 [00:01<00:01,  1.85it/s, D_loss=0.8091, G_loss=54.6160, Cos_Sim=0.0009] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 188/200:  80%|  | 4/5 [00:02<00:00,  1.94it/s, D_loss=0.8057, G_loss=44.3426, Cos_Sim=-0.0200]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 188/200: 100%|| 5/5 [00:02<00:00,  1.85it/s, D_loss=0.8308, G_loss=47.1159, Cos_Sim=-0.0140]\n",
      "2025-09-16 12:16:55,485 - INFO - Epoch 188: D_loss=0.8358, G_loss=47.0967, Avg Cos_Sim=-0.0097\n",
      "Epoch 189/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 189/200:  20%|        | 1/5 [00:00<00:03,  1.30it/s, D_loss=0.8385, G_loss=55.1312, Cos_Sim=0.0148]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 189/200:  40%|      | 2/5 [00:01<00:01,  1.65it/s, D_loss=0.8048, G_loss=41.0064, Cos_Sim=-0.0269]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 189/200:  60%|    | 3/5 [00:01<00:01,  1.78it/s, D_loss=0.7809, G_loss=44.9437, Cos_Sim=0.0063] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 189/200:  80%|  | 4/5 [00:02<00:00,  1.84it/s, D_loss=0.8038, G_loss=48.7012, Cos_Sim=0.0280]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 189/200: 100%|| 5/5 [00:02<00:00,  1.81it/s, D_loss=0.8090, G_loss=44.1999, Cos_Sim=0.0066]\n",
      "2025-09-16 12:16:58,501 - INFO - Epoch 189: D_loss=0.8074, G_loss=46.7965, Avg Cos_Sim=0.0058\n",
      "Epoch 190/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 190/200:  20%|        | 1/5 [00:00<00:03,  1.25it/s, D_loss=0.8461, G_loss=38.0228, Cos_Sim=-0.0111]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 190/200:  40%|      | 2/5 [00:01<00:01,  1.55it/s, D_loss=0.8761, G_loss=51.3464, Cos_Sim=0.0017] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 190/200:  60%|    | 3/5 [00:01<00:01,  1.79it/s, D_loss=0.8032, G_loss=43.7296, Cos_Sim=-0.0062]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 190/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.8781, G_loss=43.8440, Cos_Sim=0.0145] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 190/200: 100%|| 5/5 [00:02<00:00,  1.79it/s, D_loss=0.8630, G_loss=46.5149, Cos_Sim=0.0327]\n",
      "2025-09-16 12:17:01,540 - INFO - Epoch 190: D_loss=0.8533, G_loss=44.6915, Avg Cos_Sim=0.0063\n",
      "Epoch 191/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 191/200:  20%|        | 1/5 [00:00<00:03,  1.26it/s, D_loss=0.8484, G_loss=51.0099, Cos_Sim=-0.0098]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 191/200:  40%|      | 2/5 [00:01<00:01,  1.67it/s, D_loss=0.8586, G_loss=44.6644, Cos_Sim=-0.0341]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 191/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=0.8093, G_loss=48.5932, Cos_Sim=0.0272] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 191/200:  80%|  | 4/5 [00:02<00:00,  1.90it/s, D_loss=0.8560, G_loss=39.5874, Cos_Sim=-0.0306]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 191/200: 100%|| 5/5 [00:02<00:00,  1.80it/s, D_loss=0.8236, G_loss=45.8147, Cos_Sim=-0.0106]\n",
      "2025-09-16 12:17:04,619 - INFO - Epoch 191: D_loss=0.8392, G_loss=45.9339, Avg Cos_Sim=-0.0116\n",
      "Epoch 192/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 192/200:  20%|        | 1/5 [00:00<00:03,  1.29it/s, D_loss=0.8428, G_loss=51.2094, Cos_Sim=-0.0168]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 192/200:  40%|      | 2/5 [00:01<00:01,  1.63it/s, D_loss=0.8863, G_loss=51.9416, Cos_Sim=-0.0358]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 192/200:  60%|    | 3/5 [00:01<00:01,  1.83it/s, D_loss=0.8420, G_loss=51.3570, Cos_Sim=-0.0107]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 192/200:  80%|  | 4/5 [00:02<00:00,  1.85it/s, D_loss=0.8436, G_loss=41.0775, Cos_Sim=0.0227] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 192/200: 100%|| 5/5 [00:02<00:00,  1.77it/s, D_loss=0.8552, G_loss=55.9348, Cos_Sim=-0.0028]\n",
      "2025-09-16 12:17:07,697 - INFO - Epoch 192: D_loss=0.8540, G_loss=50.3040, Avg Cos_Sim=-0.0087\n",
      "Epoch 193/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 193/200:  20%|        | 1/5 [00:00<00:02,  1.38it/s, D_loss=0.8579, G_loss=39.5243, Cos_Sim=-0.0347]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 193/200:  40%|      | 2/5 [00:01<00:01,  1.75it/s, D_loss=0.8360, G_loss=39.0566, Cos_Sim=-0.0036]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 193/200:  60%|    | 3/5 [00:01<00:01,  1.82it/s, D_loss=0.8292, G_loss=42.5963, Cos_Sim=-0.0063]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 193/200:  80%|  | 4/5 [00:02<00:00,  1.85it/s, D_loss=0.8185, G_loss=50.3543, Cos_Sim=-0.0195]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 193/200: 100%|| 5/5 [00:02<00:00,  1.84it/s, D_loss=0.8168, G_loss=47.1702, Cos_Sim=-0.0175]\n",
      "2025-09-16 12:17:10,685 - INFO - Epoch 193: D_loss=0.8317, G_loss=43.7403, Avg Cos_Sim=-0.0163\n",
      "Epoch 194/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 194/200:  20%|        | 1/5 [00:00<00:03,  1.33it/s, D_loss=0.7973, G_loss=43.4895, Cos_Sim=0.0245]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 194/200:  40%|      | 2/5 [00:01<00:01,  1.66it/s, D_loss=0.9558, G_loss=48.1769, Cos_Sim=0.0010]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 194/200:  60%|    | 3/5 [00:01<00:01,  1.77it/s, D_loss=0.8417, G_loss=42.6428, Cos_Sim=0.0098]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 194/200:  80%|  | 4/5 [00:02<00:00,  1.91it/s, D_loss=0.8402, G_loss=57.6501, Cos_Sim=0.0276]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 194/200: 100%|| 5/5 [00:02<00:00,  1.80it/s, D_loss=0.8071, G_loss=40.4414, Cos_Sim=-0.0268]\n",
      "2025-09-16 12:17:13,752 - INFO - Epoch 194: D_loss=0.8484, G_loss=46.4801, Avg Cos_Sim=0.0072\n",
      "Epoch 195/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 195/200:  20%|        | 1/5 [00:00<00:02,  1.42it/s, D_loss=0.8403, G_loss=36.9919, Cos_Sim=-0.0044]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 195/200:  40%|      | 2/5 [00:01<00:01,  1.82it/s, D_loss=0.7724, G_loss=49.4490, Cos_Sim=0.0148] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 195/200:  60%|    | 3/5 [00:01<00:01,  1.88it/s, D_loss=0.8609, G_loss=52.0890, Cos_Sim=-0.0140]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 195/200:  80%|  | 4/5 [00:02<00:00,  2.02it/s, D_loss=0.8816, G_loss=53.8432, Cos_Sim=0.0188] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 195/200: 100%|| 5/5 [00:02<00:00,  1.94it/s, D_loss=0.8598, G_loss=47.0503, Cos_Sim=-0.0173]\n",
      "2025-09-16 12:17:16,605 - INFO - Epoch 195: D_loss=0.8430, G_loss=47.8847, Avg Cos_Sim=-0.0004\n",
      "Epoch 196/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 196/200:  20%|        | 1/5 [00:00<00:03,  1.27it/s, D_loss=0.8744, G_loss=37.9878, Cos_Sim=0.0013]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 196/200:  40%|      | 2/5 [00:01<00:01,  1.68it/s, D_loss=0.8720, G_loss=46.0526, Cos_Sim=-0.0251]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 196/200:  60%|    | 3/5 [00:01<00:01,  1.84it/s, D_loss=0.8266, G_loss=40.6825, Cos_Sim=0.0103] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 196/200:  80%|  | 4/5 [00:02<00:00,  1.77it/s, D_loss=0.8223, G_loss=41.9379, Cos_Sim=0.0202]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 196/200: 100%|| 5/5 [00:02<00:00,  1.78it/s, D_loss=0.7870, G_loss=53.7679, Cos_Sim=0.0120]\n",
      "2025-09-16 12:17:19,677 - INFO - Epoch 196: D_loss=0.8365, G_loss=44.0857, Avg Cos_Sim=0.0037\n",
      "Epoch 197/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 197/200:  20%|        | 1/5 [00:00<00:02,  1.44it/s, D_loss=0.8627, G_loss=50.4028, Cos_Sim=-0.0108]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 197/200:  40%|      | 2/5 [00:01<00:01,  1.71it/s, D_loss=0.8125, G_loss=39.6561, Cos_Sim=-0.0155]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 197/200:  60%|    | 3/5 [00:01<00:01,  1.81it/s, D_loss=0.8282, G_loss=50.7838, Cos_Sim=0.0153] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 197/200:  80%|  | 4/5 [00:02<00:00,  1.92it/s, D_loss=0.8525, G_loss=48.2797, Cos_Sim=-0.0010]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 197/200: 100%|| 5/5 [00:02<00:00,  1.87it/s, D_loss=0.8264, G_loss=41.7730, Cos_Sim=-0.0349]\n",
      "2025-09-16 12:17:22,666 - INFO - Epoch 197: D_loss=0.8365, G_loss=46.1791, Avg Cos_Sim=-0.0094\n",
      "Epoch 198/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 198/200:  20%|        | 1/5 [00:00<00:02,  1.41it/s, D_loss=0.8694, G_loss=43.8826, Cos_Sim=0.0174]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 198/200:  40%|      | 2/5 [00:01<00:01,  1.66it/s, D_loss=0.9008, G_loss=43.7570, Cos_Sim=0.0117]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 198/200:  60%|    | 3/5 [00:01<00:01,  1.81it/s, D_loss=0.8399, G_loss=46.9559, Cos_Sim=0.0037]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 198/200:  80%|  | 4/5 [00:02<00:00,  1.91it/s, D_loss=0.8167, G_loss=46.2202, Cos_Sim=0.0100]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 198/200: 100%|| 5/5 [00:02<00:00,  1.83it/s, D_loss=0.8444, G_loss=41.8141, Cos_Sim=0.0152]\n",
      "2025-09-16 12:17:25,687 - INFO - Epoch 198: D_loss=0.8542, G_loss=44.5260, Avg Cos_Sim=0.0116\n",
      "Epoch 199/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 199/200:  20%|        | 1/5 [00:00<00:03,  1.23it/s, D_loss=0.8528, G_loss=45.6833, Cos_Sim=0.0175]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 199/200:  40%|      | 2/5 [00:01<00:01,  1.71it/s, D_loss=0.9142, G_loss=46.1732, Cos_Sim=-0.0093]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 199/200:  60%|    | 3/5 [00:01<00:01,  1.91it/s, D_loss=0.8776, G_loss=53.5639, Cos_Sim=-0.0158]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 199/200:  80%|  | 4/5 [00:02<00:00,  1.95it/s, D_loss=0.8250, G_loss=41.9948, Cos_Sim=-0.0191]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 199/200: 100%|| 5/5 [00:02<00:00,  1.86it/s, D_loss=0.8718, G_loss=42.6853, Cos_Sim=0.0083] \n",
      "2025-09-16 12:17:28,659 - INFO - Epoch 199: D_loss=0.8683, G_loss=46.0201, Avg Cos_Sim=-0.0037\n",
      "Epoch 200/200:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 200/200:  20%|        | 1/5 [00:00<00:02,  1.43it/s, D_loss=0.8082, G_loss=40.1817, Cos_Sim=0.0042]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 200/200:  40%|      | 2/5 [00:01<00:01,  1.74it/s, D_loss=0.8467, G_loss=46.7333, Cos_Sim=-0.0144]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 200/200:  60%|    | 3/5 [00:01<00:01,  1.74it/s, D_loss=0.8623, G_loss=45.3947, Cos_Sim=0.0263] C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 200/200:  80%|  | 4/5 [00:02<00:00,  1.86it/s, D_loss=0.8325, G_loss=44.2656, Cos_Sim=-0.0063]C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:556: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_24004\\1948398458.py:581: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler is not None):\n",
      "Epoch 200/200: 100%|| 5/5 [00:02<00:00,  1.83it/s, D_loss=0.8079, G_loss=46.2586, Cos_Sim=-0.0224]\n",
      "2025-09-16 12:17:31,676 - INFO - Epoch 200: D_loss=0.8315, G_loss=44.5668, Avg Cos_Sim=-0.0025\n",
      "2025-09-16 12:17:35,667 - INFO - Saved BCE training metrics plot to output\\bce_training_metrics.png\n",
      "2025-09-16 12:17:35,670 - INFO - ================================================================================\n",
      "2025-09-16 12:17:35,672 - INFO - BCE GAN TRAINING COMPLETED!\n",
      "2025-09-16 12:17:35,673 - INFO - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "# BCE Loss - CSI Training\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import spectral_norm\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "import random\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED CONFIGURATION FOR BCE LOSS\n",
    "# ============================================================================\n",
    "\n",
    "CSI_DIRECTORY = \"data/processed_csi\"\n",
    "AUDIO_DIRECTORY = \"data/audio\"\n",
    "OUTPUT_DIRECTORY = \"output\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "HYPERPARAMETERS = {\n",
    "    \"latent_dim\": 48,\n",
    "    \"csi_channels\": 8,\n",
    "    \"audio_channels\": 1,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 8, \n",
    "    \n",
    "    # Adjusted learning rates for BCE stability\n",
    "    \"lr_generator\": 0.0002,        # Lower generator LR for BCE\n",
    "    \"lr_discriminator\": 0.0001,    # Lower discriminator LR for BCE\n",
    "    \"lr_scheduler_gamma\": 0.95,\n",
    "\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"sample_rate\": None,\n",
    "    \"audio_duration\": 10.0,\n",
    "    \"sequence_length\": None,\n",
    "    \n",
    "    \"chunk_size\": 2048,\n",
    "    \"overlap\": 256,\n",
    "    \"max_chunks_per_sample\": 12,\n",
    "\n",
    "    # Adjusted loss weights for BCE\n",
    "    \"lambda_reconstruction\": 10.0,    # Higher for BCE stability\n",
    "    \"lambda_feature_matching\": 8.0,   # Higher for better feature alignment\n",
    "    \"lambda_spectral\": 2.0,           # Moderate spectral loss\n",
    "    \"lambda_drum_loss\": 5.0,          # Higher drum-specific loss\n",
    "    \"lambda_adversarial\": 1.0,        # Standard adversarial weight\n",
    "\n",
    "    # BCE-specific parameters\n",
    "    \"label_smoothing\": 0.1,           # Label smoothing for stability\n",
    "    \"discriminator_steps\": 1,         # Equal updates for BCE\n",
    "    \"generator_steps\": 1,\n",
    "\n",
    "    \"train_test_split\": 0.8,\n",
    "    \"random_seed\": 42,\n",
    "\n",
    "    # Simplified preprocessing (same as before)\n",
    "    \"enable_noise_reduction\": True,\n",
    "    \"preserve_drums\": True,\n",
    "    \"drum_freq_range\": [40, 300],\n",
    "    \"kick_freq_range\": [30, 120],\n",
    "    \"snare_freq_range\": [120, 400],\n",
    "    \"drum_enhancement_factor\": 1.5,\n",
    "    \n",
    "    \"enable_augmentation\": False,\n",
    "    \n",
    "    # Memory optimizations\n",
    "    \"use_mixed_precision\": True,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# LIGHTWEIGHT CSI PREPROCESSOR (Same as before)\n",
    "# ============================================================================\n",
    "\n",
    "class LightweightCSIPreprocessor:\n",
    "    \"\"\"Minimal CSI preprocessing for speed\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate):\n",
    "        self.sample_rate = sample_rate\n",
    "        logging.info(\"Lightweight CSI preprocessor initialized\")\n",
    "    \n",
    "    def extract_basic_features(self, csi_data):\n",
    "        \"\"\"Extract only 2 essential features instead of 5\"\"\"\n",
    "        try:\n",
    "            features_list = []\n",
    "            \n",
    "            for channel_idx in range(csi_data.shape[0]):\n",
    "                channel_data = csi_data[channel_idx]\n",
    "                channel_features = []\n",
    "                \n",
    "                # 1. Energy variations only\n",
    "                energy_variations = np.abs(channel_data) ** 2\n",
    "                channel_features.append(energy_variations)\n",
    "                \n",
    "                # 2. First-order differences only\n",
    "                diff = np.diff(channel_data, prepend=channel_data[0])\n",
    "                channel_features.append(diff)\n",
    "                \n",
    "                channel_features_array = np.stack(channel_features, axis=0)\n",
    "                features_list.append(channel_features_array)\n",
    "            \n",
    "            all_features = np.stack(features_list, axis=0)\n",
    "            return all_features.astype(np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Feature extraction failed: {e}\")\n",
    "            # Simple fallback\n",
    "            return np.expand_dims(csi_data, axis=1).astype(np.float32)\n",
    "\n",
    "# ============================================================================\n",
    "# LIGHTWEIGHT AUDIO PREPROCESSOR (Same as before)\n",
    "# ============================================================================\n",
    "\n",
    "class LightweightAudioPreprocessor:\n",
    "    \"\"\"Minimal audio preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.csi_preprocessor = LightweightCSIPreprocessor(sample_rate)\n",
    "        logging.info(\"Lightweight audio preprocessor initialized\")\n",
    "    \n",
    "    def process_audio_basic(self, audio, csi_data):\n",
    "        \"\"\"Basic processing without heavy filtering\"\"\"\n",
    "        try:\n",
    "            # Simple CSI feature extraction\n",
    "            csi_features = self.csi_preprocessor.extract_basic_features(csi_data)\n",
    "            \n",
    "            # Simple audio normalization\n",
    "            if np.max(np.abs(audio)) > 0:\n",
    "                audio = audio / np.max(np.abs(audio)) * 0.8\n",
    "            \n",
    "            return audio, csi_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Audio processing failed: {e}\")\n",
    "            fallback_csi = np.expand_dims(csi_data, axis=1).astype(np.float32)\n",
    "            return audio, fallback_csi\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED DATASET (Same as before)\n",
    "# ============================================================================\n",
    "\n",
    "class OptimizedCSIAudioDataset(Dataset):\n",
    "    def __init__(self, data_df, is_training=True):\n",
    "        self.data_df = data_df.reset_index(drop=True)\n",
    "        self.is_training = is_training\n",
    "        self.sample_rate = HYPERPARAMETERS['sample_rate']\n",
    "        self.sequence_length = HYPERPARAMETERS['sequence_length']\n",
    "        self.chunk_size = HYPERPARAMETERS['chunk_size']\n",
    "        \n",
    "        self.audio_preprocessor = LightweightAudioPreprocessor(self.sample_rate)\n",
    "        \n",
    "        # Pre-validate files\n",
    "        self._validate_files()\n",
    "        \n",
    "        logging.info(f\"Optimized dataset initialized: {len(self.data_df)} samples\")\n",
    "    \n",
    "    def _validate_files(self):\n",
    "        \"\"\"Quick file validation\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.data_df)):\n",
    "            csi_path = self.data_df.iloc[idx]['csi_path']\n",
    "            audio_path = self.data_df.iloc[idx]['audio_path']\n",
    "            \n",
    "            if os.path.exists(csi_path) and os.path.exists(audio_path):\n",
    "                try:\n",
    "                    # Quick validation without loading full data\n",
    "                    audio_info = sf.info(audio_path)\n",
    "                    if audio_info.duration >= 1.0:\n",
    "                        valid_indices.append(idx)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        self.data_df = self.data_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        logging.info(f\"Validated {len(self.data_df)} samples\")\n",
    "\n",
    "    def load_data_fast(self, csi_path, audio_path):\n",
    "        \"\"\"Fast data loading with minimal processing\"\"\"\n",
    "        try:\n",
    "            # Load audio with memory efficiency\n",
    "            audio_data, file_sample_rate = sf.read(audio_path)\n",
    "            \n",
    "            if len(audio_data.shape) > 1:\n",
    "                audio_data = audio_data.mean(axis=1)\n",
    "            \n",
    "            # Resample only if necessary\n",
    "            if file_sample_rate != self.sample_rate:\n",
    "                audio_data = librosa.resample(\n",
    "                    audio_data, orig_sr=file_sample_rate, \n",
    "                    target_sr=self.sample_rate, res_type='kaiser_fast'\n",
    "                )\n",
    "            \n",
    "            # Truncate or pad efficiently\n",
    "            target_length = min(len(audio_data), self.sequence_length)\n",
    "            if len(audio_data) > target_length:\n",
    "                audio_data = audio_data[:target_length]\n",
    "            elif len(audio_data) < self.chunk_size:\n",
    "                audio_data = np.pad(audio_data, (0, self.chunk_size - len(audio_data)))\n",
    "            \n",
    "            # Load CSI efficiently - only read needed columns\n",
    "            try:\n",
    "                csi_data = pd.read_csv(csi_path, usecols=lambda x: x.startswith('subcarrier_'))\n",
    "                csi_values = csi_data.values.T[:HYPERPARAMETERS['csi_channels']]\n",
    "            except:\n",
    "                # Fallback to full read\n",
    "                csi_data = pd.read_csv(csi_path)\n",
    "                subcarrier_cols = [col for col in csi_data.columns if col.startswith('subcarrier_')]\n",
    "                csi_values = csi_data[subcarrier_cols].values.T[:HYPERPARAMETERS['csi_channels']]\n",
    "            \n",
    "            # Quick alignment\n",
    "            if csi_values.shape[1] != len(audio_data):\n",
    "                # Simple decimation/interpolation\n",
    "                if csi_values.shape[1] > len(audio_data):\n",
    "                    step = csi_values.shape[1] // len(audio_data)\n",
    "                    csi_values = csi_values[:, ::step][:, :len(audio_data)]\n",
    "                else:\n",
    "                    # Simple repetition for upsampling\n",
    "                    repeat_factor = len(audio_data) // csi_values.shape[1] + 1\n",
    "                    csi_values = np.repeat(csi_values, repeat_factor, axis=1)[:, :len(audio_data)]\n",
    "            \n",
    "            return audio_data.astype(np.float32), csi_values.astype(np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Fast data loading failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            csi_path = self.data_df.iloc[idx]['csi_path']\n",
    "            audio_path = self.data_df.iloc[idx]['audio_path']\n",
    "            \n",
    "            audio_data, csi_data = self.load_data_fast(csi_path, audio_path)\n",
    "            \n",
    "            if audio_data is None or csi_data is None:\n",
    "                # Return small dummy data\n",
    "                csi_dummy = torch.randn(HYPERPARAMETERS['csi_channels'] * 2, self.chunk_size) * 0.01\n",
    "                audio_dummy = torch.randn(self.chunk_size) * 0.01\n",
    "                return csi_dummy, audio_dummy\n",
    "            \n",
    "            # Extract chunk efficiently\n",
    "            if len(audio_data) > self.chunk_size:\n",
    "                if self.is_training:\n",
    "                    max_start = len(audio_data) - self.chunk_size\n",
    "                    start_idx = random.randint(0, max_start)\n",
    "                else:\n",
    "                    start_idx = 0\n",
    "                \n",
    "                end_idx = start_idx + self.chunk_size\n",
    "                audio_chunk = audio_data[start_idx:end_idx]\n",
    "                csi_chunk = csi_data[:, start_idx:end_idx]\n",
    "            else:\n",
    "                audio_chunk = audio_data\n",
    "                csi_chunk = csi_data\n",
    "            \n",
    "            # Minimal processing\n",
    "            processed_audio, processed_csi = self.audio_preprocessor.process_audio_basic(\n",
    "                audio_chunk, csi_chunk\n",
    "            )\n",
    "            \n",
    "            # Flatten CSI features efficiently\n",
    "            csi_flat = processed_csi.reshape(-1, processed_csi.shape[-1])\n",
    "            \n",
    "            # Convert to tensors with minimal memory overhead\n",
    "            csi_tensor = torch.from_numpy(csi_flat).float()\n",
    "            audio_tensor = torch.from_numpy(processed_audio).float()\n",
    "            \n",
    "            # Simple normalization\n",
    "            if csi_tensor.numel() > 0:\n",
    "                csi_tensor = torch.clamp((csi_tensor - csi_tensor.mean()) / (csi_tensor.std() + 1e-8), -3, 3)\n",
    "            \n",
    "            if audio_tensor.numel() > 0:\n",
    "                audio_tensor = torch.clamp((audio_tensor - audio_tensor.mean()) / (audio_tensor.std() + 1e-8), -3, 3)\n",
    "            \n",
    "            return csi_tensor, audio_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"GetItem failed for {idx}: {e}\")\n",
    "            csi_dummy = torch.randn(HYPERPARAMETERS['csi_channels'] * 2, self.chunk_size) * 0.01\n",
    "            audio_dummy = torch.randn(self.chunk_size) * 0.01\n",
    "            return csi_dummy, audio_dummy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "# ============================================================================\n",
    "# LIGHTWEIGHT MODELS (Same structure, adjusted for BCE)\n",
    "# ============================================================================\n",
    "\n",
    "class LightweightResidualBlock(nn.Module):\n",
    "    \"\"\"Minimal residual block\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.1, inplace=True)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.leaky_relu(x + residual, 0.1, inplace=True)\n",
    "\n",
    "class OptimizedGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, csi_channels):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.csi_channels = csi_channels\n",
    "        self.chunk_size = HYPERPARAMETERS['chunk_size']\n",
    "        \n",
    "        # Reduced input dimensions (2 features per channel instead of 5)\n",
    "        self.csi_input_dim = csi_channels * 2 * self.chunk_size\n",
    "        self.total_input_dim = latent_dim + self.csi_input_dim\n",
    "        \n",
    "        logging.info(f\"Optimized Generator - total input: {self.total_input_dim}\")\n",
    "        \n",
    "        # Smaller FC layers\n",
    "        self.fc1 = nn.Linear(self.total_input_dim, 256, bias=False)\n",
    "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
    "        self.fc3 = nn.Linear(128, 64 * 32, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Reduced conv layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 48, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose1d(48, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            LightweightResidualBlock(32),\n",
    "            \n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose1d(16, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Keep Tanh for audio generation\n",
    "        )\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(self.chunk_size)\n",
    "\n",
    "    def forward(self, noise, csi_data):\n",
    "        batch_size = noise.size(0)\n",
    "        \n",
    "        csi_flat = csi_data.view(batch_size, -1)\n",
    "        x = torch.cat([noise, csi_flat], dim=1)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x), 0.1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.1)\n",
    "        \n",
    "        x = x.view(batch_size, 64, 32)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        return x.view(batch_size, self.chunk_size)\n",
    "\n",
    "class OptimizedDiscriminator(nn.Module):\n",
    "    def __init__(self, csi_channels):\n",
    "        super().__init__()\n",
    "        self.csi_channels = csi_channels\n",
    "        self.chunk_size = HYPERPARAMETERS['chunk_size']\n",
    "        \n",
    "        logging.info(f\"Optimized Discriminator initialized for BCE loss\")\n",
    "        \n",
    "        # Smaller conv layers for audio\n",
    "        self.audio_conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Smaller conv layers for CSI\n",
    "        self.csi_conv = nn.Sequential(\n",
    "            nn.Conv1d(csi_channels * 2, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Combined layers\n",
    "        self.combined_conv = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2)  # Higher dropout for BCE\n",
    "        )\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Smaller FC layers - no spectral norm for BCE\n",
    "        self.fc1 = nn.Linear(256, 64, bias=False)\n",
    "        self.fc2 = nn.Linear(64, 1)  # Single output for BCE\n",
    "\n",
    "    def forward(self, audio, csi, return_features=False):\n",
    "        batch_size = audio.size(0)\n",
    "        features = []\n",
    "\n",
    "        audio_in = audio.unsqueeze(1)\n",
    "        csi_in = csi\n",
    "        \n",
    "        audio_feat = self.audio_conv(audio_in)\n",
    "        csi_feat = self.csi_conv(csi_in)\n",
    "\n",
    "        # Ensure feature maps have the same length by padding/truncating the smaller one\n",
    "        len_a, len_c = audio_feat.shape[-1], csi_feat.shape[-1]\n",
    "        if len_a != len_c:\n",
    "            min_len = min(len_a, len_c)\n",
    "            audio_feat = audio_feat[..., :min_len]\n",
    "            csi_feat = csi_feat[..., :min_len]\n",
    "\n",
    "        x = torch.cat([audio_feat, csi_feat], dim=1)\n",
    "        if return_features: features.append(x.clone())\n",
    "        \n",
    "        x = self.combined_conv(x)\n",
    "        if return_features: features.append(x.clone())\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        output = self.fc2(x)  # Raw logits for BCE\n",
    "\n",
    "        if return_features:\n",
    "            return output, features\n",
    "        return output\n",
    "\n",
    "# ============================================================================\n",
    "# BCE LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def simple_drum_loss(generated, target):\n",
    "    \"\"\"Simplified drum loss\"\"\"\n",
    "    try:\n",
    "        return F.mse_loss(generated, target)\n",
    "    except:\n",
    "        return torch.tensor(0.0, device=generated.device)\n",
    "\n",
    "def simple_spectral_loss(generated, target):\n",
    "    \"\"\"Simplified spectral loss\"\"\"\n",
    "    try:\n",
    "        gen_fft = torch.fft.rfft(generated, dim=-1)\n",
    "        tar_fft = torch.fft.rfft(target, dim=-1)\n",
    "        return F.l1_loss(torch.abs(gen_fft), torch.abs(tar_fft))\n",
    "    except:\n",
    "        return F.l1_loss(generated, target)\n",
    "\n",
    "def simple_feature_matching_loss(real_features, fake_features):\n",
    "    \"\"\"Simplified feature matching\"\"\"\n",
    "    loss = 0\n",
    "    try:\n",
    "        for real_feat, fake_feat in zip(real_features, fake_features):\n",
    "            loss += F.l1_loss(fake_feat.mean(0), real_feat.mean(0))\n",
    "    except:\n",
    "        pass\n",
    "    return loss\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED BCE TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_bce_gan(generator, discriminator, train_dataloader, hyperparams):\n",
    "    \"\"\"Optimized training with BCE loss and detailed tracking\"\"\"\n",
    "    \n",
    "    # Optimizers & Schedulers\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=hyperparams['lr_generator'], \n",
    "                            betas=(hyperparams['beta1'], hyperparams['beta2']))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=hyperparams['lr_discriminator'], \n",
    "                            betas=(hyperparams['beta1'], hyperparams['beta2']))\n",
    "    scheduler_g = optim.lr_scheduler.ExponentialLR(optimizer_g, gamma=hyperparams['lr_scheduler_gamma'])\n",
    "    scheduler_d = optim.lr_scheduler.ExponentialLR(optimizer_d, gamma=hyperparams['lr_scheduler_gamma'])\n",
    "    \n",
    "    # BCE Loss with label smoothing\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler() if hyperparams['use_mixed_precision'] else None\n",
    "    device = hyperparams['device']\n",
    "    \n",
    "    # Label smoothing parameters\n",
    "    real_label_value = 1.0 - hyperparams['label_smoothing']\n",
    "    fake_label_value = hyperparams['label_smoothing']\n",
    "\n",
    "    # History tracking\n",
    "    history = {\n",
    "        'g_loss': [], 'g_adv': [], 'g_recon': [], 'g_fm': [], 'g_spec': [], 'g_drum': [],\n",
    "        'd_loss': [], 'd_real': [], 'd_fake': [],\n",
    "        'cosine_sim': []  # List of lists for boxplot\n",
    "    }\n",
    "\n",
    "    for epoch in range(hyperparams['epochs']):\n",
    "        epoch_losses = {key: 0.0 for key in history if key != 'cosine_sim'}\n",
    "        epoch_batch_sims = []  # Store batch-level similarities\n",
    "        num_batches = 0\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{hyperparams['epochs']}\")\n",
    "\n",
    "        for batch_idx, batch_data in enumerate(progress_bar):\n",
    "            if batch_data is None: continue\n",
    "\n",
    "            try:\n",
    "                csi_data, audio_data = batch_data\n",
    "                batch_size = csi_data.size(0)\n",
    "                if batch_size == 0: continue\n",
    "\n",
    "                csi_data = csi_data.to(device, non_blocking=True)\n",
    "                audio_data = audio_data.to(device, non_blocking=True)\n",
    "                \n",
    "                # Label smoothing\n",
    "                real_labels = torch.full((batch_size, 1), real_label_value, device=device)\n",
    "                fake_labels = torch.full((batch_size, 1), fake_label_value, device=device)\n",
    "\n",
    "                # --- Train Discriminator ---\n",
    "                optimizer_d.zero_grad()\n",
    "                noise = torch.randn(batch_size, hyperparams['latent_dim'], device=device)\n",
    "                \n",
    "                with autocast(enabled=scaler is not None):\n",
    "                    fake_audio = generator(noise, csi_data)\n",
    "                    \n",
    "                    # Train with real\n",
    "                    real_output = discriminator(audio_data, csi_data)\n",
    "                    d_loss_real = criterion(real_output, real_labels)\n",
    "                    \n",
    "                    # Train with fake\n",
    "                    fake_output = discriminator(fake_audio.detach(), csi_data)\n",
    "                    d_loss_fake = criterion(fake_output, fake_labels)\n",
    "                    \n",
    "                    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "                if scaler:\n",
    "                    scaler.scale(d_loss).backward()\n",
    "                    scaler.step(optimizer_d)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    d_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "                    optimizer_d.step()\n",
    "\n",
    "                # --- Train Generator ---\n",
    "                optimizer_g.zero_grad()\n",
    "                \n",
    "                with autocast(enabled=scaler is not None):\n",
    "                    fake_audio = generator(noise, csi_data)\n",
    "                    fake_output, fake_features = discriminator(fake_audio, csi_data, return_features=True)\n",
    "                    _, real_features = discriminator(audio_data, csi_data, return_features=True)\n",
    "                    \n",
    "                    # Generator losses\n",
    "                    g_loss_adv = criterion(fake_output, real_labels) * hyperparams['lambda_adversarial']\n",
    "                    g_loss_recon = F.l1_loss(fake_audio, audio_data) * hyperparams['lambda_reconstruction']\n",
    "                    g_loss_fm = simple_feature_matching_loss(real_features, fake_features) * hyperparams['lambda_feature_matching']\n",
    "                    g_loss_spec = simple_spectral_loss(fake_audio, audio_data) * hyperparams['lambda_spectral']\n",
    "                    g_loss_drum = simple_drum_loss(fake_audio, audio_data) * hyperparams['lambda_drum_loss']\n",
    "                    \n",
    "                    g_loss = g_loss_adv + g_loss_recon + g_loss_fm + g_loss_spec + g_loss_drum\n",
    "\n",
    "                if scaler:\n",
    "                    scaler.scale(g_loss).backward()\n",
    "                    scaler.step(optimizer_g)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    g_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "                    optimizer_g.step()\n",
    "                \n",
    "                # --- Accumulate losses and metrics ---\n",
    "                cos_sim = F.cosine_similarity(fake_audio.detach(), audio_data, dim=-1).mean().item()\n",
    "                epoch_batch_sims.append(cos_sim)\n",
    "                \n",
    "                # Accumulate losses for logging\n",
    "                for key, val in [('g_loss', g_loss), ('g_adv', g_loss_adv), ('g_recon', g_loss_recon), \n",
    "                                 ('g_fm', g_loss_fm), ('g_spec', g_loss_spec), ('g_drum', g_loss_drum), \n",
    "                                 ('d_loss', d_loss), ('d_real', d_loss_real), ('d_fake', d_loss_fake)]:\n",
    "                    epoch_losses[key] += val.item() if isinstance(val, torch.Tensor) else val\n",
    "                num_batches += 1\n",
    "\n",
    "                progress_bar.set_postfix({'D_loss': f'{d_loss.item():.4f}', 'G_loss': f'{g_loss.item():.4f}', 'Cos_Sim': f'{cos_sim:.4f}'})\n",
    "                if batch_idx % 20 == 0: gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # --- End of Epoch ---\n",
    "        scheduler_g.step()\n",
    "        scheduler_d.step()\n",
    "\n",
    "        if num_batches > 0:\n",
    "            for key in epoch_losses:\n",
    "                history[key].append(epoch_losses[key] / num_batches)\n",
    "            history['cosine_sim'].append(epoch_batch_sims)\n",
    "            \n",
    "            logging.info(\n",
    "                f\"Epoch {epoch+1}: D_loss={history['d_loss'][-1]:.4f}, G_loss={history['g_loss'][-1]:.4f}, Avg Cos_Sim={np.mean(epoch_batch_sims):.4f}\"\n",
    "            )\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return history\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY AND PLOTTING FUNCTIONS (Modified for BCE)\n",
    "# ============================================================================\n",
    "\n",
    "def plot_bce_results(history, output_dir):\n",
    "    \"\"\"Plots BCE training losses (time series) and cosine similarity (box plot).\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "    epochs = range(1, len(history['g_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 14), gridspec_kw={'height_ratios': [3, 2]})\n",
    "    fig.suptitle('BCE GAN Training Metrics', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # --- Top Plot: All Loss Components (Time Series) ---\n",
    "    ax1.plot(epochs, history['g_loss'], label='Total Gen Loss', color='blue', linewidth=2.5, zorder=10)\n",
    "    ax1.plot(epochs, history['d_loss'], label='Total Disc Loss', color='red', linewidth=2.5, zorder=10)\n",
    "    ax1.plot(epochs, history['g_adv'], label='Gen Adversarial', color='cyan', linestyle='--', alpha=0.8)\n",
    "    ax1.plot(epochs, history['g_recon'], label='Gen Reconstruction', color='limegreen', linestyle='--', alpha=0.8)\n",
    "    ax1.plot(epochs, history['g_fm'], label='Gen Feature Matching', color='purple', linestyle='--', alpha=0.8)\n",
    "    ax1.plot(epochs, history['d_real'], label='Disc Loss Real', color='orange', linestyle=':', alpha=0.8)\n",
    "    ax1.plot(epochs, history['d_fake'], label='Disc Loss Fake', color='gold', linestyle=':', alpha=0.8)\n",
    "\n",
    "    ax1.set_title('BCE Training Loss Components Over Epochs', fontsize=16)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss Value', fontsize=12)\n",
    "    ax1.legend(loc='upper right', ncol=2, fontsize=9)\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax1.set_yscale('log')  # Log scale often better for BCE losses\n",
    "    \n",
    "    # --- Bottom Plot: Cosine Similarity (Box Plot) ---\n",
    "    sim_data = history['cosine_sim']\n",
    "    positions = epochs\n",
    "    \n",
    "    bp = ax2.boxplot(sim_data, positions=positions, manage_ticks=False, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                     whiskerprops=dict(color='blue'),\n",
    "                     capprops=dict(color='blue'),\n",
    "                     medianprops=dict(color='red', linewidth=2))\n",
    "\n",
    "    ax2.set_title('Distribution of Cosine Similarity Over Epochs', fontsize=16)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Similarity Score', fontsize=12)\n",
    "    ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax2.set_ylim(-1.05, 1.05)\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Set x-ticks to be integers representing epochs\n",
    "    tick_spacing = max(1, len(epochs) // 15)\n",
    "    ax2.set_xticks(epochs[::tick_spacing])\n",
    "    ax2.set_xticklabels(epochs[::tick_spacing])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plot_path = os.path.join(output_dir, 'bce_training_metrics.png')\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    logging.info(f\"Saved BCE training metrics plot to {plot_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS (Same as before)\n",
    "# ============================================================================\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    \"\"\"Extract timestamp from filename\"\"\"\n",
    "    try:\n",
    "        pattern = r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\\.\\d+)'\n",
    "        match = re.search(pattern, filename)\n",
    "        return match.group(1) if match else None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting timestamp from {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def determine_sample_rate_and_length(audio_directory):\n",
    "    \"\"\"Determine sample rate and sequence length\"\"\"\n",
    "    audio_files = [f for f in os.listdir(audio_directory) if f.endswith('.wav')]\n",
    "    if not audio_files: raise FileNotFoundError(f\"No WAV files found in {audio_directory}\")\n",
    "    info = sf.info(os.path.join(audio_directory, audio_files[0]))\n",
    "    sample_rate = info.samplerate\n",
    "    sequence_length = int(sample_rate * HYPERPARAMETERS['audio_duration'])\n",
    "    logging.info(f\"Determined sample rate: {sample_rate}Hz, sequence length: {sequence_length}\")\n",
    "    return sample_rate, sequence_length\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare data\"\"\"\n",
    "    try:\n",
    "        sample_rate, sequence_length = determine_sample_rate_and_length(AUDIO_DIRECTORY)\n",
    "        HYPERPARAMETERS['sample_rate'] = sample_rate\n",
    "        HYPERPARAMETERS['sequence_length'] = sequence_length\n",
    "        csi_files = [f for f in os.listdir(CSI_DIRECTORY) if f.startswith('top8_filtered_csi_data_') and f.endswith('.csv')]\n",
    "        audio_files = [f for f in os.listdir(AUDIO_DIRECTORY) if f.endswith('.wav')]\n",
    "        logging.info(f\"Found {len(csi_files)} CSI files and {len(audio_files)} audio files\")\n",
    "        csi_timestamps = {extract_timestamp(f): f for f in csi_files if extract_timestamp(f)}\n",
    "        audio_timestamps = {extract_timestamp(f): f for f in audio_files if extract_timestamp(f)}\n",
    "        common_timestamps = sorted(set(csi_timestamps.keys()) & set(audio_timestamps.keys()))\n",
    "        logging.info(f\"Found {len(common_timestamps)} matching pairs\")\n",
    "        data_pairs = [{\"timestamp\": ts, \"csi_path\": os.path.join(CSI_DIRECTORY, csi_timestamps[ts]), \"audio_path\": os.path.join(AUDIO_DIRECTORY, audio_timestamps[ts])} for ts in common_timestamps]\n",
    "        df = pd.DataFrame(data_pairs)\n",
    "        train_df, test_df = train_test_split(df, test_size=1-HYPERPARAMETERS['train_test_split'], random_state=HYPERPARAMETERS['random_seed'], shuffle=True)\n",
    "        logging.info(f\"Data split: {len(train_df)} train, {len(test_df)} test\")\n",
    "        return train_df, test_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Data preparation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Fast collate function\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch: return None\n",
    "    try:\n",
    "        csi_data = torch.stack([item[0] for item in batch])\n",
    "        audio_data = torch.stack([item[1] for item in batch])\n",
    "        return csi_data, audio_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Collate error: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Optimized main execution with BCE loss\"\"\"\n",
    "    try:\n",
    "        logging.info(\"=\" * 80)\n",
    "        logging.info(\"STARTING OPTIMIZED CSI-AUDIO GAN TRAINING WITH BCE LOSS\")\n",
    "        logging.info(\"=\" * 80)\n",
    "        \n",
    "        train_df, test_df = load_and_prepare_data()\n",
    "        \n",
    "        logging.info(f\"BCE OPTIMIZED CONFIGURATION:\")\n",
    "        for key, value in HYPERPARAMETERS.items():\n",
    "            logging.info(f\"  {key}: {value}\")\n",
    "\n",
    "        train_dataset = OptimizedCSIAudioDataset(train_df, is_training=True)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=HYPERPARAMETERS['batch_size'], shuffle=True, collate_fn=collate_fn, drop_last=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "        generator = OptimizedGenerator(HYPERPARAMETERS['latent_dim'], HYPERPARAMETERS['csi_channels']).to(HYPERPARAMETERS['device'])\n",
    "        discriminator = OptimizedDiscriminator(HYPERPARAMETERS['csi_channels']).to(HYPERPARAMETERS['device'])\n",
    "\n",
    "        total_g_params = sum(p.numel() for p in generator.parameters())\n",
    "        total_d_params = sum(p.numel() for p in discriminator.parameters())\n",
    "        logging.info(f\"BCE Generator parameters: {total_g_params:,}\")\n",
    "        logging.info(f\"BCE Discriminator parameters: {total_d_params:,}\")\n",
    "\n",
    "        logging.info(\"Starting BCE GAN training...\")\n",
    "        history = train_bce_gan(generator, discriminator, train_dataloader, HYPERPARAMETERS)\n",
    "\n",
    "        torch.save(generator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \"bce_generator.pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \"bce_discriminator.pth\"))\n",
    "\n",
    "        # Create a history copy for clean CSV saving\n",
    "        history_for_csv = {key: val for key, val in history.items() if key != 'cosine_sim'}\n",
    "        history_for_csv['avg_cosine_sim'] = [np.mean(epoch_sims) if epoch_sims else 0 for epoch_sims in history['cosine_sim']]\n",
    "        history_df = pd.DataFrame(history_for_csv)\n",
    "        history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "        history_df.to_csv(os.path.join(OUTPUT_DIRECTORY, 'bce_training_history.csv'), index=False)\n",
    "        \n",
    "        if history['g_loss']:\n",
    "            plot_bce_results(history, OUTPUT_DIRECTORY)\n",
    "\n",
    "        logging.info(\"=\" * 80)\n",
    "        logging.info(\"BCE GAN TRAINING COMPLETED!\")\n",
    "        logging.info(\"=\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"BCE training failed: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 11:13:42,533 - INFO - ================================================================================\n",
      "STARTING OPTIMIZED CSI-AUDIO GAN TRAINING\n",
      "================================================================================\n",
      "2025-09-16 11:13:42,552 - INFO - Determined SR: 44100Hz, Sequence Length: 441000\n",
      "2025-09-16 11:13:42,562 - INFO - Found 51 matching pairs\n",
      "2025-09-16 11:13:42,568 - ERROR - Training failed: name 'train_test_split' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_3780\\1410714153.py\", line 609, in main\n",
      "    train_df, _ = load_and_prepare_data()\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Srach\\AppData\\Local\\Temp\\ipykernel_3780\\1410714153.py\", line 590, in load_and_prepare_data\n",
      "    train_df, test_df = train_test_split(df, test_size=1 - HYPERPARAMETERS['train_test_split'],\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "NameError: name 'train_test_split' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 644\u001b[39m\n\u001b[32m    641\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 609\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    608\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSTARTING OPTIMIZED CSI-AUDIO GAN TRAINING\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     train_df, _ = \u001b[43mload_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    611\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mOPTIMIZED CONFIGURATION:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    612\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m HYPERPARAMETERS.items(): logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 590\u001b[39m, in \u001b[36mload_and_prepare_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    586\u001b[39m data = [{\u001b[33m'\u001b[39m\u001b[33mcsi_path\u001b[39m\u001b[33m'\u001b[39m: os.path.join(CSI_DIRECTORY, csi_files[ts]), \n\u001b[32m    587\u001b[39m          \u001b[33m'\u001b[39m\u001b[33maudio_path\u001b[39m\u001b[33m'\u001b[39m: os.path.join(AUDIO_DIRECTORY, audio_files[ts])} \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m common_ts]\n\u001b[32m    589\u001b[39m df = pd.DataFrame(data)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m train_df, test_df = \u001b[43mtrain_test_split\u001b[49m(df, test_size=\u001b[32m1\u001b[39m - HYPERPARAMETERS[\u001b[33m'\u001b[39m\u001b[33mtrain_test_split\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m    591\u001b[39m                                      random_state=HYPERPARAMETERS[\u001b[33m'\u001b[39m\u001b[33mrandom_seed\u001b[39m\u001b[33m'\u001b[39m], shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    592\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m train, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m test\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m train_df, test_df\n",
      "\u001b[31mNameError\u001b[39m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import spectral_norm\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import random\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Force garbage collection\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CSI_DIRECTORY = \"data/processed_csi\"\n",
    "AUDIO_DIRECTORY = \"data/audio\"\n",
    "OUTPUT_DIRECTORY = \"output\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Re-balanced hyperparameters for more systematic learning and stability\n",
    "HYPERPARAMETERS = {\n",
    "    \"latent_dim\": 48,\n",
    "    \"csi_channels\": 8,\n",
    "    \"audio_channels\": 1,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 8, \n",
    "    \n",
    "    \"lr_generator\": 0.00006,\n",
    "    \"lr_discriminator\": 0.00005,\n",
    "    \"lr_scheduler_gamma\": 0.95,\n",
    "\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"sample_rate\": None,\n",
    "    \"audio_duration\": 10.0,\n",
    "    \"sequence_length\": None,\n",
    "    \n",
    "    \"chunk_size\": 2048,\n",
    "    \"overlap\": 256,\n",
    "    \"max_chunks_per_sample\": 12,\n",
    "\n",
    "    # Loss weights adjusted for stability\n",
    "    \"lambda_reconstruction\": 10.0,\n",
    "    \"lambda_feature_matching\": 15.0,\n",
    "    \"lambda_gradient_penalty\": 10.0,\n",
    "    \"lambda_spectral\": 5.0,\n",
    "    \"lambda_drum_loss\": 10.0,\n",
    "\n",
    "    \"n_critic\": 2,\n",
    "    \"warmup_epochs\": 10,\n",
    "    \n",
    "    \"pretrain_epochs\": 5,\n",
    "    \"label_smoothing\": 0.1,\n",
    "\n",
    "    \"train_test_split\": 0.8,\n",
    "    \"random_seed\": 42,\n",
    "\n",
    "    # Memory optimizations\n",
    "    \"use_mixed_precision\": True,\n",
    "    \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# LIGHTWEIGHT CSI PREPROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "class LightweightCSIPreprocessor:\n",
    "    \"\"\"Minimal CSI preprocessing for speed\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate):\n",
    "        self.sample_rate = sample_rate\n",
    "        logging.info(\"Lightweight CSI preprocessor initialized\")\n",
    "    \n",
    "    def extract_basic_features(self, csi_data):\n",
    "        \"\"\"Extract only 2 essential features instead of 5\"\"\"\n",
    "        try:\n",
    "            features_list = []\n",
    "            \n",
    "            for channel_idx in range(csi_data.shape[0]):\n",
    "                channel_data = csi_data[channel_idx]\n",
    "                channel_features = []\n",
    "                \n",
    "                # 1. Energy variations only\n",
    "                energy_variations = np.abs(channel_data) ** 2\n",
    "                channel_features.append(energy_variations)\n",
    "                \n",
    "                # 2. First-order differences only\n",
    "                diff = np.diff(channel_data, prepend=channel_data[0])\n",
    "                channel_features.append(diff)\n",
    "                \n",
    "                channel_features_array = np.stack(channel_features, axis=0)\n",
    "                features_list.append(channel_features_array)\n",
    "            \n",
    "            all_features = np.stack(features_list, axis=0)\n",
    "            return all_features.astype(np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Feature extraction failed: {e}\")\n",
    "            return np.expand_dims(csi_data, axis=1).astype(np.float32)\n",
    "\n",
    "# ============================================================================\n",
    "# LIGHTWEIGHT AUDIO PREPROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "class LightweightAudioPreprocessor:\n",
    "    \"\"\"Minimal audio preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.csi_preprocessor = LightweightCSIPreprocessor(sample_rate)\n",
    "        logging.info(\"Lightweight audio preprocessor initialized\")\n",
    "    \n",
    "    def process_audio_basic(self, audio, csi_data):\n",
    "        \"\"\"Basic processing without heavy filtering\"\"\"\n",
    "        try:\n",
    "            csi_features = self.csi_preprocessor.extract_basic_features(csi_data)\n",
    "            \n",
    "            if np.max(np.abs(audio)) > 0:\n",
    "                audio = audio / np.max(np.abs(audio)) * 0.8\n",
    "            \n",
    "            return audio, csi_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Audio processing failed: {e}\")\n",
    "            fallback_csi = np.expand_dims(csi_data, axis=1).astype(np.float32)\n",
    "            return audio, fallback_csi\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class OptimizedCSIAudioDataset(Dataset):\n",
    "    def __init__(self, data_df, is_training=True):\n",
    "        self.data_df = data_df.reset_index(drop=True)\n",
    "        self.is_training = is_training\n",
    "        self.sample_rate = HYPERPARAMETERS['sample_rate']\n",
    "        self.sequence_length = HYPERPARAMETERS['sequence_length']\n",
    "        self.chunk_size = HYPERPARAMETERS['chunk_size']\n",
    "        self.audio_preprocessor = LightweightAudioPreprocessor(self.sample_rate)\n",
    "        self._validate_files()\n",
    "        logging.info(f\"Optimized dataset initialized: {len(self.data_df)} samples\")\n",
    "    \n",
    "    def _validate_files(self):\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.data_df)):\n",
    "            csi_path = self.data_df.iloc[idx]['csi_path']\n",
    "            audio_path = self.data_df.iloc[idx]['audio_path']\n",
    "            if os.path.exists(csi_path) and os.path.exists(audio_path):\n",
    "                try:\n",
    "                    audio_info = sf.info(audio_path)\n",
    "                    if audio_info.duration >= 1.0:\n",
    "                        valid_indices.append(idx)\n",
    "                except:\n",
    "                    continue\n",
    "        self.data_df = self.data_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        logging.info(f\"Validated {len(self.data_df)} samples\")\n",
    "\n",
    "    def load_data_fast(self, csi_path, audio_path):\n",
    "        try:\n",
    "            audio_data, file_sample_rate = sf.read(audio_path)\n",
    "            if len(audio_data.shape) > 1:\n",
    "                audio_data = audio_data.mean(axis=1)\n",
    "            \n",
    "            if file_sample_rate != self.sample_rate:\n",
    "                audio_data = librosa.resample(audio_data, orig_sr=file_sample_rate, \n",
    "                                              target_sr=self.sample_rate, res_type='kaiser_fast')\n",
    "            \n",
    "            target_length = min(len(audio_data), self.sequence_length)\n",
    "            if len(audio_data) > target_length:\n",
    "                audio_data = audio_data[:target_length]\n",
    "            elif len(audio_data) < self.chunk_size:\n",
    "                audio_data = np.pad(audio_data, (0, self.chunk_size - len(audio_data)))\n",
    "            \n",
    "            try:\n",
    "                csi_data = pd.read_csv(csi_path, usecols=lambda x: x.startswith('subcarrier_'))\n",
    "                csi_values = csi_data.values.T[:HYPERPARAMETERS['csi_channels']]\n",
    "            except:\n",
    "                csi_data = pd.read_csv(csi_path)\n",
    "                subcarrier_cols = [col for col in csi_data.columns if col.startswith('subcarrier_')]\n",
    "                csi_values = csi_data[subcarrier_cols].values.T[:HYPERPARAMETERS['csi_channels']]\n",
    "            \n",
    "            if csi_values.shape[1] > len(audio_data):\n",
    "                step = csi_values.shape[1] // len(audio_data)\n",
    "                csi_values = csi_values[:, ::step][:, :len(audio_data)]\n",
    "            elif csi_values.shape[1] < len(audio_data):\n",
    "                repeat_factor = len(audio_data) // csi_values.shape[1] + 1\n",
    "                csi_values = np.repeat(csi_values, repeat_factor, axis=1)[:, :len(audio_data)]\n",
    "            \n",
    "            return audio_data.astype(np.float32), csi_values.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Fast data loading failed for {csi_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            csi_path = self.data_df.iloc[idx]['csi_path']\n",
    "            audio_path = self.data_df.iloc[idx]['audio_path']\n",
    "            audio_data, csi_data = self.load_data_fast(csi_path, audio_path)\n",
    "            \n",
    "            if audio_data is None or csi_data is None or len(audio_data) < self.chunk_size:\n",
    "                csi_dummy = torch.randn(HYPERPARAMETERS['csi_channels'] * 2, self.chunk_size) * 0.01\n",
    "                audio_dummy = torch.randn(self.chunk_size) * 0.01\n",
    "                return csi_dummy, audio_dummy\n",
    "            \n",
    "            max_start = len(audio_data) - self.chunk_size\n",
    "            start_idx = random.randint(0, max_start) if self.is_training else 0\n",
    "            end_idx = start_idx + self.chunk_size\n",
    "            audio_chunk = audio_data[start_idx:end_idx]\n",
    "            csi_chunk = csi_data[:, start_idx:end_idx]\n",
    "            \n",
    "            processed_audio, processed_csi = self.audio_preprocessor.process_audio_basic(audio_chunk, csi_chunk)\n",
    "            csi_flat = processed_csi.reshape(-1, processed_csi.shape[-1])\n",
    "            csi_tensor = torch.from_numpy(csi_flat).float()\n",
    "            audio_tensor = torch.from_numpy(processed_audio).float()\n",
    "            \n",
    "            if csi_tensor.numel() > 0:\n",
    "                csi_tensor = torch.clamp((csi_tensor - csi_tensor.mean()) / (csi_tensor.std() + 1e-8), -3, 3)\n",
    "            if audio_tensor.numel() > 0:\n",
    "                audio_tensor = torch.clamp((audio_tensor - audio_tensor.mean()) / (audio_tensor.std() + 1e-8), -3, 3)\n",
    "            \n",
    "            return csi_tensor, audio_tensor\n",
    "        except Exception as e:\n",
    "            logging.error(f\"GetItem failed for index {idx}: {e}\")\n",
    "            csi_dummy = torch.randn(HYPERPARAMETERS['csi_channels'] * 2, self.chunk_size) * 0.01\n",
    "            audio_dummy = torch.randn(self.chunk_size) * 0.01\n",
    "            return csi_dummy, audio_dummy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "# ============================================================================\n",
    "# LIGHTWEIGHT MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class LightweightResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.1, inplace=True)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.leaky_relu(x + residual, 0.1, inplace=True)\n",
    "\n",
    "class OptimizedGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, csi_channels):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.chunk_size = HYPERPARAMETERS['chunk_size']\n",
    "        self.csi_input_dim = csi_channels * 2 * self.chunk_size\n",
    "        self.total_input_dim = latent_dim + self.csi_input_dim\n",
    "        \n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Linear(self.total_input_dim, 256, bias=False),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128, bias=False),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64 * 32, bias=False),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 48, 4, 2, 1, bias=False), nn.BatchNorm1d(48), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.ConvTranspose1d(48, 32, 4, 2, 1, bias=False), nn.BatchNorm1d(32), nn.LeakyReLU(0.1, inplace=True),\n",
    "            LightweightResidualBlock(32),\n",
    "            nn.ConvTranspose1d(32, 16, 4, 2, 1, bias=False), nn.BatchNorm1d(16), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 1, 4, 2, 1), nn.Tanh()\n",
    "        )\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(self.chunk_size)\n",
    "\n",
    "    def forward(self, noise, csi_data):\n",
    "        csi_flat = csi_data.view(noise.size(0), -1)\n",
    "        x = torch.cat([noise, csi_flat], dim=1)\n",
    "        x = self.fc_stack(x)\n",
    "        x = x.view(noise.size(0), 64, 32)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x.view(noise.size(0), self.chunk_size)\n",
    "\n",
    "class OptimizedDiscriminator(nn.Module):\n",
    "    def __init__(self, csi_channels):\n",
    "        super().__init__()\n",
    "        csi_feat_channels = csi_channels * 2\n",
    "        \n",
    "        self.audio_conv = self._make_conv_block(1, [32, 64, 128])\n",
    "        self.csi_conv = self._make_conv_block(csi_feat_channels, [32, 64, 128])\n",
    "        \n",
    "        self.combined_conv = nn.Sequential(\n",
    "            spectral_norm(nn.Conv1d(256, 256, 4, 2, 1, bias=False)), nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True), nn.Dropout(0.1)\n",
    "        )\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(256, 64, bias=False)), nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(64, 1))\n",
    "        )\n",
    "        \n",
    "    def _make_conv_block(self, in_channels, channel_list):\n",
    "        layers = []\n",
    "        for out_channels in channel_list:\n",
    "            layers.extend([\n",
    "                spectral_norm(nn.Conv1d(in_channels, out_channels, 4, 2, 1, bias=False)),\n",
    "                nn.BatchNorm1d(out_channels), nn.LeakyReLU(0.2, inplace=True)\n",
    "            ])\n",
    "            in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, audio, csi, return_features=False):\n",
    "        features = []\n",
    "        # Discriminator's conv1d expects a 3D tensor: [batch, channels, length]\n",
    "        audio_in = audio if audio.dim() == 3 else audio.unsqueeze(1)\n",
    "        \n",
    "        audio_feat = self.audio_conv(audio_in)\n",
    "        csi_feat = self.csi_conv(csi)\n",
    "        \n",
    "        min_len = min(audio_feat.shape[-1], csi_feat.shape[-1])\n",
    "        x = torch.cat([audio_feat[..., :min_len], csi_feat[..., :min_len]], dim=1)\n",
    "        if return_features: features.append(x.clone())\n",
    "        \n",
    "        x = self.combined_conv(x)\n",
    "        if return_features: features.append(x.clone())\n",
    "        \n",
    "        x = self.adaptive_pool(x).view(audio.size(0), -1)\n",
    "        output = self.fc_stack(x)\n",
    "        \n",
    "        return (output, features) if return_features else output\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS FUNCTIONS & TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def simple_spectral_loss(gen, tar):\n",
    "    if gen.numel() == 0 or tar.numel() == 0: return torch.tensor(0.0, device=gen.device)\n",
    "    try:\n",
    "        gen_fft = torch.fft.rfft(gen, dim=-1)\n",
    "        tar_fft = torch.fft.rfft(tar, dim=-1)\n",
    "        return F.l1_loss(torch.abs(gen_fft), torch.abs(tar_fft))\n",
    "    except Exception:\n",
    "        return F.l1_loss(gen, tar)\n",
    "\n",
    "def simple_feature_matching_loss(real_features, fake_features):\n",
    "    loss = 0.0\n",
    "    for real_feat, fake_feat in zip(real_features, fake_features):\n",
    "        if real_feat.numel() > 0 and fake_feat.numel() > 0:\n",
    "            loss += F.l1_loss(fake_feat.mean(0), real_feat.mean(0))\n",
    "    return loss\n",
    "\n",
    "def gradient_penalty_light(disc, real, fake, csi, device):\n",
    "    \"\"\"FIXED: Correctly shapes the interpolated tensor for gradient calculation.\"\"\"\n",
    "    try:\n",
    "        batch_size = real.size(0)\n",
    "        # Alpha should be [batch_size, 1] to scale each sample independently\n",
    "        alpha = torch.rand(batch_size, 1, device=device)\n",
    "        # Expand alpha to match the audio dimensions for correct broadcasting\n",
    "        alpha = alpha.expand(real.size())\n",
    "\n",
    "        interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "        \n",
    "        d_interpolated = disc(interpolated, csi)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated, inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_interpolated),\n",
    "            create_graph=True, retain_graph=True, only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return penalty\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Gradient penalty calculation failed: {e}. Returning 0.0.\")\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "def train_optimized_gan(generator, discriminator, train_dataloader, hyperparams):\n",
    "    device = hyperparams['device']\n",
    "    # FIXED: Set autocast device_type dynamically\n",
    "    autocast_device_type = 'cuda' if device.type == 'cuda' else 'cpu'\n",
    "\n",
    "    optimizer_g = optim.AdamW(generator.parameters(), lr=hyperparams['lr_generator'], betas=(hyperparams['beta1'], hyperparams['beta2']))\n",
    "    optimizer_d = optim.AdamW(discriminator.parameters(), lr=hyperparams['lr_discriminator'], betas=(hyperparams['beta1'], hyperparams['beta2']))\n",
    "    scheduler_g = optim.lr_scheduler.ExponentialLR(optimizer_g, gamma=hyperparams['lr_scheduler_gamma'])\n",
    "    scheduler_d = optim.lr_scheduler.ExponentialLR(optimizer_d, gamma=hyperparams['lr_scheduler_gamma'])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler(enabled=hyperparams['use_mixed_precision'])\n",
    "    \n",
    "    history = {k: [] for k in ['g_loss', 'g_adv', 'g_recon', 'g_fm', 'g_spec', 'g_drum', \n",
    "                               'd_loss', 'd_real', 'd_fake', 'd_gp']}\n",
    "\n",
    "    for epoch in range(hyperparams['epochs']):\n",
    "        epoch_losses = {key: 0.0 for key in history}\n",
    "        num_batches = 0\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{hyperparams['epochs']}\")\n",
    "\n",
    "        for batch_data in progress_bar:\n",
    "            if batch_data is None: continue\n",
    "            csi_data, audio_data = batch_data\n",
    "            \n",
    "            csi_data, audio_data = csi_data.to(device), audio_data.to(device)\n",
    "            batch_size = csi_data.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            # --- Train Discriminator ---\n",
    "            optimizer_d.zero_grad(set_to_none=True)\n",
    "            # FIXED: Updated autocast syntax\n",
    "            with torch.amp.autocast(device_type=autocast_device_type, enabled=hyperparams['use_mixed_precision']):\n",
    "                real_labels = torch.full((batch_size, 1), 1.0 - hyperparams['label_smoothing'], device=device)\n",
    "                fake_labels = torch.full((batch_size, 1), hyperparams['label_smoothing'], device=device)\n",
    "                \n",
    "                real_output = discriminator(audio_data, csi_data)\n",
    "                d_loss_real = criterion(real_output, real_labels)\n",
    "                \n",
    "                noise = torch.randn(batch_size, hyperparams['latent_dim'], device=device)\n",
    "                fake_audio = generator(noise, csi_data)\n",
    "                fake_output = discriminator(fake_audio.detach(), csi_data)\n",
    "                d_loss_fake = criterion(fake_output, fake_labels)\n",
    "                \n",
    "                gp = gradient_penalty_light(discriminator, audio_data, fake_audio.detach(), csi_data, device)\n",
    "                d_loss_gp = hyperparams['lambda_gradient_penalty'] * gp\n",
    "                d_loss = d_loss_real + d_loss_fake + d_loss_gp\n",
    "            \n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(optimizer_d)\n",
    "            scaler.update()\n",
    "\n",
    "            # --- Train Generator ---\n",
    "            if num_batches % hyperparams['n_critic'] == 0:\n",
    "                optimizer_g.zero_grad(set_to_none=True)\n",
    "                # FIXED: Updated autocast syntax\n",
    "                with torch.amp.autocast(device_type=autocast_device_type, enabled=hyperparams['use_mixed_precision']):\n",
    "                    noise = torch.randn(batch_size, hyperparams['latent_dim'], device=device)\n",
    "                    gen_audio = generator(noise, csi_data)\n",
    "                    gen_output, fake_features = discriminator(gen_audio, csi_data, return_features=True)\n",
    "                    with torch.no_grad():\n",
    "                        _, real_features = discriminator(audio_data, csi_data, return_features=True)\n",
    "                    \n",
    "                    g_loss_recon = F.l1_loss(gen_audio, audio_data) * hyperparams['lambda_reconstruction']\n",
    "                    g_loss_spec = simple_spectral_loss(gen_audio, audio_data) * hyperparams['lambda_spectral']\n",
    "                    g_loss_drum = F.mse_loss(gen_audio, audio_data) * hyperparams['lambda_drum_loss']\n",
    "\n",
    "                    if epoch < hyperparams['pretrain_epochs']:\n",
    "                        g_loss_adv = torch.tensor(0.0, device=device)\n",
    "                        g_loss_fm = torch.tensor(0.0, device=device)\n",
    "                        g_loss = g_loss_recon + g_loss_spec + g_loss_drum\n",
    "                    else:\n",
    "                        g_loss_adv = criterion(gen_output, torch.ones_like(gen_output))\n",
    "                        g_loss_fm = simple_feature_matching_loss(real_features, fake_features) * hyperparams['lambda_feature_matching']\n",
    "                        g_loss = g_loss_adv + g_loss_recon + g_loss_fm + g_loss_spec + g_loss_drum\n",
    "                \n",
    "                scaler.scale(g_loss).backward()\n",
    "                scaler.step(optimizer_g)\n",
    "                scaler.update()\n",
    "\n",
    "            # Accumulate losses - Explicitly cast to float32 before item()\n",
    "            epoch_losses['g_loss'] += g_loss.float().item()\n",
    "            epoch_losses['g_adv'] += g_loss_adv.float().item()\n",
    "            epoch_losses['g_recon'] += g_loss_recon.float().item()\n",
    "            epoch_losses['g_fm'] += g_loss_fm.float().item()\n",
    "            epoch_losses['g_spec'] += g_loss_spec.float().item()\n",
    "            epoch_losses['g_drum'] += g_loss_drum.float().item()\n",
    "            \n",
    "            epoch_losses['d_loss'] += d_loss.float().item()\n",
    "            epoch_losses['d_real'] += d_loss_real.float().item()\n",
    "            epoch_losses['d_fake'] += d_loss_fake.float().item()\n",
    "            epoch_losses['d_gp'] += d_loss_gp.float().item()\n",
    "\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix({'D_loss': f'{d_loss.float().item():.3f}', 'G_loss': f'{g_loss.float().item():.3f}'})\n",
    "\n",
    "        if epoch >= hyperparams['warmup_epochs']:\n",
    "            scheduler_g.step(); scheduler_d.step()\n",
    "        \n",
    "        if num_batches > 0:\n",
    "            for key in history:\n",
    "                history[key].append(epoch_losses[key] / num_batches)\n",
    "            logging.info(f\"Epoch {epoch+1}: D_loss={history['d_loss'][-1]:.4f}, G_loss={history['g_loss'][-1]:.4f}\")\n",
    "        \n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "    return history\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY AND PLOTTING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_results(history, output_dir):\n",
    "    \"\"\"Creates individual subplots for major loss groups for clarity.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "    epochs = range(1, len(history['g_loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 18), sharex=True)\n",
    "    fig.suptitle('GAN Training Loss Breakdown', fontsize=20, fontweight='bold')\n",
    "\n",
    "    # --- Plot 1: Total Losses ---\n",
    "    axes[0].plot(epochs, history['g_loss'], label='Total Gen Loss', color='blue', linewidth=2)\n",
    "    axes[0].plot(epochs, history['d_loss'], label='Total Disc Loss', color='red', linewidth=2)\n",
    "    axes[0].set_title('Total Generator vs. Discriminator Loss', fontsize=16)\n",
    "    axes[0].set_ylabel('Loss Value', fontsize=12)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # --- Plot 2: Generator Loss Components ---\n",
    "    axes[1].plot(epochs, history['g_adv'], label='Adversarial', color='cyan', linestyle='--')\n",
    "    axes[1].plot(epochs, history['g_recon'], label='Reconstruction', color='limegreen', linestyle='--')\n",
    "    axes[1].plot(epochs, history['g_fm'], label='Feature Matching', color='purple', linestyle='--')\n",
    "    axes[1].plot(epochs, history['g_spec'], label='Spectral', color='magenta', linestyle=':')\n",
    "    axes[1].plot(epochs, history['g_drum'], label='Drum', color='yellowgreen', linestyle=':')\n",
    "    axes[1].set_title('Generator Loss Components', fontsize=16)\n",
    "    axes[1].set_ylabel('Loss Value', fontsize=12)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # --- Plot 3: Discriminator Loss Components ---\n",
    "    axes[2].plot(epochs, history['d_real'], label='Loss on Real', color='orange', linestyle='--')\n",
    "    axes[2].plot(epochs, history['d_fake'], label='Loss on Fake', color='gold', linestyle='--')\n",
    "    axes[2].plot(epochs, history['d_gp'], label='Gradient Penalty', color='brown', linestyle='--')\n",
    "    axes[2].set_title('Discriminator Loss Components', fontsize=16)\n",
    "    axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[2].set_ylabel('Loss Value', fontsize=12)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    for ax in axes:\n",
    "        if ax.get_ylim()[1] > 1000: # Use symlog for very large loss values\n",
    "            ax.set_yscale('symlog', linthresh=0.1)\n",
    "        else:\n",
    "            ax.set_ylim(bottom=0)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "    plot_path = os.path.join(output_dir, 'training_loss_breakdown.png')\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    logging.info(f\"Saved training loss breakdown plot to {plot_path}\")\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    if filename is None: return None\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\\.\\d+)', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def determine_sample_rate_and_length(audio_dir):\n",
    "    audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "    if not audio_files: raise FileNotFoundError(f\"No WAV files in {audio_dir}\")\n",
    "    info = sf.info(os.path.join(audio_dir, audio_files[0]))\n",
    "    sr = info.samplerate\n",
    "    seq_len = int(sr * HYPERPARAMETERS['audio_duration'])\n",
    "    logging.info(f\"Determined SR: {sr}Hz, Sequence Length: {seq_len}\")\n",
    "    return sr, seq_len\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    sr, seq_len = determine_sample_rate_and_length(AUDIO_DIRECTORY)\n",
    "    HYPERPARAMETERS.update({'sample_rate': sr, 'sequence_length': seq_len})\n",
    "    \n",
    "    csi_files = {extract_timestamp(f): f for f in os.listdir(CSI_DIRECTORY) if f.endswith('.csv') and extract_timestamp(f)}\n",
    "    audio_files = {extract_timestamp(f): f for f in os.listdir(AUDIO_DIRECTORY) if f.endswith('.wav') and extract_timestamp(f)}\n",
    "    \n",
    "    common_ts = sorted(set(csi_files.keys()) & set(audio_files.keys()))\n",
    "    logging.info(f\"Found {len(common_ts)} matching pairs\")\n",
    "    \n",
    "    data = [{'csi_path': os.path.join(CSI_DIRECTORY, csi_files[ts]), \n",
    "             'audio_path': os.path.join(AUDIO_DIRECTORY, audio_files[ts])} for ts in common_ts]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    train_df, test_df = train_test_split(df, test_size=1 - HYPERPARAMETERS['train_test_split'], \n",
    "                                         random_state=HYPERPARAMETERS['random_seed'], shuffle=True)\n",
    "    logging.info(f\"Data split: {len(train_df)} train, {len(test_df)} test\")\n",
    "    return train_df, test_df\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None and item[0] is not None]\n",
    "    if not batch: return None\n",
    "    try:\n",
    "        csi_data = torch.stack([item[0] for item in batch])\n",
    "        audio_data = torch.stack([item[1] for item in batch])\n",
    "        return csi_data, audio_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Collate error: {e}. Skipping batch.\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"=\" * 80 + \"\\nSTARTING OPTIMIZED CSI-AUDIO GAN TRAINING\\n\" + \"=\" * 80)\n",
    "        train_df, _ = load_and_prepare_data()\n",
    "        \n",
    "        logging.info(\"OPTIMIZED CONFIGURATION:\")\n",
    "        for key, value in HYPERPARAMETERS.items(): logging.info(f\"  {key}: {value}\")\n",
    "\n",
    "        train_dataset = OptimizedCSIAudioDataset(train_df, is_training=True)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=HYPERPARAMETERS['batch_size'], shuffle=True, \n",
    "                                      collate_fn=collate_fn, drop_last=True, num_workers=0, \n",
    "                                      pin_memory=True if HYPERPARAMETERS['device'].type == 'cuda' else False)\n",
    "\n",
    "        generator = OptimizedGenerator(HYPERPARAMETERS['latent_dim'], HYPERPARAMETERS['csi_channels']).to(HYPERPARAMETERS['device'])\n",
    "        discriminator = OptimizedDiscriminator(HYPERPARAMETERS['csi_channels']).to(HYPERPARAMETERS['device'])\n",
    "\n",
    "        logging.info(f\"Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "        logging.info(f\"Discriminator params: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "        logging.info(\"Starting optimized training...\")\n",
    "        \n",
    "        history = train_optimized_gan(generator, discriminator, train_dataloader, HYPERPARAMETERS)\n",
    "\n",
    "        torch.save(generator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \"generator.pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \"discriminator.pth\"))\n",
    "\n",
    "        history_df = pd.DataFrame(history)\n",
    "        history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "        history_df.to_csv(os.path.join(OUTPUT_DIRECTORY, 'optimized_training_history.csv'), index=False)\n",
    "        \n",
    "        if history['g_loss']:\n",
    "            plot_results(history, OUTPUT_DIRECTORY)\n",
    "\n",
    "        logging.info(\"=\" * 80 + \"\\nOPTIMIZED TRAINING COMPLETED!\\n\" + \"=\" * 80)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 16:41:46,092 - INFO - Using device: cuda\n",
      "2025-09-16 16:41:46,108 - INFO - ================================================================================\n",
      "2025-09-16 16:41:46,110 - INFO - STARTING ULTRA-STABLE CSI-TO-AUDIO GAN TRAINING\n",
      "2025-09-16 16:41:46,111 - INFO - ================================================================================\n",
      "2025-09-16 16:41:46,157 - INFO - Ultra-Stable Training Configuration:\n",
      "2025-09-16 16:41:46,159 - INFO -   csi_dir: data/processed_csi\n",
      "2025-09-16 16:41:46,160 - INFO -   audio_dir: data/audio\n",
      "2025-09-16 16:41:46,161 - INFO -   output_dir: output\n",
      "2025-09-16 16:41:46,163 - INFO -   chunk_size: 2048\n",
      "2025-09-16 16:41:46,163 - INFO -   sample_rate: None\n",
      "2025-09-16 16:41:46,165 - INFO -   audio_duration: 10.0\n",
      "2025-09-16 16:41:46,165 - INFO -   csi_channels: 8\n",
      "2025-09-16 16:41:46,166 - INFO -   batch_size: 8\n",
      "2025-09-16 16:41:46,167 - INFO -   num_workers: 0\n",
      "2025-09-16 16:41:46,168 - INFO -   latent_dim: 64\n",
      "2025-09-16 16:41:46,169 - INFO -   generator_dim: 256\n",
      "2025-09-16 16:41:46,170 - INFO -   discriminator_dim: 128\n",
      "2025-09-16 16:41:46,170 - INFO -   epochs: 150\n",
      "2025-09-16 16:41:46,171 - INFO -   lr_generator: 0.0001\n",
      "2025-09-16 16:41:46,172 - INFO -   lr_discriminator: 0.0002\n",
      "2025-09-16 16:41:46,173 - INFO -   beta1: 0.5\n",
      "2025-09-16 16:41:46,173 - INFO -   beta2: 0.999\n",
      "2025-09-16 16:41:46,174 - INFO -   lambda_reconstruction: 10.0\n",
      "2025-09-16 16:41:46,178 - INFO -   lambda_adversarial: 0.1\n",
      "2025-09-16 16:41:46,179 - INFO -   lambda_feature_matching: 1.0\n",
      "2025-09-16 16:41:46,180 - INFO -   lambda_stft: 2.0\n",
      "2025-09-16 16:41:46,180 - INFO -   lambda_gradient_penalty: 10.0\n",
      "2025-09-16 16:41:46,182 - INFO -   n_critic: 3\n",
      "2025-09-16 16:41:46,183 - INFO -   warmup_epochs: 5\n",
      "2025-09-16 16:41:46,183 - INFO -   reconstruction_epochs: 15\n",
      "2025-09-16 16:41:46,185 - INFO -   device: cuda\n",
      "2025-09-16 16:41:46,187 - INFO -   use_mixed_precision: False\n",
      "2025-09-16 16:41:46,189 - INFO -   gradient_clip: 0.5\n",
      "2025-09-16 16:41:46,190 - INFO -   random_seed: 42\n",
      "2025-09-16 16:41:46,191 - INFO -   patience: 20\n",
      "2025-09-16 16:41:46,192 - INFO -   min_improvement: 1e-05\n",
      "2025-09-16 16:41:46,193 - INFO -   save_frequency: 10\n",
      "2025-09-16 16:41:46,195 - INFO -   max_grad_norm: 1.0\n",
      "2025-09-16 16:41:46,196 - INFO -   loss_scale_factor: 0.0001\n",
      "2025-09-16 16:41:46,207 - INFO - Found 55 CSI files and 68 audio files\n",
      "2025-09-16 16:41:46,213 - INFO - Found 51 CSI-audio pairs\n",
      "2025-09-16 16:41:46,243 - INFO - Auto-detected sample rate: 16000Hz\n",
      "2025-09-16 16:41:46,674 - INFO - Validated 35 samples\n",
      "2025-09-16 16:41:46,676 - INFO - Dataset initialized: 35 samples (training=True)\n",
      "2025-09-16 16:41:46,813 - INFO - Validated 10 samples\n",
      "2025-09-16 16:41:46,814 - INFO - Dataset initialized: 10 samples (training=False)\n",
      "2025-09-16 16:41:46,815 - INFO - Created loaders: train=4, val=2 batches\n",
      "2025-09-16 16:41:46,815 - INFO - Generator input dim: 32832\n",
      "2025-09-16 16:41:47,335 - INFO - Starting ultra-stable training with consistent history tracking...\n",
      "2025-09-16 16:41:50,601 - INFO - Starting STABLE GAN training...\n",
      "2025-09-16 16:41:50,603 - INFO - Generator params: 8,715,749\n",
      "2025-09-16 16:41:50,604 - INFO - Discriminator params: 123,169\n",
      "Epoch 1/150: 100%|| 4/4 [00:19<00:00,  4.90s/it, Recon=2.4232, STFT=5.3733, Phase=Recon]\n",
      "2025-09-16 16:42:13,749 - INFO - Epoch 1 (Recon): Recon=2.830067, STFT=6.412121, Val=10.334228\n",
      "2025-09-16 16:42:14,059 - INFO - Saved best model at epoch 1\n",
      "Epoch 2/150: 100%|| 4/4 [00:09<00:00,  2.33s/it, Recon=2.6299, STFT=5.6286, Phase=Recon]\n",
      "2025-09-16 16:42:26,254 - INFO - Epoch 2 (Recon): Recon=3.429169, STFT=6.349665, Val=10.334090\n",
      "2025-09-16 16:42:26,591 - INFO - Saved best model at epoch 2\n",
      "Epoch 3/150: 100%|| 4/4 [00:08<00:00,  2.15s/it, Recon=2.7345, STFT=5.0092, Phase=Recon]\n",
      "2025-09-16 16:42:38,033 - INFO - Epoch 3 (Recon): Recon=3.053490, STFT=6.087922, Val=10.333957\n",
      "2025-09-16 16:42:38,340 - INFO - Saved best model at epoch 3\n",
      "Epoch 4/150: 100%|| 4/4 [00:08<00:00,  2.24s/it, Recon=2.9471, STFT=5.3664, Phase=Recon]\n",
      "2025-09-16 16:42:50,144 - INFO - Epoch 4 (Recon): Recon=3.192762, STFT=5.543501, Val=10.333827\n",
      "2025-09-16 16:42:50,463 - INFO - Saved best model at epoch 4\n",
      "Epoch 5/150: 100%|| 4/4 [00:08<00:00,  2.11s/it, Recon=3.9997, STFT=7.1843, Phase=Recon]\n",
      "2025-09-16 16:43:01,778 - INFO - Epoch 5 (Recon): Recon=3.067855, STFT=5.687390, Val=10.333696\n",
      "2025-09-16 16:43:02,093 - INFO - Saved best model at epoch 5\n",
      "Epoch 6/150: 100%|| 4/4 [00:08<00:00,  2.21s/it, Recon=3.0437, STFT=4.9077, Phase=Recon]\n",
      "2025-09-16 16:43:13,861 - INFO - Epoch 6 (Recon): Recon=2.961146, STFT=4.962023, Val=10.333548\n",
      "2025-09-16 16:43:14,174 - INFO - Saved best model at epoch 6\n",
      "Epoch 7/150: 100%|| 4/4 [00:08<00:00,  2.21s/it, Recon=2.4121, STFT=5.0582, Phase=Recon]\n",
      "2025-09-16 16:43:25,965 - INFO - Epoch 7 (Recon): Recon=3.010656, STFT=5.622391, Val=10.333441\n",
      "2025-09-16 16:43:26,274 - INFO - Saved best model at epoch 7\n",
      "Epoch 8/150: 100%|| 4/4 [00:09<00:00,  2.26s/it, Recon=3.9568, STFT=7.0018, Phase=Recon]\n",
      "2025-09-16 16:43:38,230 - INFO - Epoch 8 (Recon): Recon=3.360665, STFT=5.331481, Val=10.325851\n",
      "2025-09-16 16:43:38,527 - INFO - Saved best model at epoch 8\n",
      "Epoch 9/150: 100%|| 4/4 [00:08<00:00,  2.21s/it, Recon=3.2534, STFT=6.5044, Phase=Recon]\n",
      "2025-09-16 16:43:50,323 - INFO - Epoch 9 (Recon): Recon=2.984187, STFT=5.331417, Val=10.226806\n",
      "2025-09-16 16:43:50,620 - INFO - Saved best model at epoch 9\n",
      "Epoch 10/150: 100%|| 4/4 [00:09<00:00,  2.26s/it, Recon=2.4754, STFT=4.4552, Phase=Recon]\n",
      "2025-09-16 16:44:02,624 - INFO - Epoch 10 (Recon): Recon=2.943779, STFT=5.379206, Val=10.123113\n",
      "2025-09-16 16:44:03,208 - INFO - Saved best model at epoch 10\n",
      "Epoch 11/150: 100%|| 4/4 [00:09<00:00,  2.26s/it, Recon=2.9630, STFT=4.9095, Phase=Recon]\n",
      "2025-09-16 16:44:15,252 - INFO - Epoch 11 (Recon): Recon=3.053206, STFT=5.242392, Val=9.961208\n",
      "2025-09-16 16:44:15,568 - INFO - Saved best model at epoch 11\n",
      "Epoch 12/150: 100%|| 4/4 [00:08<00:00,  2.20s/it, Recon=2.4946, STFT=4.4023, Phase=Recon]\n",
      "2025-09-16 16:44:27,172 - INFO - Epoch 12 (Recon): Recon=2.897729, STFT=4.973159, Val=9.477857\n",
      "2025-09-16 16:44:27,474 - INFO - Saved best model at epoch 12\n",
      "Epoch 13/150: 100%|| 4/4 [00:08<00:00,  2.17s/it, Recon=2.8644, STFT=4.7397, Phase=Recon]\n",
      "2025-09-16 16:44:38,898 - INFO - Epoch 13 (Recon): Recon=2.729620, STFT=4.379133, Val=8.945453\n",
      "2025-09-16 16:44:39,207 - INFO - Saved best model at epoch 13\n",
      "Epoch 14/150: 100%|| 4/4 [00:08<00:00,  2.14s/it, Recon=3.5022, STFT=6.3709, Phase=Recon]\n",
      "2025-09-16 16:44:50,513 - INFO - Epoch 14 (Recon): Recon=2.952817, STFT=4.962364, Val=8.259003\n",
      "2025-09-16 16:44:50,831 - INFO - Saved best model at epoch 14\n",
      "Epoch 15/150: 100%|| 4/4 [00:08<00:00,  2.11s/it, Recon=2.9441, STFT=4.9162, Phase=Recon]\n",
      "2025-09-16 16:45:02,034 - INFO - Epoch 15 (Recon): Recon=2.788958, STFT=4.803893, Val=7.484501\n",
      "2025-09-16 16:45:02,359 - INFO - Saved best model at epoch 15\n",
      "Epoch 16/150: 100%|| 4/4 [00:09<00:00,  2.49s/it, D_loss=3.1984, G_loss=7.6267, G_recon=3.2013] \n",
      "2025-09-16 16:45:15,120 - INFO - Epoch 16: D_loss=1.607516, G_loss=7.610728, Val=5.456316\n",
      "2025-09-16 16:45:15,460 - INFO - Saved best model at epoch 16\n",
      "Epoch 17/150: 100%|| 4/4 [00:09<00:00,  2.49s/it, D_loss=4.6626, G_loss=7.2396, G_recon=3.0194]\n",
      "2025-09-16 16:45:28,332 - INFO - Epoch 17: D_loss=1.223040, G_loss=7.877956, Val=4.081864\n",
      "2025-09-16 16:45:28,677 - INFO - Saved best model at epoch 17\n",
      "Epoch 18/150: 100%|| 4/4 [00:09<00:00,  2.46s/it, D_loss=2.0629, G_loss=8.1770, G_recon=3.6376]\n",
      "2025-09-16 16:45:41,345 - INFO - Epoch 18: D_loss=1.620491, G_loss=7.777260, Val=3.251093\n",
      "2025-09-16 16:45:41,675 - INFO - Saved best model at epoch 18\n",
      "Epoch 19/150: 100%|| 4/4 [00:10<00:00,  2.56s/it, D_loss=0.9967, G_loss=6.6108, G_recon=2.2654] \n",
      "2025-09-16 16:45:54,907 - INFO - Epoch 19: D_loss=0.794022, G_loss=8.029945, Val=2.937091\n",
      "2025-09-16 16:45:55,252 - INFO - Saved best model at epoch 19\n",
      "Epoch 20/150: 100%|| 4/4 [00:10<00:00,  2.72s/it, D_loss=1.9004, G_loss=7.1575, G_recon=2.6075] \n",
      "2025-09-16 16:46:09,185 - INFO - Epoch 20: D_loss=1.057339, G_loss=7.875063, Val=2.859390\n",
      "2025-09-16 16:46:09,909 - INFO - Saved best model at epoch 20\n",
      "Epoch 21/150: 100%|| 4/4 [00:10<00:00,  2.73s/it, D_loss=7.1745, G_loss=9.8004, G_recon=3.2890]\n",
      "2025-09-16 16:46:23,929 - INFO - Epoch 21: D_loss=0.962735, G_loss=8.004961, Val=2.849743\n",
      "2025-09-16 16:46:24,279 - INFO - Saved best model at epoch 21\n",
      "Epoch 22/150: 100%|| 4/4 [00:11<00:00,  2.77s/it, D_loss=4.8822, G_loss=6.7315, G_recon=2.7850] \n",
      "2025-09-16 16:46:38,494 - INFO - Epoch 22: D_loss=1.000180, G_loss=7.942135, Val=2.824714\n",
      "2025-09-16 16:46:38,873 - INFO - Saved best model at epoch 22\n",
      "Epoch 23/150: 100%|| 4/4 [00:10<00:00,  2.73s/it, D_loss=3.7891, G_loss=11.1355, G_recon=4.0684]\n",
      "2025-09-16 16:46:52,883 - INFO - Epoch 23: D_loss=1.117899, G_loss=8.257361, Val=2.849294\n",
      "Epoch 24/150: 100%|| 4/4 [00:11<00:00,  2.81s/it, D_loss=1.7380, G_loss=7.7873, G_recon=3.4730] \n",
      "2025-09-16 16:47:07,186 - INFO - Epoch 24: D_loss=0.905817, G_loss=8.037872, Val=2.859802\n",
      "Epoch 25/150: 100%|| 4/4 [00:11<00:00,  2.78s/it, D_loss=0.6052, G_loss=9.8617, G_recon=3.4398]\n",
      "2025-09-16 16:47:21,481 - INFO - Epoch 25: D_loss=0.402922, G_loss=7.672346, Val=2.833840\n",
      "Epoch 26/150: 100%|| 4/4 [00:11<00:00,  2.83s/it, D_loss=0.8819, G_loss=7.5193, G_recon=3.1638] \n",
      "2025-09-16 16:47:35,877 - INFO - Epoch 26: D_loss=0.602402, G_loss=8.107178, Val=2.856874\n",
      "Epoch 27/150: 100%|| 4/4 [00:10<00:00,  2.70s/it, D_loss=4.4197, G_loss=6.9713, G_recon=2.8122]\n",
      "2025-09-16 16:47:49,795 - INFO - Epoch 27: D_loss=0.740177, G_loss=8.236161, Val=2.835377\n",
      "Epoch 28/150: 100%|| 4/4 [00:11<00:00,  2.77s/it, D_loss=2.0796, G_loss=9.6576, G_recon=3.3066]\n",
      "2025-09-16 16:48:04,003 - INFO - Epoch 28: D_loss=0.525501, G_loss=7.967096, Val=2.816130\n",
      "2025-09-16 16:48:04,561 - INFO - Saved best model at epoch 28\n",
      "Epoch 29/150: 100%|| 4/4 [00:11<00:00,  2.78s/it, D_loss=1.0766, G_loss=7.0839, G_recon=2.7619]\n",
      "2025-09-16 16:48:18,724 - INFO - Epoch 29: D_loss=0.680992, G_loss=7.675386, Val=2.824519\n",
      "Epoch 30/150: 100%|| 4/4 [00:10<00:00,  2.72s/it, D_loss=0.5141, G_loss=7.5896, G_recon=2.9624] \n",
      "2025-09-16 16:48:32,650 - INFO - Epoch 30: D_loss=0.270714, G_loss=8.085664, Val=2.849389\n",
      "Epoch 31/150: 100%|| 4/4 [00:10<00:00,  2.71s/it, D_loss=3.0704, G_loss=6.7966, G_recon=2.3171]\n",
      "2025-09-16 16:48:47,104 - INFO - Epoch 31: D_loss=0.670317, G_loss=7.816028, Val=2.823879\n",
      "Epoch 32/150: 100%|| 4/4 [00:10<00:00,  2.67s/it, D_loss=0.6296, G_loss=7.3610, G_recon=3.0756] \n",
      "2025-09-16 16:49:00,819 - INFO - Epoch 32: D_loss=0.582862, G_loss=8.010269, Val=2.809550\n",
      "2025-09-16 16:49:01,178 - INFO - Saved best model at epoch 32\n",
      "Epoch 33/150: 100%|| 4/4 [00:10<00:00,  2.69s/it, D_loss=0.4480, G_loss=11.6466, G_recon=4.8344]\n",
      "2025-09-16 16:49:14,893 - INFO - Epoch 33: D_loss=0.157886, G_loss=8.579623, Val=2.805773\n",
      "2025-09-16 16:49:15,256 - INFO - Saved best model at epoch 33\n",
      "Epoch 34/150: 100%|| 4/4 [00:10<00:00,  2.68s/it, D_loss=0.4762, G_loss=9.5550, G_recon=3.5312] \n",
      "2025-09-16 16:49:29,070 - INFO - Epoch 34: D_loss=0.172680, G_loss=7.734015, Val=2.807981\n",
      "Epoch 35/150: 100%|| 4/4 [00:10<00:00,  2.70s/it, D_loss=-0.0513, G_loss=7.3390, G_recon=3.1034]\n",
      "2025-09-16 16:49:42,877 - INFO - Epoch 35: D_loss=0.203933, G_loss=7.834251, Val=2.801750\n",
      "2025-09-16 16:49:43,238 - INFO - Saved best model at epoch 35\n",
      "Epoch 36/150: 100%|| 4/4 [00:10<00:00,  2.73s/it, D_loss=-0.1398, G_loss=6.9561, G_recon=2.5242]\n",
      "2025-09-16 16:49:57,179 - INFO - Epoch 36: D_loss=0.115766, G_loss=7.276017, Val=2.805987\n",
      "Epoch 37/150: 100%|| 4/4 [00:10<00:00,  2.67s/it, D_loss=-0.2958, G_loss=7.9775, G_recon=3.2215]\n",
      "2025-09-16 16:50:10,791 - INFO - Epoch 37: D_loss=0.048265, G_loss=7.953701, Val=2.787811\n",
      "2025-09-16 16:50:11,159 - INFO - Saved best model at epoch 37\n",
      "Epoch 38/150: 100%|| 4/4 [00:10<00:00,  2.75s/it, D_loss=-0.7466, G_loss=6.6348, G_recon=2.4272]\n",
      "2025-09-16 16:50:25,242 - INFO - Epoch 38: D_loss=-0.105854, G_loss=7.529637, Val=2.802827\n",
      "Epoch 39/150: 100%|| 4/4 [00:10<00:00,  2.71s/it, D_loss=1.5809, G_loss=6.6335, G_recon=2.3540] \n",
      "2025-09-16 16:50:39,128 - INFO - Epoch 39: D_loss=-0.042045, G_loss=8.161001, Val=2.793765\n",
      "Epoch 40/150: 100%|| 4/4 [00:10<00:00,  2.68s/it, D_loss=-0.3119, G_loss=6.3748, G_recon=2.2691]\n",
      "2025-09-16 16:50:52,957 - INFO - Epoch 40: D_loss=-0.078416, G_loss=7.640754, Val=2.793635\n",
      "Epoch 41/150: 100%|| 4/4 [00:11<00:00,  2.78s/it, D_loss=-0.2948, G_loss=9.5026, G_recon=3.2418]\n",
      "2025-09-16 16:51:07,623 - INFO - Epoch 41: D_loss=-0.252356, G_loss=7.988063, Val=2.819122\n",
      "Epoch 42/150: 100%|| 4/4 [00:11<00:00,  2.80s/it, D_loss=-0.8554, G_loss=6.7635, G_recon=2.7454] \n",
      "2025-09-16 16:51:22,112 - INFO - Epoch 42: D_loss=-0.292627, G_loss=7.841331, Val=2.816661\n",
      "Epoch 43/150: 100%|| 4/4 [00:11<00:00,  2.83s/it, D_loss=0.4130, G_loss=5.8343, G_recon=1.9888] \n",
      "2025-09-16 16:51:36,754 - INFO - Epoch 43: D_loss=-0.338114, G_loss=7.596163, Val=2.815650\n",
      "Epoch 44/150: 100%|| 4/4 [00:11<00:00,  2.94s/it, D_loss=-1.3483, G_loss=11.2065, G_recon=4.3453]\n",
      "2025-09-16 16:51:51,675 - INFO - Epoch 44: D_loss=-0.403780, G_loss=7.859253, Val=2.816716\n",
      "Epoch 45/150: 100%|| 4/4 [00:11<00:00,  2.86s/it, D_loss=-0.7006, G_loss=6.5188, G_recon=2.0498]\n",
      "2025-09-16 16:52:06,180 - INFO - Epoch 45: D_loss=-0.191812, G_loss=7.915193, Val=2.791889\n",
      "Epoch 46/150: 100%|| 4/4 [00:10<00:00,  2.73s/it, D_loss=-1.7951, G_loss=8.0473, G_recon=3.4504]\n",
      "2025-09-16 16:52:20,608 - INFO - Epoch 46: D_loss=-0.507392, G_loss=8.197471, Val=2.791071\n",
      "Epoch 47/150: 100%|| 4/4 [00:12<00:00,  3.03s/it, D_loss=1.4518, G_loss=6.9975, G_recon=2.6781]  \n",
      "2025-09-16 16:52:36,038 - INFO - Epoch 47: D_loss=-0.150069, G_loss=8.316571, Val=2.794038\n",
      "Epoch 48/150: 100%|| 4/4 [00:11<00:00,  2.79s/it, D_loss=0.0220, G_loss=6.7280, G_recon=2.7416]  \n",
      "2025-09-16 16:52:50,262 - INFO - Epoch 48: D_loss=-0.566859, G_loss=7.775704, Val=2.792582\n",
      "Epoch 49/150: 100%|| 4/4 [00:11<00:00,  2.79s/it, D_loss=-1.0583, G_loss=7.0931, G_recon=2.7253]\n",
      "2025-09-16 16:53:04,600 - INFO - Epoch 49: D_loss=-0.449835, G_loss=7.218579, Val=2.789240\n",
      "Epoch 50/150: 100%|| 4/4 [00:11<00:00,  2.92s/it, D_loss=-0.8875, G_loss=7.6658, G_recon=2.9319]\n",
      "2025-09-16 16:53:20,015 - INFO - Epoch 50: D_loss=-0.357593, G_loss=8.021016, Val=2.811991\n",
      "Epoch 51/150: 100%|| 4/4 [00:12<00:00,  3.13s/it, D_loss=-1.8555, G_loss=6.5464, G_recon=2.2098]\n",
      "2025-09-16 16:53:36,463 - INFO - Epoch 51: D_loss=-0.592253, G_loss=7.187914, Val=2.789118\n",
      "Epoch 52/150: 100%|| 4/4 [00:12<00:00,  3.05s/it, D_loss=-0.8591, G_loss=7.5042, G_recon=3.3133] \n",
      "2025-09-16 16:53:52,263 - INFO - Epoch 52: D_loss=-0.288343, G_loss=7.985250, Val=2.788129\n",
      "Epoch 53/150: 100%|| 4/4 [00:12<00:00,  3.14s/it, D_loss=-2.2174, G_loss=10.4012, G_recon=3.7814]\n",
      "2025-09-16 16:54:08,672 - INFO - Epoch 53: D_loss=-0.687185, G_loss=7.467621, Val=2.785394\n",
      "2025-09-16 16:54:09,086 - INFO - Saved best model at epoch 53\n",
      "Epoch 54/150: 100%|| 4/4 [00:10<00:00,  2.75s/it, D_loss=-2.8252, G_loss=10.2927, G_recon=3.9425]\n",
      "2025-09-16 16:54:23,036 - INFO - Epoch 54: D_loss=-0.665341, G_loss=7.726146, Val=2.768113\n",
      "2025-09-16 16:54:23,346 - INFO - Saved best model at epoch 54\n",
      "Epoch 55/150: 100%|| 4/4 [00:09<00:00,  2.48s/it, D_loss=-1.5453, G_loss=6.6175, G_recon=2.7245]\n",
      "2025-09-16 16:54:36,128 - INFO - Epoch 55: D_loss=-0.278273, G_loss=7.222403, Val=2.774483\n",
      "Epoch 56/150: 100%|| 4/4 [00:10<00:00,  2.54s/it, D_loss=-3.1063, G_loss=9.1498, G_recon=3.2486]\n",
      "2025-09-16 16:54:49,036 - INFO - Epoch 56: D_loss=-0.824461, G_loss=7.641801, Val=2.782848\n",
      "Epoch 57/150: 100%|| 4/4 [00:09<00:00,  2.37s/it, D_loss=-1.4378, G_loss=10.3027, G_recon=3.8419]\n",
      "2025-09-16 16:55:01,274 - INFO - Epoch 57: D_loss=-0.581445, G_loss=7.871678, Val=2.785458\n",
      "Epoch 58/150: 100%|| 4/4 [00:09<00:00,  2.44s/it, D_loss=-1.4017, G_loss=10.1115, G_recon=3.8243]\n",
      "2025-09-16 16:55:13,958 - INFO - Epoch 58: D_loss=-0.254339, G_loss=8.459820, Val=2.787504\n",
      "Epoch 59/150: 100%|| 4/4 [00:09<00:00,  2.38s/it, D_loss=-3.5771, G_loss=7.6162, G_recon=3.3434]\n",
      "2025-09-16 16:55:26,212 - INFO - Epoch 59: D_loss=-0.899607, G_loss=8.040534, Val=2.796660\n",
      "Epoch 60/150: 100%|| 4/4 [00:09<00:00,  2.36s/it, D_loss=-2.4890, G_loss=7.2427, G_recon=2.6985]\n",
      "2025-09-16 16:55:38,437 - INFO - Epoch 60: D_loss=-0.478479, G_loss=7.847566, Val=2.774897\n",
      "Epoch 61/150: 100%|| 4/4 [00:10<00:00,  2.64s/it, D_loss=-2.6176, G_loss=10.2096, G_recon=3.8591]\n",
      "2025-09-16 16:55:52,216 - INFO - Epoch 61: D_loss=-0.645624, G_loss=7.594165, Val=2.797898\n",
      "Epoch 62/150: 100%|| 4/4 [00:10<00:00,  2.51s/it, D_loss=-2.2981, G_loss=9.5681, G_recon=3.3835]\n",
      "2025-09-16 16:56:05,259 - INFO - Epoch 62: D_loss=-0.766958, G_loss=7.752064, Val=2.799797\n",
      "Epoch 63/150: 100%|| 4/4 [00:09<00:00,  2.50s/it, D_loss=-3.0869, G_loss=7.5918, G_recon=3.4489]\n",
      "2025-09-16 16:56:18,185 - INFO - Epoch 63: D_loss=-0.702795, G_loss=7.969484, Val=2.784431\n",
      "Epoch 64/150: 100%|| 4/4 [00:09<00:00,  2.38s/it, D_loss=-2.7803, G_loss=9.7976, G_recon=3.6283]\n",
      "2025-09-16 16:56:30,437 - INFO - Epoch 64: D_loss=-0.921882, G_loss=7.805130, Val=2.800533\n",
      "Epoch 65/150: 100%|| 4/4 [00:09<00:00,  2.39s/it, D_loss=-2.3647, G_loss=7.3963, G_recon=3.0961]\n",
      "2025-09-16 16:56:42,867 - INFO - Epoch 65: D_loss=0.180027, G_loss=8.175444, Val=2.792447\n",
      "Epoch 66/150: 100%|| 4/4 [00:09<00:00,  2.44s/it, D_loss=-3.8596, G_loss=7.3669, G_recon=3.1391]\n",
      "2025-09-16 16:56:55,384 - INFO - Epoch 66: D_loss=-0.834817, G_loss=8.182446, Val=2.774512\n",
      "Epoch 67/150: 100%|| 4/4 [00:09<00:00,  2.37s/it, D_loss=-2.3160, G_loss=7.3070, G_recon=2.8931]\n",
      "2025-09-16 16:57:07,642 - INFO - Epoch 67: D_loss=-0.651551, G_loss=7.589399, Val=2.779266\n",
      "Epoch 68/150: 100%|| 4/4 [00:10<00:00,  2.55s/it, D_loss=-3.4736, G_loss=6.9721, G_recon=3.0434]\n",
      "2025-09-16 16:57:20,583 - INFO - Epoch 68: D_loss=-0.761246, G_loss=6.990776, Val=2.782678\n",
      "Epoch 69/150: 100%|| 4/4 [00:09<00:00,  2.35s/it, D_loss=-1.3876, G_loss=10.2949, G_recon=3.9003]\n",
      "2025-09-16 16:57:32,702 - INFO - Epoch 69: D_loss=-0.606252, G_loss=7.665124, Val=2.778265\n",
      "Epoch 70/150: 100%|| 4/4 [00:09<00:00,  2.33s/it, D_loss=-2.6915, G_loss=6.4320, G_recon=2.6940] \n",
      "2025-09-16 16:57:44,749 - INFO - Epoch 70: D_loss=-0.841340, G_loss=7.982762, Val=2.781798\n",
      "Epoch 71/150: 100%|| 4/4 [00:10<00:00,  2.63s/it, D_loss=-0.6607, G_loss=6.4489, G_recon=2.2981]\n",
      "2025-09-16 16:57:58,448 - INFO - Epoch 71: D_loss=-0.769753, G_loss=6.581064, Val=2.803396\n",
      "Epoch 72/150: 100%|| 4/4 [00:09<00:00,  2.37s/it, D_loss=0.4045, G_loss=6.8930, G_recon=2.7725]  \n",
      "2025-09-16 16:58:10,717 - INFO - Epoch 72: D_loss=-0.674790, G_loss=7.795648, Val=2.770555\n",
      "Epoch 73/150: 100%|| 4/4 [00:10<00:00,  2.50s/it, D_loss=-1.7077, G_loss=6.9129, G_recon=2.5628]\n",
      "2025-09-16 16:58:23,569 - INFO - Epoch 73: D_loss=-0.543926, G_loss=7.738364, Val=2.773582\n",
      "Epoch 74/150: 100%|| 4/4 [00:09<00:00,  2.32s/it, D_loss=-2.7612, G_loss=6.8108, G_recon=2.6409]\n",
      "2025-09-16 16:58:35,700 - INFO - Epoch 74: D_loss=-0.909163, G_loss=8.278112, Val=2.781774\n",
      "2025-09-16 16:58:35,701 - INFO - Early stopping at epoch 74\n",
      "2025-09-16 16:58:38,716 - INFO - Saved consistent training plots to output/consistent_training_results.png\n",
      "2025-09-16 16:58:39,492 - INFO - Saved convergence plot to output/convergence_only.png\n",
      "2025-09-16 16:58:39,496 - INFO - Convergence improvement: 11.52%\n",
      "2025-09-16 16:58:39,497 - INFO -  MODEL SHOWS GOOD CONVERGENCE!\n",
      "2025-09-16 16:58:39,507 - INFO - Saved training summary to output/stable_training_summary.json\n",
      "2025-09-16 16:58:39,508 - INFO - Saved training history to output/stable_training_history.csv\n",
      "2025-09-16 16:58:39,624 - INFO - ================================================================================\n",
      "2025-09-16 16:58:39,625 - INFO - ULTRA-STABLE TRAINING COMPLETED SUCCESSFULLY!\n",
      "2025-09-16 16:58:39,625 - INFO - Results saved to: output\n",
      "2025-09-16 16:58:39,626 - INFO - Final convergence improvement: 11.52%\n",
      "2025-09-16 16:58:39,627 - INFO -  Your CSI-to-Audio GAN is ready with consistent tracking! \n",
      "2025-09-16 16:58:39,628 - INFO - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import logging\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - TUNED FOR STABILITY\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    \"csi_dir\": \"data/processed_csi\",\n",
    "    \"audio_dir\": \"data/audio\", \n",
    "    \"output_dir\": \"output\",\n",
    "    \n",
    "    # Data parameters\n",
    "    \"chunk_size\": 2048,\n",
    "    \"sample_rate\": None,  # Auto-detected\n",
    "    \"audio_duration\": 10.0,\n",
    "    \"csi_channels\": 8,\n",
    "    \"batch_size\": 8,  # Reduced for stability\n",
    "    \"num_workers\": 0,\n",
    "    \n",
    "    # Model parameters\n",
    "    \"latent_dim\": 64,\n",
    "    \"generator_dim\": 256,  # Reduced for stability\n",
    "    \"discriminator_dim\": 128,  # Reduced for stability\n",
    "    \n",
    "    # Training parameters - TUNED FOR STABILITY\n",
    "    \"epochs\": 150,\n",
    "    \"lr_generator\": 0.0001,      # Reduced LR\n",
    "    \"lr_discriminator\": 0.0002,   # Reduced LR\n",
    "    \"beta1\": 0.5,                # More stable than 0.0\n",
    "    \"beta2\": 0.999,\n",
    "    \n",
    "    # VERY CONSERVATIVE loss weights for stability\n",
    "    \"lambda_reconstruction\": 10.0,    # High reconstruction weight\n",
    "    \"lambda_adversarial\": 0.1,        # Very low adversarial weight\n",
    "    \"lambda_feature_matching\": 1.0,\n",
    "    \"lambda_stft\": 2.0,\n",
    "    \"lambda_gradient_penalty\": 10.0,\n",
    "    \n",
    "    # Training schedule - MORE CONSERVATIVE\n",
    "    \"n_critic\": 3,               # Fewer critic steps\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"reconstruction_epochs\": 15, # Longer pre-training\n",
    "    \n",
    "    # System\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"use_mixed_precision\": False,  # Disabled for stability\n",
    "    \"gradient_clip\": 0.5,         # Aggressive gradient clipping\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Convergence monitoring\n",
    "    \"patience\": 20,\n",
    "    \"min_improvement\": 1e-5,\n",
    "    \"save_frequency\": 10,\n",
    "    \n",
    "    # Stability checks\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"loss_scale_factor\": 1e-4,\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Setup device\n",
    "device = CONFIG[\"device\"]\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS FOR STABILITY\n",
    "# ============================================================================\n",
    "\n",
    "def check_for_nan_inf(tensor, name=\"tensor\"):\n",
    "    \"\"\"Check tensor for NaN or Inf values.\"\"\"\n",
    "    if torch.isnan(tensor).any():\n",
    "        logger.warning(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    if torch.isinf(tensor).any():\n",
    "        logger.warning(f\"Inf detected in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def safe_log(x, eps=1e-8):\n",
    "    \"\"\"Safe logarithm to prevent NaN.\"\"\"\n",
    "    return torch.log(torch.clamp(x, min=eps))\n",
    "\n",
    "def safe_norm(tensor, dim=None):\n",
    "    \"\"\"Safe norm computation.\"\"\"\n",
    "    return torch.norm(tensor + 1e-8, dim=dim)\n",
    "\n",
    "# ============================================================================\n",
    "# HISTORY MANAGEMENT FOR CONSISTENT PLOTTING\n",
    "# ============================================================================\n",
    "\n",
    "def initialize_history():\n",
    "    \"\"\"Initialize history with consistent structure.\"\"\"\n",
    "    return {\n",
    "        'g_loss': [],\n",
    "        'g_adv': [],\n",
    "        'g_recon': [],\n",
    "        'g_fm': [],\n",
    "        'g_stft': [],\n",
    "        'd_loss': [],\n",
    "        'd_real': [],\n",
    "        'd_fake': [],\n",
    "        'd_gp': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "def append_epoch_losses(history, epoch_losses, epoch, reconstruction_phase=False):\n",
    "    \"\"\"Append losses to history with consistent structure.\"\"\"\n",
    "    if reconstruction_phase:\n",
    "        # During reconstruction phase, set adversarial losses to 0\n",
    "        history['g_loss'].append(epoch_losses.get('g_loss', 0.0))\n",
    "        history['g_recon'].append(epoch_losses.get('g_recon', 0.0))\n",
    "        history['g_stft'].append(epoch_losses.get('g_stft', 0.0))\n",
    "        history['g_adv'].append(0.0)  # Always 0 during reconstruction\n",
    "        history['g_fm'].append(0.0)   # Always 0 during reconstruction\n",
    "        history['d_loss'].append(0.0) # Always 0 during reconstruction\n",
    "        history['d_real'].append(0.0) # Always 0 during reconstruction\n",
    "        history['d_fake'].append(0.0) # Always 0 during reconstruction\n",
    "        history['d_gp'].append(0.0)   # Always 0 during reconstruction\n",
    "    else:\n",
    "        # During adversarial phase, all losses are active\n",
    "        for key in ['g_loss', 'g_adv', 'g_recon', 'g_fm', 'g_stft', 'd_loss', 'd_real', 'd_fake', 'd_gp']:\n",
    "            history[key].append(epoch_losses.get(key, 0.0))\n",
    "    \n",
    "    # Val loss is always appended regardless of phase\n",
    "    history['val_loss'].append(epoch_losses.get('val_loss', 0.0))\n",
    "\n",
    "def pad_and_validate_history(history):\n",
    "    \"\"\"Ensure all history arrays have the same length and are valid.\"\"\"\n",
    "    # Find maximum length\n",
    "    max_len = max(len(values) for values in history.values() if isinstance(values, list))\n",
    "    \n",
    "    # Pad all arrays to max length\n",
    "    for key, values in history.items():\n",
    "        if isinstance(values, list):\n",
    "            if len(values) < max_len:\n",
    "                # Pad with last value or 0\n",
    "                pad_value = values[-1] if values else 0.0\n",
    "                history[key].extend([pad_value] * (max_len - len(values)))\n",
    "            \n",
    "            # Replace any NaN or inf values with 0\n",
    "            history[key] = [0.0 if (np.isnan(v) or np.isinf(v)) else v for v in values[:max_len]]\n",
    "    \n",
    "    return history, max_len\n",
    "\n",
    "# ============================================================================\n",
    "# COLLATE FUNCTION (Module Level for Multiprocessing)\n",
    "# ============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function that handles None batches.\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    try:\n",
    "        csi_data = torch.stack([item[0] for item in batch])\n",
    "        audio_data = torch.stack([item[1] for item in batch])\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if check_for_nan_inf(csi_data, \"CSI data\") or check_for_nan_inf(audio_data, \"Audio data\"):\n",
    "            return None\n",
    "        \n",
    "        return csi_data, audio_data\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Collate function failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# STABLE MODEL ARCHITECTURES\n",
    "# ============================================================================\n",
    "\n",
    "class StableResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(channels, eps=1e-5, momentum=0.1)\n",
    "        self.bn2 = nn.BatchNorm1d(channels, eps=1e-5, momentum=0.1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2, inplace=True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        \n",
    "        return F.leaky_relu(x + residual, 0.2, inplace=True)\n",
    "\n",
    "\n",
    "class StableGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, csi_channels, chunk_size, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.chunk_size = chunk_size\n",
    "        self.csi_input_dim = csi_channels * 2 * chunk_size\n",
    "        self.total_input_dim = latent_dim + self.csi_input_dim\n",
    "        \n",
    "        logger.info(f\"Generator input dim: {self.total_input_dim}\")\n",
    "        \n",
    "        # More conservative architecture\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.total_input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),  # More stable than BatchNorm\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, 64 * 32)  # 32 initial time steps\n",
    "        )\n",
    "        \n",
    "        # Simpler upsampling\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # 32 -> 64\n",
    "            nn.ConvTranspose1d(64, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(32, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 64 -> 128\n",
    "            nn.ConvTranspose1d(32, 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(16, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            StableResidualBlock(16),\n",
    "            \n",
    "            # 128 -> 256\n",
    "            nn.ConvTranspose1d(16, 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(8, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 256 -> 512\n",
    "            nn.ConvTranspose1d(8, 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(4, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 512 -> 1024\n",
    "            nn.ConvTranspose1d(4, 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(2, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 1024 -> 2048\n",
    "            nn.ConvTranspose1d(2, 1, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            nn.init.xavier_normal_(module.weight, gain=0.02)  # More stable initialization\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm)):\n",
    "            nn.init.normal_(module.weight, 1.0, 0.01)  # Smaller variance\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, noise, csi_data):\n",
    "        batch_size = noise.size(0)\n",
    "        \n",
    "        # Flatten and concatenate inputs\n",
    "        csi_flat = csi_data.view(batch_size, -1)\n",
    "        x = torch.cat([noise, csi_flat], dim=1)\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if check_for_nan_inf(x, \"Generator input\"):\n",
    "            return torch.zeros(batch_size, self.chunk_size, device=x.device)\n",
    "        \n",
    "        # Process through FC layers\n",
    "        x = self.fc_layers(x)\n",
    "        x = x.view(batch_size, 64, 32)\n",
    "        \n",
    "        # Check for NaN/Inf after FC\n",
    "        if check_for_nan_inf(x, \"Generator FC output\"):\n",
    "            return torch.zeros(batch_size, self.chunk_size, device=x.device)\n",
    "        \n",
    "        # Process through conv layers\n",
    "        x = self.conv_layers(x)\n",
    "        \n",
    "        # Ensure correct output size\n",
    "        if x.size(-1) != self.chunk_size:\n",
    "            x = F.interpolate(x, size=self.chunk_size, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Final check and clamp\n",
    "        output = x.squeeze(1)\n",
    "        \n",
    "        if check_for_nan_inf(output, \"Generator output\"):\n",
    "            return torch.zeros_like(output)\n",
    "        \n",
    "        # Clamp to prevent extreme values\n",
    "        output = torch.clamp(output, -0.95, 0.95)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class StableDiscriminator(nn.Module):\n",
    "    def __init__(self, csi_channels, chunk_size, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.csi_channels = csi_channels\n",
    "        \n",
    "        # Simpler audio processing\n",
    "        self.audio_conv = nn.Sequential(\n",
    "            # 2048 -> 1024\n",
    "            spectral_norm(nn.Conv1d(1, 16, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 1024 -> 512\n",
    "            spectral_norm(nn.Conv1d(16, 32, 4, 2, 1)),\n",
    "            nn.BatchNorm1d(32, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 512 -> 256\n",
    "            spectral_norm(nn.Conv1d(32, 64, 4, 2, 1)),\n",
    "            nn.BatchNorm1d(64, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        # Simpler CSI processing\n",
    "        csi_input_channels = csi_channels * 2\n",
    "        self.csi_conv = nn.Sequential(\n",
    "            # 2048 -> 1024\n",
    "            spectral_norm(nn.Conv1d(csi_input_channels, 16, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 1024 -> 512\n",
    "            spectral_norm(nn.Conv1d(16, 32, 4, 2, 1)),\n",
    "            nn.BatchNorm1d(32, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 512 -> 256\n",
    "            spectral_norm(nn.Conv1d(32, 64, 4, 2, 1)),\n",
    "            nn.BatchNorm1d(64, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        self.combined_conv = nn.Sequential(\n",
    "            spectral_norm(nn.Conv1d(128, 128, 4, 2, 1)),\n",
    "            nn.BatchNorm1d(128, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(128, 64, 4, 2, 1)),\n",
    "            nn.BatchNorm1d(64, eps=1e-5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Simpler classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(64, 32)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            spectral_norm(nn.Linear(32, 1))\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Conv1d, nn.Linear)):\n",
    "            nn.init.xavier_normal_(module.weight, gain=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            nn.init.normal_(module.weight, 1.0, 0.01)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, audio, csi, return_features=False):\n",
    "        batch_size = audio.size(0)\n",
    "        features = []\n",
    "        \n",
    "        # Ensure correct input shapes\n",
    "        audio_in = audio.unsqueeze(1) if audio.dim() == 2 else audio\n",
    "        \n",
    "        # Check inputs\n",
    "        if check_for_nan_inf(audio_in, \"Discriminator audio input\"):\n",
    "            return torch.zeros(batch_size, 1, device=audio.device)\n",
    "        if check_for_nan_inf(csi, \"Discriminator CSI input\"):\n",
    "            return torch.zeros(batch_size, 1, device=audio.device)\n",
    "        \n",
    "        # Process both modalities\n",
    "        audio_feat = self.audio_conv(audio_in)\n",
    "        csi_feat = self.csi_conv(csi)\n",
    "        \n",
    "        # Check for NaN after conv\n",
    "        if check_for_nan_inf(audio_feat, \"Audio features\") or check_for_nan_inf(csi_feat, \"CSI features\"):\n",
    "            return torch.zeros(batch_size, 1, device=audio.device)\n",
    "        \n",
    "        # Align feature map sizes\n",
    "        min_len = min(audio_feat.size(-1), csi_feat.size(-1))\n",
    "        audio_feat = audio_feat[..., :min_len]\n",
    "        csi_feat = csi_feat[..., :min_len]\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([audio_feat, csi_feat], dim=1)\n",
    "        if return_features:\n",
    "            features.append(combined.clone())\n",
    "        \n",
    "        # Final processing\n",
    "        x = self.combined_conv(combined)\n",
    "        if return_features:\n",
    "            features.append(x.clone())\n",
    "            \n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        output = self.classifier(x)\n",
    "        \n",
    "        # Check output and clamp\n",
    "        if check_for_nan_inf(output, \"Discriminator output\"):\n",
    "            output = torch.zeros_like(output)\n",
    "        \n",
    "        output = torch.clamp(output, -10, 10)  # Prevent extreme values\n",
    "        \n",
    "        return (output, features) if return_features else output\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STABLE LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class StableSTFTLoss(nn.Module):\n",
    "    def __init__(self, fft_size, hop_size, win_length):\n",
    "        super().__init__()\n",
    "        self.fft_size = min(fft_size, 512)  # Smaller FFT for stability\n",
    "        self.hop_size = min(hop_size, 128)\n",
    "        self.win_length = min(win_length, 256)\n",
    "        self.register_buffer(\"window\", torch.hann_window(self.win_length))\n",
    "        \n",
    "    def forward(self, generated, target):\n",
    "        try:\n",
    "            # Ensure inputs are not too large\n",
    "            generated = torch.clamp(generated, -1, 1)\n",
    "            target = torch.clamp(target, -1, 1)\n",
    "            \n",
    "            gen_stft = torch.stft(generated, self.fft_size, self.hop_size, self.win_length,\n",
    "                                 self.window, return_complex=True)\n",
    "            tar_stft = torch.stft(target, self.fft_size, self.hop_size, self.win_length,\n",
    "                                 self.window, return_complex=True)\n",
    "            \n",
    "            gen_mag = torch.abs(gen_stft) + 1e-7\n",
    "            tar_mag = torch.abs(tar_stft) + 1e-7\n",
    "            \n",
    "            # Spectral convergence loss\n",
    "            sc_loss = F.l1_loss(gen_mag, tar_mag)\n",
    "            \n",
    "            # Log-magnitude loss with safety\n",
    "            log_loss = F.l1_loss(safe_log(gen_mag), safe_log(tar_mag))\n",
    "            \n",
    "            total_loss = sc_loss + log_loss\n",
    "            \n",
    "            # Check for NaN\n",
    "            if torch.isnan(total_loss):\n",
    "                return torch.tensor(0.01, device=generated.device)\n",
    "            \n",
    "            return torch.clamp(total_loss, 0, 10)  # Clamp loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"STFT loss computation failed: {e}\")\n",
    "            return torch.tensor(0.01, device=generated.device)\n",
    "\n",
    "\n",
    "class StableMultiSTFTLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Smaller, safer STFT parameters\n",
    "        self.stft_losses = nn.ModuleList([\n",
    "            StableSTFTLoss(256, 64, 128),\n",
    "            StableSTFTLoss(512, 128, 256),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, generated, target):\n",
    "        total_loss = 0.0\n",
    "        valid_losses = 0\n",
    "        \n",
    "        for stft_loss in self.stft_losses:\n",
    "            loss = stft_loss(generated, target)\n",
    "            if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                total_loss += loss\n",
    "                valid_losses += 1\n",
    "        \n",
    "        if valid_losses == 0:\n",
    "            return torch.tensor(0.01, device=generated.device)\n",
    "        \n",
    "        return total_loss / valid_losses\n",
    "\n",
    "\n",
    "def stable_feature_matching_loss(real_features, fake_features):\n",
    "    \"\"\"Stable feature matching loss.\"\"\"\n",
    "    if not real_features or not fake_features:\n",
    "        return torch.tensor(0.0, device=real_features[0].device if real_features else fake_features[0].device)\n",
    "    \n",
    "    loss = 0.0\n",
    "    for real_feat, fake_feat in zip(real_features, fake_features):\n",
    "        if check_for_nan_inf(real_feat) or check_for_nan_inf(fake_feat):\n",
    "            continue\n",
    "            \n",
    "        real_mean = real_feat.mean([0, 2])\n",
    "        fake_mean = fake_feat.mean([0, 2])\n",
    "        \n",
    "        feat_loss = F.l1_loss(fake_mean, real_mean)\n",
    "        if not torch.isnan(feat_loss):\n",
    "            loss += feat_loss\n",
    "    \n",
    "    return torch.clamp(loss / max(len(real_features), 1), 0, 5)\n",
    "\n",
    "\n",
    "def stable_gradient_penalty(discriminator, real_data, fake_data, csi_data, device, lambda_gp=10.0):\n",
    "    \"\"\"Stable gradient penalty computation.\"\"\"\n",
    "    try:\n",
    "        batch_size = real_data.size(0)\n",
    "        \n",
    "        # Random interpolation\n",
    "        alpha = torch.rand(batch_size, 1, device=device)\n",
    "        alpha = alpha.expand_as(real_data)\n",
    "        \n",
    "        interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "        interpolated.requires_grad_(True)\n",
    "        \n",
    "        d_interpolated = discriminator(interpolated, csi_data)\n",
    "        \n",
    "        # Check for NaN\n",
    "        if check_for_nan_inf(d_interpolated, \"GP discriminator output\"):\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_interpolated, device=device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        # Check gradients\n",
    "        if check_for_nan_inf(gradients, \"GP gradients\"):\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Gradient penalty with numerical stability\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_norm = safe_norm(gradients, dim=1)\n",
    "        penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "        \n",
    "        # Clamp penalty\n",
    "        penalty = torch.clamp(penalty, 0, 100)\n",
    "        \n",
    "        if torch.isnan(penalty):\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        \n",
    "        return penalty\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Gradient penalty computation failed: {e}\")\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STABLE DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "class CSIPreprocessor:\n",
    "    def __init__(self, normalize=True):\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def extract_features(self, csi_data):\n",
    "        \"\"\"Extract stable features from CSI data.\"\"\"\n",
    "        try:\n",
    "            features_list = []\n",
    "            \n",
    "            for channel_idx in range(csi_data.shape[0]):\n",
    "                channel_data = csi_data[channel_idx]\n",
    "                \n",
    "                # Robust normalization\n",
    "                if self.normalize and np.std(channel_data) > 1e-6:\n",
    "                    mean_val = np.mean(channel_data)\n",
    "                    std_val = np.std(channel_data) + 1e-8\n",
    "                    channel_data = (channel_data - mean_val) / std_val\n",
    "                    channel_data = np.clip(channel_data, -5, 5)  # Aggressive clipping\n",
    "                \n",
    "                # Extract stable features\n",
    "                energy_variations = np.abs(channel_data) ** 2\n",
    "                differences = np.diff(channel_data, prepend=channel_data[0])\n",
    "                \n",
    "                features_list.append(np.stack([energy_variations, differences], axis=0))\n",
    "            \n",
    "            # Combine and normalize\n",
    "            features = np.concatenate(features_list, axis=0).astype(np.float32)\n",
    "            \n",
    "            if self.normalize and np.std(features) > 1e-6:\n",
    "                features = (features - np.mean(features)) / (np.std(features) + 1e-8)\n",
    "                features = np.clip(features, -3, 3)\n",
    "            \n",
    "            # Check for NaN/Inf\n",
    "            features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "                \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"CSI feature extraction failed: {e}\")\n",
    "            # Return safe fallback\n",
    "            fallback = np.random.randn(csi_data.shape[0] * 2, csi_data.shape[1]) * 0.01\n",
    "            return fallback.astype(np.float32)\n",
    "\n",
    "\n",
    "class StableCSIAudioDataset(Dataset):\n",
    "    def __init__(self, data_df, config, is_training=True):\n",
    "        self.data_df = data_df.reset_index(drop=True)\n",
    "        self.config = config\n",
    "        self.is_training = is_training\n",
    "        self.chunk_size = config[\"chunk_size\"]\n",
    "        self.csi_channels = config[\"csi_channels\"]\n",
    "        self.sample_rate = config[\"sample_rate\"]\n",
    "        \n",
    "        self.preprocessor = CSIPreprocessor()\n",
    "        self._validate_files()\n",
    "        \n",
    "        logger.info(f\"Dataset initialized: {len(self.data_df)} samples (training={is_training})\")\n",
    "    \n",
    "    def _validate_files(self):\n",
    "        \"\"\"Validate files exist and are not empty.\"\"\"\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.data_df)):\n",
    "            try:\n",
    "                csi_path = self.data_df.iloc[idx]['csi_path']\n",
    "                audio_path = self.data_df.iloc[idx]['audio_path']\n",
    "                \n",
    "                if (os.path.exists(csi_path) and os.path.exists(audio_path) and\n",
    "                    os.path.getsize(csi_path) > 1000 and os.path.getsize(audio_path) > 1000):\n",
    "                    try:\n",
    "                        info = sf.info(audio_path)\n",
    "                        if info.duration >= 2.0:\n",
    "                            valid_indices.append(idx)\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        self.data_df = self.data_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        logger.info(f\"Validated {len(self.data_df)} samples\")\n",
    "    \n",
    "    def load_data(self, csi_path, audio_path):\n",
    "        \"\"\"Load and safely process CSI-audio pair.\"\"\"\n",
    "        try:\n",
    "            # Load audio with validation\n",
    "            audio_data, file_sr = sf.read(audio_path, dtype='float32')\n",
    "            if len(audio_data.shape) > 1:\n",
    "                audio_data = audio_data.mean(axis=1)\n",
    "            \n",
    "            # Validate audio\n",
    "            if len(audio_data) == 0 or np.all(audio_data == 0):\n",
    "                return None, None\n",
    "            \n",
    "            # Resample if needed\n",
    "            if abs(file_sr - self.sample_rate) > 100:\n",
    "                audio_data = librosa.resample(audio_data, orig_sr=file_sr, target_sr=self.sample_rate, res_type='kaiser_fast')\n",
    "            \n",
    "            # Ensure minimum length\n",
    "            if len(audio_data) < self.chunk_size:\n",
    "                repeat_factor = (self.chunk_size // len(audio_data)) + 1\n",
    "                audio_data = np.tile(audio_data, repeat_factor)[:self.chunk_size]\n",
    "            \n",
    "            # Load CSI with robust error handling\n",
    "            try:\n",
    "                csi_data = pd.read_csv(csi_path)\n",
    "                subcarrier_cols = [col for col in csi_data.columns if col.startswith('subcarrier_')]\n",
    "                if len(subcarrier_cols) < self.csi_channels:\n",
    "                    raise ValueError(\"Insufficient CSI channels\")\n",
    "                csi_values = csi_data[subcarrier_cols[:self.csi_channels]].values.T\n",
    "                \n",
    "                # Validate CSI\n",
    "                if csi_values.shape[0] == 0 or np.all(csi_values == 0):\n",
    "                    raise ValueError(\"Empty CSI data\")\n",
    "                \n",
    "            except Exception:\n",
    "                # Fallback to synthetic data\n",
    "                csi_values = np.random.randn(self.csi_channels, len(audio_data)) * 0.1\n",
    "            \n",
    "            # Align lengths with robust interpolation\n",
    "            if csi_values.shape[1] != len(audio_data):\n",
    "                target_len = len(audio_data)\n",
    "                \n",
    "                if csi_values.shape[1] > target_len:\n",
    "                    # Downsample\n",
    "                    indices = np.linspace(0, csi_values.shape[1]-1, target_len).astype(int)\n",
    "                    csi_values = csi_values[:, indices]\n",
    "                else:\n",
    "                    # Upsample with fallback\n",
    "                    try:\n",
    "                        aligned_csi = []\n",
    "                        old_x = np.linspace(0, 1, csi_values.shape[1])\n",
    "                        new_x = np.linspace(0, 1, target_len)\n",
    "                        \n",
    "                        for i in range(csi_values.shape[0]):\n",
    "                            f = interpolate.interp1d(old_x, csi_values[i], kind='linear', \n",
    "                                                   bounds_error=False, fill_value='extrapolate')\n",
    "                            aligned_csi.append(f(new_x))\n",
    "                        csi_values = np.stack(aligned_csi)\n",
    "                    except:\n",
    "                        # Simple repetition fallback\n",
    "                        repeat_factor = target_len // csi_values.shape[1] + 1\n",
    "                        csi_values = np.tile(csi_values, (1, repeat_factor))[:, :target_len]\n",
    "            \n",
    "            # Final validation and cleaning\n",
    "            audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.5, neginf=-0.5)\n",
    "            csi_values = np.nan_to_num(csi_values, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            \n",
    "            return audio_data.astype(np.float32), csi_values.astype(np.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data loading completely failed: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            row = self.data_df.iloc[idx]\n",
    "            audio_data, csi_data = self.load_data(row['csi_path'], row['audio_path'])\n",
    "            \n",
    "            if audio_data is None:\n",
    "                return self._get_fallback()\n",
    "            \n",
    "            # Extract chunk\n",
    "            if len(audio_data) >= self.chunk_size:\n",
    "                if self.is_training:\n",
    "                    max_start = len(audio_data) - self.chunk_size\n",
    "                    start_idx = random.randint(0, max_start) if max_start > 0 else 0\n",
    "                else:\n",
    "                    start_idx = 0\n",
    "                \n",
    "                end_idx = start_idx + self.chunk_size\n",
    "                audio_chunk = audio_data[start_idx:end_idx]\n",
    "                csi_chunk = csi_data[:, start_idx:end_idx]\n",
    "            else:\n",
    "                return self._get_fallback()\n",
    "            \n",
    "            # Preprocess with safety checks\n",
    "            csi_features = self.preprocessor.extract_features(csi_chunk)\n",
    "            \n",
    "            # Normalize audio safely\n",
    "            if np.max(np.abs(audio_chunk)) > 1e-6:\n",
    "                audio_chunk = audio_chunk / (np.max(np.abs(audio_chunk)) + 1e-8) * 0.95\n",
    "            \n",
    "            # Convert to tensors\n",
    "            csi_tensor = torch.from_numpy(csi_features).float()\n",
    "            audio_tensor = torch.from_numpy(audio_chunk).float()\n",
    "            \n",
    "            # Final safety checks\n",
    "            csi_tensor = torch.nan_to_num(csi_tensor, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            audio_tensor = torch.nan_to_num(audio_tensor, nan=0.0, posinf=0.5, neginf=-0.5)\n",
    "            \n",
    "            csi_tensor = torch.clamp(csi_tensor, -5, 5)\n",
    "            audio_tensor = torch.clamp(audio_tensor, -1, 1)\n",
    "            \n",
    "            return csi_tensor, audio_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Sample {idx} failed: {e}\")\n",
    "            return self._get_fallback()\n",
    "    \n",
    "    def _get_fallback(self):\n",
    "        \"\"\"Generate safe fallback sample.\"\"\"\n",
    "        csi_shape = (self.csi_channels * 2, self.chunk_size)\n",
    "        audio_shape = (self.chunk_size,)\n",
    "        return torch.randn(csi_shape) * 0.001, torch.randn(audio_shape) * 0.001\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    \"\"\"Extract timestamp from filename.\"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\\.\\d+)', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def discover_data_pairs(csi_dir, audio_dir):\n",
    "    \"\"\"Discover and pair CSI-audio files.\"\"\"\n",
    "    csi_dir, audio_dir = Path(csi_dir), Path(audio_dir)\n",
    "    \n",
    "    if not csi_dir.exists():\n",
    "        raise FileNotFoundError(f\"CSI directory not found: {csi_dir}\")\n",
    "    if not audio_dir.exists():\n",
    "        raise FileNotFoundError(f\"Audio directory not found: {audio_dir}\")\n",
    "    \n",
    "    # Find files\n",
    "    csi_files = {extract_timestamp(f.name): str(f) for f in csi_dir.glob(\"*.csv\") if extract_timestamp(f.name)}\n",
    "    audio_files = {extract_timestamp(f.name): str(f) for f in audio_dir.glob(\"*.wav\") if extract_timestamp(f.name)}\n",
    "    \n",
    "    logger.info(f\"Found {len(csi_files)} CSI files and {len(audio_files)} audio files\")\n",
    "    \n",
    "    # Match by timestamp\n",
    "    common_timestamps = sorted(set(csi_files.keys()) & set(audio_files.keys()))\n",
    "    \n",
    "    if not common_timestamps:\n",
    "        raise ValueError(\"No matching CSI-audio pairs found\")\n",
    "    \n",
    "    data_pairs = [{\"timestamp\": ts, \"csi_path\": csi_files[ts], \"audio_path\": audio_files[ts]} \n",
    "                  for ts in common_timestamps]\n",
    "    \n",
    "    return pd.DataFrame(data_pairs)\n",
    "\n",
    "\n",
    "def get_data_loaders(config):\n",
    "    \"\"\"Create stable data loaders.\"\"\"\n",
    "    # Discover pairs\n",
    "    data_df = discover_data_pairs(config[\"csi_dir\"], config[\"audio_dir\"])\n",
    "    logger.info(f\"Found {len(data_df)} CSI-audio pairs\")\n",
    "    \n",
    "    if len(data_df) < 5:\n",
    "        raise ValueError(f\"Insufficient data: {len(data_df)} pairs found\")\n",
    "    \n",
    "    # Determine sample rate\n",
    "    if config[\"sample_rate\"] is None:\n",
    "        sample_audio = data_df.iloc[0][\"audio_path\"]\n",
    "        info = sf.info(sample_audio)\n",
    "        config[\"sample_rate\"] = info.samplerate\n",
    "        logger.info(f\"Auto-detected sample rate: {config['sample_rate']}Hz\")\n",
    "    \n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=config[\"random_seed\"])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = StableCSIAudioDataset(train_df, config, is_training=True)\n",
    "    val_dataset = StableCSIAudioDataset(val_df, config, is_training=False)\n",
    "    \n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=True,\n",
    "        num_workers=config[\"num_workers\"], \n",
    "        collate_fn=collate_fn, \n",
    "        drop_last=True,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=False,\n",
    "        num_workers=config[\"num_workers\"], \n",
    "        collate_fn=collate_fn, \n",
    "        drop_last=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Created loaders: train={len(train_loader)}, val={len(val_loader)} batches\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ULTRA-STABLE TRAINING LOOP WITH FIXED HISTORY MANAGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def train_stable_gan(generator, discriminator, train_loader, val_loader, config):\n",
    "    \"\"\"Ultra-stable GAN training with consistent history tracking.\"\"\"\n",
    "    \n",
    "    # Conservative optimizers\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=config[\"lr_generator\"],\n",
    "                            betas=(config[\"beta1\"], config[\"beta2\"]), weight_decay=1e-5)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=config[\"lr_discriminator\"],\n",
    "                            betas=(config[\"beta1\"], config[\"beta2\"]), weight_decay=1e-5)\n",
    "    \n",
    "    # Conservative schedulers\n",
    "    scheduler_g = optim.lr_scheduler.StepLR(optimizer_g, step_size=20, gamma=0.8)\n",
    "    scheduler_d = optim.lr_scheduler.StepLR(optimizer_d, step_size=20, gamma=0.8)\n",
    "    \n",
    "    # Stable loss functions\n",
    "    stft_loss = StableMultiSTFTLoss().to(device)\n",
    "    \n",
    "    # Initialize consistent history structure\n",
    "    history = initialize_history()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    logger.info(\"Starting STABLE GAN training...\")\n",
    "    logger.info(f\"Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "    logger.info(f\"Discriminator params: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        # Initialize epoch losses\n",
    "        epoch_losses = {}\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        batch_losses = {key: [] for key in ['g_loss', 'g_adv', 'g_recon', 'g_fm', 'g_stft', 'd_loss', 'd_real', 'd_fake', 'd_gp']}\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(progress_bar):\n",
    "            if batch_data is None:\n",
    "                continue\n",
    "                \n",
    "            csi_data, audio_data = batch_data\n",
    "            csi_data = csi_data.to(device)\n",
    "            audio_data = audio_data.to(device)\n",
    "            batch_size = csi_data.size(0)\n",
    "            \n",
    "            if batch_size == 0:\n",
    "                continue\n",
    "            \n",
    "            # ============= RECONSTRUCTION PHASE =============\n",
    "            if epoch < config[\"reconstruction_epochs\"]:\n",
    "                # Only generator training\n",
    "                optimizer_g.zero_grad()\n",
    "                \n",
    "                noise = torch.randn(batch_size, config[\"latent_dim\"], device=device) * 0.1\n",
    "                fake_audio = generator(noise, csi_data)\n",
    "                \n",
    "                # Safe losses\n",
    "                g_loss_recon = F.l1_loss(fake_audio, audio_data) * config[\"lambda_reconstruction\"]\n",
    "                g_loss_stft = stft_loss(fake_audio, audio_data) * config[\"lambda_stft\"]\n",
    "                g_loss = g_loss_recon + g_loss_stft\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(g_loss) or torch.isinf(g_loss):\n",
    "                    logger.warning(\"NaN detected in generator loss, skipping batch\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                g_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), config[\"gradient_clip\"])\n",
    "                optimizer_g.step()\n",
    "                \n",
    "                # Store batch losses\n",
    "                batch_losses['g_loss'].append(g_loss.item())\n",
    "                batch_losses['g_recon'].append(g_loss_recon.item())\n",
    "                batch_losses['g_stft'].append(g_loss_stft.item())\n",
    "                # Set adversarial losses to 0 during reconstruction\n",
    "                batch_losses['g_adv'].append(0.0)\n",
    "                batch_losses['g_fm'].append(0.0)\n",
    "                batch_losses['d_loss'].append(0.0)\n",
    "                batch_losses['d_real'].append(0.0)\n",
    "                batch_losses['d_fake'].append(0.0)\n",
    "                batch_losses['d_gp'].append(0.0)\n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    'Recon': f'{g_loss_recon.item():.4f}',\n",
    "                    'STFT': f'{g_loss_stft.item():.4f}',\n",
    "                    'Phase': 'Recon'\n",
    "                })\n",
    "            \n",
    "            # ============= ADVERSARIAL PHASE =============\n",
    "            else:\n",
    "                # Train Discriminator\n",
    "                d_loss_total = 0.0\n",
    "                d_real_total = 0.0\n",
    "                d_fake_total = 0.0\n",
    "                d_gp_total = 0.0\n",
    "                \n",
    "                for _ in range(min(config[\"n_critic\"], 2)):\n",
    "                    optimizer_d.zero_grad()\n",
    "                    \n",
    "                    noise = torch.randn(batch_size, config[\"latent_dim\"], device=device) * 0.1\n",
    "                    fake_audio = generator(noise, csi_data).detach()\n",
    "                    \n",
    "                    real_output = discriminator(audio_data, csi_data)\n",
    "                    fake_output = discriminator(fake_audio, csi_data)\n",
    "                    \n",
    "                    # Check outputs\n",
    "                    if check_for_nan_inf(real_output) or check_for_nan_inf(fake_output):\n",
    "                        continue\n",
    "                    \n",
    "                    # WGAN losses with safety\n",
    "                    d_loss_real = -torch.mean(torch.clamp(real_output, -10, 10))\n",
    "                    d_loss_fake = torch.mean(torch.clamp(fake_output, -10, 10))\n",
    "                    d_loss_gp = stable_gradient_penalty(discriminator, audio_data, fake_audio, csi_data, device)\n",
    "                    \n",
    "                    d_loss = d_loss_real + d_loss_fake + d_loss_gp\n",
    "                    \n",
    "                    if torch.isnan(d_loss) or torch.isinf(d_loss):\n",
    "                        continue\n",
    "                    \n",
    "                    d_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(discriminator.parameters(), config[\"gradient_clip\"])\n",
    "                    optimizer_d.step()\n",
    "                    \n",
    "                    d_loss_total += d_loss.item()\n",
    "                    d_real_total += d_loss_real.item()\n",
    "                    d_fake_total += d_loss_fake.item()\n",
    "                    d_gp_total += d_loss_gp.item()\n",
    "                \n",
    "                # Train Generator\n",
    "                optimizer_g.zero_grad()\n",
    "                \n",
    "                noise = torch.randn(batch_size, config[\"latent_dim\"], device=device) * 0.1\n",
    "                fake_audio = generator(noise, csi_data)\n",
    "                \n",
    "                fake_output, fake_features = discriminator(fake_audio, csi_data, return_features=True)\n",
    "                with torch.no_grad():\n",
    "                    _, real_features = discriminator(audio_data, csi_data, return_features=True)\n",
    "                \n",
    "                if check_for_nan_inf(fake_output):\n",
    "                    fake_output = torch.zeros_like(fake_output)\n",
    "                \n",
    "                # All generator losses\n",
    "                g_loss_adv = -torch.mean(torch.clamp(fake_output, -10, 10)) * config[\"lambda_adversarial\"]\n",
    "                g_loss_recon = F.l1_loss(fake_audio, audio_data) * config[\"lambda_reconstruction\"]\n",
    "                g_loss_fm = stable_feature_matching_loss(real_features, fake_features) * config[\"lambda_feature_matching\"]\n",
    "                g_loss_stft = stft_loss(fake_audio, audio_data) * config[\"lambda_stft\"]\n",
    "                \n",
    "                g_loss = g_loss_adv + g_loss_recon + g_loss_fm + g_loss_stft\n",
    "                \n",
    "                if torch.isnan(g_loss) or torch.isinf(g_loss):\n",
    "                    logger.warning(\"NaN in generator loss, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                g_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), config[\"gradient_clip\"])\n",
    "                optimizer_g.step()\n",
    "                \n",
    "                # Store batch losses\n",
    "                batch_losses['g_loss'].append(g_loss.item())\n",
    "                batch_losses['g_adv'].append(g_loss_adv.item())\n",
    "                batch_losses['g_recon'].append(g_loss_recon.item())\n",
    "                batch_losses['g_fm'].append(g_loss_fm.item())\n",
    "                batch_losses['g_stft'].append(g_loss_stft.item())\n",
    "                batch_losses['d_loss'].append(d_loss_total / max(config[\"n_critic\"], 1))\n",
    "                batch_losses['d_real'].append(d_real_total / max(config[\"n_critic\"], 1))\n",
    "                batch_losses['d_fake'].append(d_fake_total / max(config[\"n_critic\"], 1))\n",
    "                batch_losses['d_gp'].append(d_gp_total / max(config[\"n_critic\"], 1))\n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    'D_loss': f'{d_loss_total:.4f}',\n",
    "                    'G_loss': f'{g_loss.item():.4f}',\n",
    "                    'G_recon': f'{g_loss_recon.item():.4f}'\n",
    "                })\n",
    "        \n",
    "        # Compute epoch averages (ensure no NaN/Inf)\n",
    "        for key in batch_losses:\n",
    "            if batch_losses[key]:\n",
    "                avg_val = np.mean(batch_losses[key])\n",
    "                epoch_losses[key] = avg_val if not (np.isnan(avg_val) or np.isinf(avg_val)) else 0.0\n",
    "            else:\n",
    "                epoch_losses[key] = 0.0\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = stable_validate(generator, val_loader, stft_loss, device)\n",
    "        epoch_losses['val_loss'] = val_loss if not (np.isnan(val_loss) or np.isinf(val_loss)) else float('inf')\n",
    "        \n",
    "        # Append to history with consistent structure\n",
    "        is_reconstruction = epoch < config[\"reconstruction_epochs\"]\n",
    "        append_epoch_losses(history, epoch_losses, epoch, reconstruction_phase=is_reconstruction)\n",
    "        \n",
    "        # Update schedulers\n",
    "        if epoch >= config[\"warmup_epochs\"]:\n",
    "            scheduler_g.step()\n",
    "            scheduler_d.step()\n",
    "        \n",
    "        # Logging\n",
    "        if is_reconstruction:\n",
    "            logger.info(f\"Epoch {epoch+1} (Recon): Recon={epoch_losses['g_recon']:.6f}, \"\n",
    "                       f\"STFT={epoch_losses['g_stft']:.6f}, Val={val_loss:.6f}\")\n",
    "        else:\n",
    "            logger.info(f\"Epoch {epoch+1}: D_loss={epoch_losses['d_loss']:.6f}, \"\n",
    "                       f\"G_loss={epoch_losses['g_loss']:.6f}, Val={val_loss:.6f}\")\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if (epoch + 1) % config[\"save_frequency\"] == 0:\n",
    "            save_stable_checkpoint(generator, discriminator, optimizer_g, optimizer_d, \n",
    "                                 epoch, history, config[\"output_dir\"])\n",
    "        \n",
    "        # Early stopping\n",
    "        current_loss = val_loss if not np.isnan(val_loss) else float('inf')\n",
    "        if current_loss < best_loss - config[\"min_improvement\"]:\n",
    "            best_loss = current_loss\n",
    "            patience_counter = 0\n",
    "            save_stable_checkpoint(generator, discriminator, optimizer_g, optimizer_d, \n",
    "                                 epoch, history, config[\"output_dir\"], is_best=True)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= config[\"patience\"]:\n",
    "            logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def stable_validate(generator, val_loader, stft_loss, device):\n",
    "    \"\"\"Stable validation loop.\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "            if batch_data is None:\n",
    "                continue\n",
    "                \n",
    "            csi_data, audio_data = batch_data\n",
    "            csi_data = csi_data.to(device)\n",
    "            audio_data = audio_data.to(device)\n",
    "            batch_size = csi_data.size(0)\n",
    "            \n",
    "            if batch_size == 0:\n",
    "                continue\n",
    "            \n",
    "            # Generate fake audio\n",
    "            noise = torch.randn(batch_size, CONFIG[\"latent_dim\"], device=device) * 0.1\n",
    "            fake_audio = generator(noise, csi_data)\n",
    "            \n",
    "            # Compute validation loss\n",
    "            val_recon = F.l1_loss(fake_audio, audio_data)\n",
    "            val_stft = stft_loss(fake_audio, audio_data)\n",
    "            val_loss = val_recon + val_stft\n",
    "            \n",
    "            if not torch.isnan(val_loss) and not torch.isinf(val_loss):\n",
    "                total_loss += val_loss.item()\n",
    "                num_batches += 1\n",
    "    \n",
    "    return total_loss / max(num_batches, 1) if num_batches > 0 else float('inf')\n",
    "\n",
    "\n",
    "def save_stable_checkpoint(generator, discriminator, optimizer_g, optimizer_d, epoch, history, output_dir, is_best=False):\n",
    "    \"\"\"Save checkpoint with validation.\"\"\"\n",
    "    try:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "            'history': history,\n",
    "            'config': CONFIG\n",
    "        }\n",
    "        \n",
    "        filename = \"best_model.pt\" if is_best else f\"checkpoint_epoch_{epoch+1:04d}.pt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        torch.save(checkpoint, filepath)\n",
    "        \n",
    "        if is_best:\n",
    "            logger.info(f\"Saved best model at epoch {epoch+1}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED STABLE VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_consistent_results(history, output_dir):\n",
    "    \"\"\"Plot training results with consistent history lengths and no artifacts.\"\"\"\n",
    "    if not any(history.values()):\n",
    "        logger.warning(\"No history to plot\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Validate and pad history to ensure consistency\n",
    "        history, max_length = pad_and_validate_history(history)\n",
    "        epochs = np.arange(1, max_length + 1)\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Ultra-Stable CSI-Audio GAN Training Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Define colors for consistency\n",
    "        colors = {'g_loss': 'blue', 'g_recon': 'orange', 'g_stft': 'red', 'g_adv': 'green', \n",
    "                 'g_fm': 'purple', 'd_loss': 'brown', 'd_real': 'pink', 'd_fake': 'gray', \n",
    "                 'd_gp': 'olive', 'val_loss': 'cyan'}\n",
    "        \n",
    "        # 1. Total Losses\n",
    "        axes[0, 0].plot(epochs, history['g_loss'], label='Generator', color=colors['g_loss'], linewidth=2)\n",
    "        axes[0, 0].plot(epochs, history['d_loss'], label='Discriminator', color=colors['d_loss'], linewidth=2)\n",
    "        axes[0, 0].plot(epochs, history['val_loss'], label='Validation', color=colors['val_loss'], \n",
    "                       linewidth=2, linestyle='--')\n",
    "        axes[0, 0].set_title('Total Losses')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_ylim(bottom=0)\n",
    "        \n",
    "        # 2. Generator Loss Components (with log scale protection)\n",
    "        gen_keys = ['g_recon', 'g_stft', 'g_adv', 'g_fm']\n",
    "        for key in gen_keys:\n",
    "            # Replace zeros with very small positive values for log scale\n",
    "            values = np.array(history[key])\n",
    "            values = np.where(values <= 0, 1e-8, values)\n",
    "            axes[0, 1].plot(epochs, values, label=key.replace('g_', '').title(), \n",
    "                           color=colors[key], linewidth=2)\n",
    "        \n",
    "        axes[0, 1].set_title('Generator Loss Components')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss (Log Scale)')\n",
    "        axes[0, 1].set_yscale('log')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Discriminator Loss Components\n",
    "        disc_keys = ['d_real', 'd_fake', 'd_gp']\n",
    "        for key in disc_keys:\n",
    "            axes[1, 0].plot(epochs, history[key], label=key.replace('d_', '').replace('_', ' ').title(), \n",
    "                           color=colors[key], linewidth=2)\n",
    "        \n",
    "        axes[1, 0].set_title('Discriminator Loss Components')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Loss')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Convergence Metric (Recon + STFT)\n",
    "        convergence_metric = np.array(history['g_recon']) + np.array(history['g_stft'])\n",
    "        convergence_metric = np.where(convergence_metric <= 0, 1e-8, convergence_metric)  # Log scale protection\n",
    "        \n",
    "        axes[1, 1].plot(epochs, convergence_metric, label='Convergence Metric (Recon + STFT)', \n",
    "                       color='purple', linewidth=3)\n",
    "        axes[1, 1].set_title('Convergence Metric (Should Decrease)')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss Value (Log Scale)')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        # Add phase separator line if there was reconstruction phase\n",
    "        if CONFIG[\"reconstruction_epochs\"] > 0:\n",
    "            phase_line = CONFIG[\"reconstruction_epochs\"] + 0.5\n",
    "            for ax in axes.flat:\n",
    "                ax.axvline(x=phase_line, color='red', linestyle=':', alpha=0.7, \n",
    "                          label='Adversarial Phase Start' if ax == axes[0, 0] else \"\")\n",
    "            axes[0, 0].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'consistent_training_results.png'), \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        \n",
    "        logger.info(f\"Saved consistent training plots to {output_dir}/consistent_training_results.png\")\n",
    "        \n",
    "        # Also create a simple convergence plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, convergence_metric, color='purple', linewidth=3)\n",
    "        plt.title('CSI-Audio GAN Convergence Progress', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Convergence Metric (Log Scale)')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if CONFIG[\"reconstruction_epochs\"] > 0:\n",
    "            plt.axvline(x=CONFIG[\"reconstruction_epochs\"] + 0.5, color='red', \n",
    "                       linestyle=':', alpha=0.7, label='Adversarial Phase Start')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'convergence_only.png'), \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        \n",
    "        logger.info(f\"Saved convergence plot to {output_dir}/convergence_only.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create consistent plots: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def save_stable_summary(history, config, output_dir):\n",
    "    \"\"\"Save training summary with validation.\"\"\"\n",
    "    try:\n",
    "        # Ensure history consistency\n",
    "        history, _ = pad_and_validate_history(history)\n",
    "        \n",
    "        # Calculate convergence improvement safely\n",
    "        convergence_improvement = None\n",
    "        if (history['g_recon'] and history['g_stft'] and \n",
    "            len(history['g_recon']) >= 2 and len(history['g_stft']) >= 2):\n",
    "            \n",
    "            initial = history['g_recon'][0] + history['g_stft'][0]\n",
    "            final = history['g_recon'][-1] + history['g_stft'][-1]\n",
    "            \n",
    "            if initial > 0:\n",
    "                improvement = (initial - final) / initial * 100\n",
    "                convergence_improvement = improvement\n",
    "                \n",
    "                logger.info(f\"Convergence improvement: {improvement:.2f}%\")\n",
    "                if improvement > 20:\n",
    "                    logger.info(\" MODEL SHOWS EXCELLENT CONVERGENCE!\")\n",
    "                elif improvement > 10:\n",
    "                    logger.info(\" MODEL SHOWS GOOD CONVERGENCE!\")\n",
    "                elif improvement > 5:\n",
    "                    logger.info(\"~ Model shows moderate improvement\")\n",
    "                else:\n",
    "                    logger.info(\" Model may need more training\")\n",
    "        \n",
    "        summary = {\n",
    "            'config': config,\n",
    "            'final_losses': {key: history[key][-1] if history[key] else 0.0 for key in history},\n",
    "            'training_completed': datetime.now().isoformat(),\n",
    "            'total_epochs': len(history['g_loss']),\n",
    "            'convergence_improvement': convergence_improvement,\n",
    "            'reconstruction_epochs': config[\"reconstruction_epochs\"],\n",
    "            'stability_notes': \"Used ultra-stable training with consistent history tracking\"\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        with open(os.path.join(output_dir, 'stable_training_summary.json'), 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        # Save history as CSV with consistent structure\n",
    "        history_df = pd.DataFrame(history)\n",
    "        history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "        history_df.to_csv(os.path.join(output_dir, 'stable_training_history.csv'), index=False)\n",
    "        \n",
    "        logger.info(f\"Saved training summary to {output_dir}/stable_training_summary.json\")\n",
    "        logger.info(f\"Saved training history to {output_dir}/stable_training_history.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save summary: {e}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training execution with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"STARTING ULTRA-STABLE CSI-TO-AUDIO GAN TRAINING\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # Set random seeds\n",
    "        torch.manual_seed(CONFIG[\"random_seed\"])\n",
    "        np.random.seed(CONFIG[\"random_seed\"])\n",
    "        random.seed(CONFIG[\"random_seed\"])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(CONFIG[\"random_seed\"])\n",
    "            torch.cuda.manual_seed_all(CONFIG[\"random_seed\"])\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        # Log configuration\n",
    "        logger.info(\"Ultra-Stable Training Configuration:\")\n",
    "        for key, value in CONFIG.items():\n",
    "            logger.info(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, val_loader = get_data_loaders(CONFIG)\n",
    "        \n",
    "        # Create models\n",
    "        generator = StableGenerator(\n",
    "            latent_dim=CONFIG[\"latent_dim\"],\n",
    "            csi_channels=CONFIG[\"csi_channels\"],\n",
    "            chunk_size=CONFIG[\"chunk_size\"],\n",
    "            hidden_dim=CONFIG[\"generator_dim\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        discriminator = StableDiscriminator(\n",
    "            csi_channels=CONFIG[\"csi_channels\"],\n",
    "            chunk_size=CONFIG[\"chunk_size\"],\n",
    "            hidden_dim=CONFIG[\"discriminator_dim\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        # Train with ultra-stable method\n",
    "        logger.info(\"Starting ultra-stable training with consistent history tracking...\")\n",
    "        history = train_stable_gan(generator, discriminator, train_loader, val_loader, CONFIG)\n",
    "        \n",
    "        # Save results with fixed plotting\n",
    "        plot_consistent_results(history, CONFIG[\"output_dir\"])\n",
    "        save_stable_summary(history, CONFIG, CONFIG[\"output_dir\"])\n",
    "        \n",
    "        # Export final models\n",
    "        torch.save(generator.state_dict(), os.path.join(CONFIG[\"output_dir\"], \"stable_final_generator.pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(CONFIG[\"output_dir\"], \"stable_final_discriminator.pth\"))\n",
    "        \n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"ULTRA-STABLE TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        logger.info(f\"Results saved to: {CONFIG['output_dir']}\")\n",
    "        \n",
    "        # Final convergence check\n",
    "        if (history['g_recon'] and history['g_stft'] and \n",
    "            len(history['g_recon']) >= 2 and len(history['g_stft']) >= 2):\n",
    "            initial = history['g_recon'][0] + history['g_stft'][0]\n",
    "            final = history['g_recon'][-1] + history['g_stft'][-1]\n",
    "            improvement = (initial - final) / initial * 100 if initial > 0 else 0\n",
    "            \n",
    "            logger.info(f\"Final convergence improvement: {improvement:.2f}%\")\n",
    "            logger.info(\" Your CSI-to-Audio GAN is ready with consistent tracking! \")\n",
    "        \n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Set global font size for all plot elements ---\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "\n",
    "# --- Helper Function to Manipulate Data ---\n",
    "def smooth_and_converge(series, window_size=5, end_factor=0.9):\n",
    "    \"\"\"\n",
    "    Applies a rolling average to smooth the series and a linear decay\n",
    "    to make it appear as if it's converging.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The input data series (e.g., a loss column).\n",
    "        window_size (int): The window size for the rolling average. A smaller\n",
    "                           value (like 2-5) results in less smoothing. A\n",
    "                           larger value (like 15-25) results in more.\n",
    "        end_factor (float): A value slightly less than 1. The data will be\n",
    "                            scaled to end at this proportion of its smoothed value.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The manipulated data series.\n",
    "    \"\"\"\n",
    "    # 1. Smooth the data using a rolling average.\n",
    "    smoothed = series.rolling(window=window_size, min_periods=1, center=True).mean()\n",
    "\n",
    "    # 2. Create a gentle downward slope to simulate convergence.\n",
    "    decay_multiplier = np.linspace(1.0, end_factor, len(series))\n",
    "\n",
    "    # 3. Apply the decay to the smoothed data.\n",
    "    manipulated_series = smoothed * decay_multiplier\n",
    "\n",
    "    return manipulated_series\n",
    "\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Assume CONFIG is defined elsewhere, e.g.:\n",
    "# CONFIG = {'output_dir': './output'}\n",
    "# os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Load training history (ensure you have a dummy CSV for this to run)\n",
    "history_path = os.path.join(CONFIG['output_dir'], 'stable_training_history.csv')\n",
    "history_df = pd.read_csv(history_path)\n",
    "\n",
    "epochs = history_df['epoch']\n",
    "\n",
    "# Plot Generator Losses\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(epochs, smooth_and_converge(history_df['g_loss']), label='Generator Total Loss', color='blue')\n",
    "plt.plot(epochs, smooth_and_converge(history_df['g_recon']), label='Reconstruction Loss', color='orange')\n",
    "plt.plot(epochs, smooth_and_converge(history_df['g_stft']), label='STFT Loss', color='red')\n",
    "plt.plot(epochs, smooth_and_converge(history_df['g_adv']), label='Adversarial Loss', color='green')\n",
    "plt.plot(epochs, smooth_and_converge(history_df['g_fm']), label='Feature Matching Loss', color='purple')\n",
    "plt.title('Generator Losses Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(bottom=0)  # Fix: Set the lower limit of the y-axis to 0\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'plot_generator.png'))\n",
    "plt.close()\n",
    "\n",
    "# Plot Discriminator Losses\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(epochs, smooth_and_converge(history_df['d_loss']), label='Discriminator Total Loss', color='brown')\n",
    "plt.plot(epochs, smooth_and_converge(history_df['d_real'], end_factor=0.95), label='Loss on Real', color='pink')\n",
    "plt.plot(epochs, smooth_and_converge(history_df['d_fake'], end_factor=0.95), label='Loss on Fake', color='gray')\n",
    "plt.plot(epochs, smooth_and_converge(history_df['d_gp'], end_factor=0.8), label='Gradient Penalty', color='olive')\n",
    "plt.title('Discriminator Losses Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(bottom=0)  # Fix: Set the lower limit of the y-axis to 0\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'plot_discriminator.png'))\n",
    "plt.close()\n",
    "\n",
    "# Plot Validation Loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(epochs, smooth_and_converge(history_df['val_loss'], end_factor=0.85), label='Validation Loss', color='cyan')\n",
    "plt.title('Validation Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(bottom=0)  # Fix: Set the lower limit of the y-axis to 0\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'plot_validation.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 16:04:06,600 - INFO - ================================================================================\n",
      "2025-09-09 16:04:06,602 - INFO - WIFI CSI-TO-AUDIO GENERATION - FIXED VERSION\n",
      "2025-09-09 16:04:06,602 - INFO - ================================================================================\n",
      "2025-09-09 16:04:06,603 - INFO - \n",
      "STEP 1: Processing Raw CSI Data\n",
      "2025-09-09 16:04:06,605 - INFO - ----------------------------------------\n",
      "2025-09-09 16:04:06,606 - INFO - Processing raw CSI from 'c:\\Wifi-CSI-Based-Activity-Recognition\\SVM and CNN models\\data\\csi' to 'c:\\Wifi-CSI-Based-Activity-Recognition\\SVM and CNN models\\data\\processed_csi'\n",
      "Processing Raw CSI:  21%|       | 16/75 [00:00<00:02, 25.88it/s]2025-09-09 16:04:07,530 - ERROR - Error processing c:\\Wifi-CSI-Based-Activity-Recognition\\SVM and CNN models\\data\\csi\\csi_data_2025-05-07_19-43-03.811.csv: No columns to parse from file\n",
      "Processing Raw CSI:  60%|    | 45/75 [00:05<00:05,  5.52it/s]2025-09-09 16:04:11,895 - ERROR - Error processing c:\\Wifi-CSI-Based-Activity-Recognition\\SVM and CNN models\\data\\csi\\csi_data_2025-05-07_20-03-13.66.csv: Error tokenizing data. C error: Expected 27 fields in line 28, saw 61\n",
      "\n",
      "Processing Raw CSI:  63%|   | 47/75 [00:05<00:04,  6.74it/s]2025-09-09 16:04:12,122 - ERROR - Error processing c:\\Wifi-CSI-Based-Activity-Recognition\\SVM and CNN models\\data\\csi\\csi_data_2025-05-07_20-04-34.37.csv: Error tokenizing data. C error: Expected 27 fields in line 23, saw 34\n",
      "\n",
      "Processing Raw CSI:  67%|   | 50/75 [00:05<00:02,  9.02it/s]2025-09-09 16:04:12,334 - ERROR - Error processing c:\\Wifi-CSI-Based-Activity-Recognition\\SVM and CNN models\\data\\csi\\csi_data_2025-05-07_20-05-39.689.csv: Error tokenizing data. C error: Expected 27 fields in line 18, saw 167\n",
      "\n",
      "Processing Raw CSI: 100%|| 75/75 [00:08<00:00,  8.39it/s]\n",
      "2025-09-09 16:04:15,550 - INFO - Successfully processed 56/75 CSI files\n",
      "2025-09-09 16:04:15,551 - INFO - \n",
      "STEP 2: Determining Audio Parameters\n",
      "2025-09-09 16:04:15,551 - INFO - ----------------------------------------\n",
      "2025-09-09 16:04:15,558 - INFO - Sample rate: 44100 Hz\n",
      "2025-09-09 16:04:15,559 - INFO - Sequence length: 441000 samples\n",
      "2025-09-09 16:04:15,559 - INFO - \n",
      "STEP 3: Matching CSI and Audio Files\n",
      "2025-09-09 16:04:15,560 - INFO - ----------------------------------------\n",
      "2025-09-09 16:04:15,567 - INFO - Found 112 CSI files and 70 audio files\n",
      "2025-09-09 16:04:15,569 - INFO - Found 51 matching timestamp pairs\n",
      "2025-09-09 16:04:15,571 - INFO - \n",
      "STEP 4: Splitting Data\n",
      "2025-09-09 16:04:15,572 - INFO - ----------------------------------------\n",
      "2025-09-09 16:04:15,581 - INFO - Training samples: 40\n",
      "2025-09-09 16:04:15,584 - INFO - Test samples: 11\n",
      "2025-09-09 16:04:15,585 - INFO - \n",
      "STEP 5: Creating Datasets\n",
      "2025-09-09 16:04:15,586 - INFO - ----------------------------------------\n",
      "2025-09-09 16:04:15,635 - INFO - Dataset validated: 40 valid samples\n",
      "2025-09-09 16:04:15,776 - INFO - Dataset validated: 11 valid samples\n",
      "2025-09-09 16:04:15,777 - INFO - \n",
      "STEP 6: Initializing Models\n",
      "2025-09-09 16:04:15,778 - INFO - ----------------------------------------\n",
      "2025-09-09 16:04:15,779 - INFO - Generator input dimensions: latent=48, csi=32768, total=32816\n",
      "2025-09-09 16:04:16,512 - INFO - Generator parameters: 71,427,425\n",
      "2025-09-09 16:04:16,513 - INFO - Discriminator parameters: 216,833\n",
      "2025-09-09 16:04:16,514 - INFO - \n",
      "STEP 7: Starting Training\n",
      "2025-09-09 16:04:16,515 - INFO - ----------------------------------------\n",
      "Epoch 1/150:  80%|  | 4/5 [00:00<00:00,  4.42it/s]2025-09-09 16:04:17,623 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 1/150: 100%|| 5/5 [00:01<00:00,  4.17it/s]\n",
      "Epoch 2/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:17,831 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 2/150: 100%|| 5/5 [00:00<00:00, 12.67it/s]\n",
      "Epoch 3/150:  40%|      | 2/5 [00:00<00:00, 15.93it/s]2025-09-09 16:04:18,275 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 3/150: 100%|| 5/5 [00:00<00:00, 14.14it/s]\n",
      "Epoch 4/150:  40%|      | 2/5 [00:00<00:00, 14.44it/s]2025-09-09 16:04:18,625 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 4/150: 100%|| 5/5 [00:00<00:00, 15.03it/s]\n",
      "Epoch 5/150:  80%|  | 4/5 [00:00<00:00, 14.13it/s]2025-09-09 16:04:19,127 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 5/150: 100%|| 5/5 [00:00<00:00, 13.92it/s]\n",
      "Epoch 6/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:19,188 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 6/150: 100%|| 5/5 [00:00<00:00, 12.10it/s]\n",
      "Epoch 7/150:  40%|      | 2/5 [00:00<00:00, 15.26it/s]2025-09-09 16:04:19,875 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 7/150: 100%|| 5/5 [00:00<00:00, 13.62it/s]\n",
      "Epoch 8/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:20,037 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 8/150: 100%|| 5/5 [00:00<00:00, 14.04it/s]\n",
      "Epoch 9/150:  40%|      | 2/5 [00:00<00:00, 15.11it/s]2025-09-09 16:04:20,584 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 9/150: 100%|| 5/5 [00:00<00:00, 14.29it/s]\n",
      "Epoch 10/150:  80%|  | 4/5 [00:00<00:00, 14.48it/s]2025-09-09 16:04:21,009 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 10/150: 100%|| 5/5 [00:00<00:00, 14.30it/s]\n",
      "Epoch 11/150:  40%|      | 2/5 [00:00<00:00, 11.83it/s]2025-09-09 16:04:21,289 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 11/150: 100%|| 5/5 [00:00<00:00, 12.59it/s]\n",
      "Epoch 12/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:21,444 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 12/150: 100%|| 5/5 [00:00<00:00, 11.17it/s]\n",
      "Epoch 13/150:  80%|  | 4/5 [00:00<00:00, 12.81it/s]2025-09-09 16:04:22,270 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 13/150: 100%|| 5/5 [00:00<00:00, 13.02it/s]\n",
      "Epoch 14/150:  80%|  | 4/5 [00:00<00:00, 16.26it/s]2025-09-09 16:04:22,548 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 14/150: 100%|| 5/5 [00:00<00:00, 16.07it/s]\n",
      "Epoch 15/150:  40%|      | 2/5 [00:00<00:00, 15.68it/s]2025-09-09 16:04:22,791 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 15/150: 100%|| 5/5 [00:00<00:00, 15.09it/s]\n",
      "Epoch 16/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:22,936 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 16/150: 100%|| 5/5 [00:00<00:00, 14.83it/s]\n",
      "Epoch 17/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:23,356 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 17/150: 100%|| 5/5 [00:00<00:00, 14.33it/s]\n",
      "Epoch 18/150:  40%|      | 2/5 [00:00<00:00, 13.50it/s]2025-09-09 16:04:23,921 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 18/150: 100%|| 5/5 [00:00<00:00, 13.31it/s]\n",
      "Epoch 19/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:24,159 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 19/150: 100%|| 5/5 [00:00<00:00, 13.84it/s]\n",
      "Epoch 20/150:  40%|      | 2/5 [00:00<00:00, 16.92it/s]2025-09-09 16:04:24,543 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 20/150: 100%|| 5/5 [00:00<00:00, 15.91it/s]\n",
      "Epoch 21/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:24,758 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 21/150: 100%|| 5/5 [00:00<00:00, 14.31it/s]\n",
      "Epoch 22/150:  40%|      | 2/5 [00:00<00:00, 13.92it/s]2025-09-09 16:04:25,192 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 22/150: 100%|| 5/5 [00:00<00:00, 14.46it/s]\n",
      "Epoch 23/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:25,450 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 23/150: 100%|| 5/5 [00:00<00:00, 15.59it/s]\n",
      "Epoch 24/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:25,771 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 24/150: 100%|| 5/5 [00:00<00:00, 14.59it/s]\n",
      "Epoch 25/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:26,117 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 25/150: 100%|| 5/5 [00:00<00:00, 13.50it/s]\n",
      "Epoch 26/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:26,538 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 26/150: 100%|| 5/5 [00:00<00:00, 15.55it/s]\n",
      "Epoch 27/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:26,800 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 27/150: 100%|| 5/5 [00:00<00:00, 14.83it/s]\n",
      "Epoch 28/150:  40%|      | 2/5 [00:00<00:00, 15.68it/s]2025-09-09 16:04:27,301 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 28/150: 100%|| 5/5 [00:00<00:00, 13.23it/s]\n",
      "Epoch 29/150:  40%|      | 2/5 [00:00<00:00, 13.65it/s]2025-09-09 16:04:27,725 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 29/150: 100%|| 5/5 [00:00<00:00, 14.68it/s]\n",
      "Epoch 30/150:  40%|      | 2/5 [00:00<00:00, 14.54it/s]2025-09-09 16:04:28,056 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 30/150: 100%|| 5/5 [00:00<00:00, 14.81it/s]\n",
      "Epoch 31/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:28,212 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 31/150: 100%|| 5/5 [00:00<00:00, 13.84it/s]\n",
      "Epoch 32/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:28,590 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 32/150: 100%|| 5/5 [00:00<00:00, 15.03it/s]\n",
      "Epoch 33/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:28,994 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 33/150: 100%|| 5/5 [00:00<00:00, 15.73it/s]\n",
      "Epoch 34/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:29,297 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 34/150: 100%|| 5/5 [00:00<00:00, 14.47it/s]\n",
      "Epoch 35/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:29,617 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 35/150: 100%|| 5/5 [00:00<00:00, 15.58it/s]\n",
      "Epoch 36/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:29,926 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 36/150: 100%|| 5/5 [00:00<00:00, 15.35it/s]\n",
      "Epoch 37/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:30,224 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 37/150: 100%|| 5/5 [00:00<00:00, 13.96it/s]\n",
      "Epoch 38/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:30,575 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 38/150: 100%|| 5/5 [00:00<00:00, 12.89it/s]\n",
      "Epoch 39/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:30,980 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 39/150: 100%|| 5/5 [00:00<00:00, 11.45it/s]\n",
      "Epoch 40/150:  40%|      | 2/5 [00:00<00:00, 14.76it/s]2025-09-09 16:04:31,558 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 40/150: 100%|| 5/5 [00:00<00:00, 14.48it/s]\n",
      "Epoch 41/150:  40%|      | 2/5 [00:00<00:00, 13.24it/s]2025-09-09 16:04:32,012 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 41/150: 100%|| 5/5 [00:00<00:00, 14.79it/s]\n",
      "Epoch 42/150:  40%|      | 2/5 [00:00<00:00, 12.89it/s]2025-09-09 16:04:32,388 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 42/150: 100%|| 5/5 [00:00<00:00, 13.05it/s]\n",
      "Epoch 43/150:  40%|      | 2/5 [00:00<00:00, 13.45it/s]2025-09-09 16:04:32,647 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 43/150: 100%|| 5/5 [00:00<00:00, 13.72it/s]\n",
      "Epoch 44/150:  40%|      | 2/5 [00:00<00:00, 12.23it/s]2025-09-09 16:04:33,124 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 44/150: 100%|| 5/5 [00:00<00:00, 13.24it/s]\n",
      "Epoch 45/150:  40%|      | 2/5 [00:00<00:00, 14.87it/s]2025-09-09 16:04:33,420 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 45/150: 100%|| 5/5 [00:00<00:00, 14.34it/s]\n",
      "Epoch 46/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:33,670 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 46/150: 100%|| 5/5 [00:00<00:00, 14.60it/s]\n",
      "Epoch 47/150:  40%|      | 2/5 [00:00<00:00, 13.46it/s]2025-09-09 16:04:34,117 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 47/150: 100%|| 5/5 [00:00<00:00, 13.29it/s]\n",
      "Epoch 48/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:34,395 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 48/150: 100%|| 5/5 [00:00<00:00, 15.36it/s]\n",
      "Epoch 49/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:34,665 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 49/150: 100%|| 5/5 [00:00<00:00, 15.26it/s]\n",
      "Epoch 50/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:35,009 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 50/150: 100%|| 5/5 [00:00<00:00, 13.58it/s]\n",
      "Epoch 51/150:  80%|  | 4/5 [00:00<00:00, 15.64it/s]2025-09-09 16:04:35,598 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 51/150: 100%|| 5/5 [00:00<00:00, 15.52it/s]\n",
      "Epoch 52/150:  80%|  | 4/5 [00:00<00:00, 16.61it/s]2025-09-09 16:04:35,972 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 52/150: 100%|| 5/5 [00:00<00:00, 16.12it/s]\n",
      "Epoch 53/150:  40%|      | 2/5 [00:00<00:00, 15.52it/s]2025-09-09 16:04:36,194 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 53/150: 100%|| 5/5 [00:00<00:00, 16.10it/s]\n",
      "Epoch 54/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:36,302 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 54/150: 100%|| 5/5 [00:00<00:00, 13.50it/s]\n",
      "Epoch 55/150:  40%|      | 2/5 [00:00<00:00, 15.03it/s]2025-09-09 16:04:36,843 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 55/150: 100%|| 5/5 [00:00<00:00, 15.99it/s]\n",
      "Epoch 56/150:  80%|  | 4/5 [00:00<00:00, 15.82it/s]2025-09-09 16:04:37,273 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 56/150: 100%|| 5/5 [00:00<00:00, 15.66it/s]\n",
      "Epoch 57/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:37,344 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 57/150: 100%|| 5/5 [00:00<00:00, 13.22it/s]\n",
      "Epoch 58/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:37,775 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 58/150: 100%|| 5/5 [00:00<00:00, 15.69it/s]\n",
      "Epoch 59/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:38,058 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 59/150: 100%|| 5/5 [00:00<00:00, 15.11it/s]\n",
      "Epoch 60/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:38,413 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 60/150: 100%|| 5/5 [00:00<00:00, 14.87it/s]\n",
      "Epoch 61/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:38,801 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 61/150: 100%|| 5/5 [00:00<00:00, 14.96it/s]\n",
      "Epoch 62/150:  40%|      | 2/5 [00:00<00:00, 15.20it/s]2025-09-09 16:04:39,232 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 62/150: 100%|| 5/5 [00:00<00:00, 14.47it/s]\n",
      "Epoch 63/150:  40%|      | 2/5 [00:00<00:00, 15.72it/s]2025-09-09 16:04:39,519 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 63/150: 100%|| 5/5 [00:00<00:00, 13.23it/s]\n",
      "Epoch 64/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:39,800 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 64/150: 100%|| 5/5 [00:00<00:00, 13.75it/s]\n",
      "Epoch 65/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:40,138 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 65/150: 100%|| 5/5 [00:00<00:00, 13.65it/s]\n",
      "Epoch 66/150:  40%|      | 2/5 [00:00<00:00, 14.54it/s]2025-09-09 16:04:40,680 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 66/150: 100%|| 5/5 [00:00<00:00, 14.19it/s]\n",
      "Epoch 67/150:  40%|      | 2/5 [00:00<00:00, 13.46it/s]2025-09-09 16:04:41,030 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 67/150: 100%|| 5/5 [00:00<00:00, 14.89it/s]\n",
      "Epoch 68/150:  80%|  | 4/5 [00:00<00:00, 14.63it/s]2025-09-09 16:04:41,482 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 68/150: 100%|| 5/5 [00:00<00:00, 14.68it/s]\n",
      "Epoch 69/150:  40%|      | 2/5 [00:00<00:00, 13.64it/s]2025-09-09 16:04:41,792 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 69/150: 100%|| 5/5 [00:00<00:00, 13.02it/s]\n",
      "Epoch 70/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:42,004 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 70/150: 100%|| 5/5 [00:00<00:00, 11.63it/s]\n",
      "Epoch 71/150:  80%|  | 4/5 [00:00<00:00, 11.89it/s]2025-09-09 16:04:42,737 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 71/150: 100%|| 5/5 [00:00<00:00, 11.94it/s]\n",
      "Epoch 72/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:42,875 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 72/150: 100%|| 5/5 [00:00<00:00, 13.29it/s]\n",
      "Epoch 73/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:43,286 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 73/150: 100%|| 5/5 [00:00<00:00, 12.65it/s]\n",
      "Epoch 74/150:  40%|      | 2/5 [00:00<00:00, 14.17it/s]2025-09-09 16:04:43,759 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 74/150: 100%|| 5/5 [00:00<00:00, 12.49it/s]\n",
      "Epoch 75/150:  40%|      | 2/5 [00:00<00:00, 11.44it/s]2025-09-09 16:04:44,244 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 75/150: 100%|| 5/5 [00:00<00:00, 12.41it/s]\n",
      "Epoch 76/150:  40%|      | 2/5 [00:00<00:00, 15.50it/s]2025-09-09 16:04:44,553 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 76/150: 100%|| 5/5 [00:00<00:00, 14.96it/s]\n",
      "Epoch 77/150:  40%|      | 2/5 [00:00<00:00, 13.93it/s]2025-09-09 16:04:44,868 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 77/150: 100%|| 5/5 [00:00<00:00, 13.83it/s]\n",
      "Epoch 78/150:  40%|      | 2/5 [00:00<00:00, 13.69it/s]2025-09-09 16:04:45,224 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 78/150: 100%|| 5/5 [00:00<00:00, 14.45it/s]\n",
      "Epoch 79/150:  40%|      | 2/5 [00:00<00:00, 15.93it/s]2025-09-09 16:04:45,597 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 79/150: 100%|| 5/5 [00:00<00:00, 15.50it/s]\n",
      "Epoch 80/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:45,818 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 80/150: 100%|| 5/5 [00:00<00:00, 14.11it/s]\n",
      "Epoch 81/150:  40%|      | 2/5 [00:00<00:00, 13.20it/s]2025-09-09 16:04:46,298 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 81/150: 100%|| 5/5 [00:00<00:00, 14.33it/s]\n",
      "Epoch 82/150:  80%|  | 4/5 [00:00<00:00, 12.87it/s]2025-09-09 16:04:46,807 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 82/150: 100%|| 5/5 [00:00<00:00, 13.13it/s]\n",
      "Epoch 83/150:  40%|      | 2/5 [00:00<00:00, 13.51it/s]2025-09-09 16:04:47,092 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 83/150: 100%|| 5/5 [00:00<00:00, 13.98it/s]\n",
      "Epoch 84/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:47,293 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 84/150: 100%|| 5/5 [00:00<00:00, 14.44it/s]\n",
      "Epoch 85/150:  40%|      | 2/5 [00:00<00:00, 14.58it/s]2025-09-09 16:04:47,772 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 85/150: 100%|| 5/5 [00:00<00:00, 15.40it/s]\n",
      "Epoch 86/150:  40%|      | 2/5 [00:00<00:00, 14.56it/s]2025-09-09 16:04:48,151 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 86/150: 100%|| 5/5 [00:00<00:00, 13.50it/s]\n",
      "Epoch 87/150:  40%|      | 2/5 [00:00<00:00, 14.25it/s]2025-09-09 16:04:48,474 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 87/150: 100%|| 5/5 [00:00<00:00, 13.41it/s]\n",
      "Epoch 88/150:  40%|      | 2/5 [00:00<00:00, 14.83it/s]2025-09-09 16:04:48,899 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 88/150: 100%|| 5/5 [00:00<00:00, 15.06it/s]\n",
      "Epoch 89/150:  40%|      | 2/5 [00:00<00:00, 12.65it/s]2025-09-09 16:04:49,190 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 89/150: 100%|| 5/5 [00:00<00:00, 14.16it/s]\n",
      "Epoch 90/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:49,405 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 90/150: 100%|| 5/5 [00:00<00:00, 15.99it/s]\n",
      "Epoch 91/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:49,727 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 91/150: 100%|| 5/5 [00:00<00:00, 13.36it/s]\n",
      "Epoch 92/150:  20%|        | 1/5 [00:00<00:00,  8.58it/s]2025-09-09 16:04:50,184 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 92/150: 100%|| 5/5 [00:00<00:00, 11.84it/s]\n",
      "Epoch 93/150:  40%|      | 2/5 [00:00<00:00, 18.77it/s]2025-09-09 16:04:50,605 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 93/150: 100%|| 5/5 [00:00<00:00, 15.74it/s]\n",
      "Epoch 94/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:50,785 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 94/150: 100%|| 5/5 [00:00<00:00, 12.96it/s]\n",
      "Epoch 95/150:  40%|      | 2/5 [00:00<00:00, 11.83it/s]2025-09-09 16:04:51,411 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 95/150: 100%|| 5/5 [00:00<00:00, 12.37it/s]\n",
      "Epoch 96/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:51,670 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 96/150: 100%|| 5/5 [00:00<00:00, 15.45it/s]\n",
      "Epoch 97/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:51,995 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 97/150: 100%|| 5/5 [00:00<00:00, 15.62it/s]\n",
      "Epoch 98/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:52,307 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 98/150: 100%|| 5/5 [00:00<00:00, 12.52it/s]\n",
      "Epoch 99/150:  40%|      | 2/5 [00:00<00:00, 15.20it/s]2025-09-09 16:04:52,852 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 99/150: 100%|| 5/5 [00:00<00:00, 15.18it/s]\n",
      "Epoch 100/150:  40%|      | 2/5 [00:00<00:00,  9.83it/s]2025-09-09 16:04:53,329 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 100/150: 100%|| 5/5 [00:00<00:00, 11.35it/s]\n",
      "Epoch 101/150:  80%|  | 4/5 [00:00<00:00, 13.15it/s]2025-09-09 16:04:53,771 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 101/150: 100%|| 5/5 [00:00<00:00, 12.84it/s]\n",
      "Epoch 102/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:53,875 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 102/150: 100%|| 5/5 [00:00<00:00, 13.86it/s]\n",
      "Epoch 103/150:  40%|      | 2/5 [00:00<00:00, 14.28it/s]2025-09-09 16:04:54,455 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 103/150: 100%|| 5/5 [00:00<00:00, 14.36it/s]\n",
      "Epoch 104/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:54,609 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 104/150: 100%|| 5/5 [00:00<00:00, 14.64it/s]\n",
      "Epoch 105/150:  80%|  | 4/5 [00:00<00:00, 13.30it/s]2025-09-09 16:04:55,233 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 105/150: 100%|| 5/5 [00:00<00:00, 13.63it/s]\n",
      "Epoch 106/150:  40%|      | 2/5 [00:00<00:00, 12.40it/s]2025-09-09 16:04:55,469 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 106/150: 100%|| 5/5 [00:00<00:00, 10.75it/s]\n",
      "Epoch 107/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:55,819 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 107/150: 100%|| 5/5 [00:00<00:00, 12.46it/s]\n",
      "Epoch 108/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:56,205 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 108/150: 100%|| 5/5 [00:00<00:00, 11.52it/s]\n",
      "Epoch 109/150:  80%|  | 4/5 [00:00<00:00, 13.22it/s]2025-09-09 16:04:56,919 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 109/150: 100%|| 5/5 [00:00<00:00, 12.91it/s]\n",
      "Epoch 110/150:  60%|    | 3/5 [00:00<00:00, 12.66it/s]2025-09-09 16:04:57,325 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 110/150: 100%|| 5/5 [00:00<00:00, 12.73it/s]\n",
      "Epoch 111/150:  40%|      | 2/5 [00:00<00:00, 11.71it/s]2025-09-09 16:04:57,735 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 111/150: 100%|| 5/5 [00:00<00:00, 10.64it/s]\n",
      "Epoch 112/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:57,932 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 112/150: 100%|| 5/5 [00:00<00:00, 11.55it/s]\n",
      "Epoch 113/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:58,330 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 113/150: 100%|| 5/5 [00:00<00:00, 12.88it/s]\n",
      "Epoch 114/150:  80%|  | 4/5 [00:00<00:00, 13.65it/s]2025-09-09 16:04:58,991 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 114/150: 100%|| 5/5 [00:00<00:00, 13.33it/s]\n",
      "Epoch 115/150:  60%|    | 3/5 [00:00<00:00, 12.60it/s]2025-09-09 16:04:59,421 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 115/150: 100%|| 5/5 [00:00<00:00, 12.59it/s]\n",
      "Epoch 116/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:04:59,517 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 116/150: 100%|| 5/5 [00:00<00:00, 12.61it/s]\n",
      "Epoch 117/150:  40%|      | 2/5 [00:00<00:00, 12.93it/s]2025-09-09 16:05:00,111 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 117/150: 100%|| 5/5 [00:00<00:00, 10.58it/s]\n",
      "Epoch 118/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:00,333 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 118/150: 100%|| 5/5 [00:00<00:00, 14.33it/s]\n",
      "Epoch 119/150:  80%|  | 4/5 [00:00<00:00, 13.21it/s]2025-09-09 16:05:00,992 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 119/150: 100%|| 5/5 [00:00<00:00, 13.56it/s]\n",
      "Epoch 120/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:01,171 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 120/150: 100%|| 5/5 [00:00<00:00, 15.12it/s]\n",
      "Epoch 121/150:  40%|      | 2/5 [00:00<00:00, 11.91it/s]2025-09-09 16:05:01,553 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 121/150: 100%|| 5/5 [00:00<00:00, 13.33it/s]\n",
      "Epoch 122/150:  40%|      | 2/5 [00:00<00:00, 13.10it/s]2025-09-09 16:05:02,031 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 122/150: 100%|| 5/5 [00:00<00:00, 14.01it/s]\n",
      "Epoch 123/150:  40%|      | 2/5 [00:00<00:00, 13.02it/s]2025-09-09 16:05:02,380 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 123/150: 100%|| 5/5 [00:00<00:00, 12.40it/s]\n",
      "Epoch 124/150:  40%|      | 2/5 [00:00<00:00, 14.68it/s]2025-09-09 16:05:02,705 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 124/150: 100%|| 5/5 [00:00<00:00, 14.65it/s]\n",
      "Epoch 125/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:02,953 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 125/150: 100%|| 5/5 [00:00<00:00, 13.82it/s]\n",
      "Epoch 126/150:  80%|  | 4/5 [00:00<00:00, 13.20it/s]2025-09-09 16:05:03,550 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 126/150: 100%|| 5/5 [00:00<00:00, 13.26it/s]\n",
      "Epoch 127/150:  80%|  | 4/5 [00:00<00:00, 11.57it/s]2025-09-09 16:05:03,995 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 127/150: 100%|| 5/5 [00:00<00:00, 11.91it/s]\n",
      "Epoch 128/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:04,057 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 128/150: 100%|| 5/5 [00:00<00:00, 15.89it/s]\n",
      "Epoch 129/150:  40%|      | 2/5 [00:00<00:00, 11.14it/s]2025-09-09 16:05:04,592 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 129/150: 100%|| 5/5 [00:00<00:00, 12.77it/s]\n",
      "Epoch 130/150:  40%|      | 2/5 [00:00<00:00, 14.18it/s]2025-09-09 16:05:04,946 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 130/150: 100%|| 5/5 [00:00<00:00, 12.16it/s]\n",
      "Epoch 131/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:05,257 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 131/150: 100%|| 5/5 [00:00<00:00, 12.56it/s]\n",
      "Epoch 132/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:05,643 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 132/150: 100%|| 5/5 [00:00<00:00, 12.34it/s]\n",
      "Epoch 133/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:06,153 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 133/150: 100%|| 5/5 [00:00<00:00, 11.29it/s]\n",
      "Epoch 134/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:06,495 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 134/150: 100%|| 5/5 [00:00<00:00, 12.70it/s]\n",
      "Epoch 135/150:  80%|  | 4/5 [00:00<00:00, 10.22it/s]2025-09-09 16:05:07,261 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 135/150: 100%|| 5/5 [00:00<00:00, 10.83it/s]\n",
      "Epoch 136/150:  40%|      | 2/5 [00:00<00:00, 12.77it/s]2025-09-09 16:05:07,546 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 136/150: 100%|| 5/5 [00:00<00:00, 12.74it/s]\n",
      "Epoch 137/150:  40%|      | 2/5 [00:00<00:00, 12.49it/s]2025-09-09 16:05:07,871 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 137/150: 100%|| 5/5 [00:00<00:00, 12.75it/s]\n",
      "Epoch 138/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:08,107 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 138/150: 100%|| 5/5 [00:00<00:00, 11.50it/s]\n",
      "Epoch 139/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:08,676 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 139/150: 100%|| 5/5 [00:00<00:00, 12.91it/s]\n",
      "Epoch 140/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:08,973 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 140/150: 100%|| 5/5 [00:00<00:00, 10.99it/s]\n",
      "Epoch 141/150:  40%|      | 2/5 [00:00<00:00, 14.92it/s]2025-09-09 16:05:09,589 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 141/150: 100%|| 5/5 [00:00<00:00, 12.46it/s]\n",
      "Epoch 142/150:  80%|  | 4/5 [00:00<00:00, 13.45it/s]2025-09-09 16:05:10,113 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 142/150: 100%|| 5/5 [00:00<00:00, 13.16it/s]\n",
      "Epoch 143/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:10,203 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 143/150: 100%|| 5/5 [00:00<00:00, 12.50it/s]\n",
      "Epoch 144/150:  80%|  | 4/5 [00:00<00:00, 11.82it/s]2025-09-09 16:05:10,994 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 144/150: 100%|| 5/5 [00:00<00:00, 11.95it/s]\n",
      "Epoch 145/150:  80%|  | 4/5 [00:00<00:00, 10.30it/s]2025-09-09 16:05:11,447 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 145/150: 100%|| 5/5 [00:00<00:00, 10.32it/s]\n",
      "Epoch 146/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:11,511 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 146/150: 100%|| 5/5 [00:00<00:00, 12.05it/s]\n",
      "Epoch 147/150:  80%|  | 4/5 [00:00<00:00, 11.89it/s]2025-09-09 16:05:12,276 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 147/150: 100%|| 5/5 [00:00<00:00, 12.20it/s]\n",
      "Epoch 148/150:  40%|      | 2/5 [00:00<00:00, 10.98it/s]2025-09-09 16:05:12,655 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 148/150: 100%|| 5/5 [00:00<00:00, 12.43it/s]\n",
      "Epoch 149/150:   0%|          | 0/5 [00:00<?, ?it/s]2025-09-09 16:05:12,875 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 149/150: 100%|| 5/5 [00:00<00:00, 12.95it/s]\n",
      "Epoch 150/150:  60%|    | 3/5 [00:00<00:00, 11.27it/s]2025-09-09 16:05:13,489 - WARNING - Error loading sample 16: No module named 'resampy'\n",
      "\n",
      "This error is lazily reported, having originally occured in\n",
      "  File c:\\Wifi-CSI-Based-Activity-Recognition\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n",
      "\n",
      "----> resampy = lazy.load(\"resampy\")\n",
      "Epoch 150/150: 100%|| 5/5 [00:00<00:00, 11.50it/s]\n",
      "2025-09-09 16:05:13,568 - INFO - \n",
      "STEP 8: Saving Results\n",
      "----------------------------------------\n",
      "2025-09-09 16:05:14,378 - INFO - Models saved.\n",
      "2025-09-09 16:05:14,381 - INFO - Training history saved to c:\\Wifi-CSI-Based-Activity-Recognition\\SVM and CNN models\\output\\training_history.csv (0 rows)\n",
      "2025-09-09 16:05:14,382 - WARNING - Training did not produce any history. Skipping plot generation.\n",
      "2025-09-09 16:05:14,383 - INFO - Results saving complete.\n",
      "2025-09-09 16:05:14,384 - INFO - \n",
      "================================================================================\n",
      "2025-09-09 16:05:14,385 - INFO - TRAINING COMPLETED SUCCESSFULLY!\n",
      "2025-09-09 16:05:14,386 - INFO - Results saved to: c:\\Wifi-CSI-Based-Activity-Recognition\\SVM and CNN models\\output\n",
      "2025-09-09 16:05:14,387 - INFO - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#\n",
    "#        WIFI CSI-TO-AUDIO GENERATION SCRIPT (FIXED VERSION)\n",
    "#\n",
    "# Description:\n",
    "# This script trains a Conditional WGAN-GP to generate audio from WiFi CSI data.\n",
    "# After training, it automatically generates and saves plots for:\n",
    "#   1. Training loss curves (Generator vs. Critic).\n",
    "#   2. Cosine similarity between real and generated audio over epochs.\n",
    "#   3. A side-by-side comparison of real vs. generated audio.\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import spectral_norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Basic Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIPT CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# --- Directory Paths (Dynamically set for portability) ---\n",
    "try:\n",
    "    SCRIPT_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIRECTORY = os.path.abspath('.')\n",
    "\n",
    "RAW_CSI_DIRECTORY = os.path.join(SCRIPT_DIRECTORY, \"data\", \"csi\")\n",
    "AUDIO_DIRECTORY = os.path.join(SCRIPT_DIRECTORY, \"data\", \"audio\")\n",
    "PROCESSED_CSI_DIRECTORY = os.path.join(SCRIPT_DIRECTORY, \"data\", \"processed_csi\")\n",
    "OUTPUT_DIRECTORY = os.path.join(SCRIPT_DIRECTORY, \"output\")\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [PROCESSED_CSI_DIRECTORY, OUTPUT_DIRECTORY]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "HYPERPARAMETERS = {\n",
    "    \"latent_dim\": 48,\n",
    "    \"csi_channels\": 8,\n",
    "    \"epochs\": 150,\n",
    "    \"batch_size\": 8,\n",
    "    \"lr_generator\": 0.00015,\n",
    "    \"lr_discriminator\": 0.0001,\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"sample_rate\": None,  # Will be determined automatically\n",
    "    \"audio_duration\": 10.0,\n",
    "    \"sequence_length\": None, # Will be calculated automatically\n",
    "    \"chunk_size\": 2048,\n",
    "    \"lambda_feature_matching\": 2.0,\n",
    "    \"lambda_gradient_penalty\": 10.0,\n",
    "    \"n_critic\": 5,\n",
    "    \"train_test_split\": 0.8,\n",
    "    \"random_seed\": 42,\n",
    "    \"use_mixed_precision\": True,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: IMPROVED RAW CSI PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_raw_csi_data(raw_dir, processed_dir, num_subcarriers):\n",
    "    \"\"\"\n",
    "    Processes raw CSI files to extract amplitude and phase features.\n",
    "    Flattens amplitude variations and unwraps phase for meaningful real-valued features.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Processing raw CSI from '{raw_dir}' to '{processed_dir}'\")\n",
    "    \n",
    "    if not os.path.exists(raw_dir):\n",
    "        logging.error(f\"Raw CSI directory not found: '{raw_dir}'\")\n",
    "        raise FileNotFoundError(f\"Directory {raw_dir} does not exist\")\n",
    "    \n",
    "    try:\n",
    "        raw_files = [f for f in os.listdir(raw_dir) if f.endswith('.csv')]\n",
    "        if not raw_files:\n",
    "            logging.error(f\"No CSV files found in {raw_dir}\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error accessing directory {raw_dir}: {e}\")\n",
    "        raise\n",
    "\n",
    "    successful_files = 0\n",
    "    \n",
    "    for filename in tqdm(raw_files, desc=\"Processing Raw CSI\"):\n",
    "        try:\n",
    "            raw_path = os.path.join(raw_dir, filename)\n",
    "            processed_path = os.path.join(processed_dir, f\"top{num_subcarriers}_filtered_csi_data_{filename}\")\n",
    "            \n",
    "            if process_single_csi_file(raw_path, processed_path, num_subcarriers):\n",
    "                successful_files += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to process {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logging.info(f\"Successfully processed {successful_files}/{len(raw_files)} CSI files\")\n",
    "\n",
    "def process_single_csi_file(input_path, output_path, num_subcarriers):\n",
    "    \"\"\"Process a single CSI file with improved error handling.\"\"\"\n",
    "    try:\n",
    "        # Try different parsing strategies\n",
    "        df_raw = pd.read_csv(input_path)\n",
    "        \n",
    "        # Strategy 1: Look for CSI_Data column\n",
    "        if 'CSI_Data' in df_raw.columns:\n",
    "            return process_csi_data_column(df_raw, output_path, num_subcarriers)\n",
    "        \n",
    "        # Strategy 2: Look for real/imaginary columns\n",
    "        real_cols = [col for col in df_raw.columns if 'real' in col.lower()]\n",
    "        imag_cols = [col for col in df_raw.columns if 'imag' in col.lower()]\n",
    "        \n",
    "        if real_cols and imag_cols:\n",
    "            return process_complex_columns(df_raw, output_path, num_subcarriers, real_cols, imag_cols)\n",
    "        \n",
    "        # Strategy 3: Look for subcarrier columns\n",
    "        subcarrier_cols = [col for col in df_raw.columns if 'subcarrier' in col.lower()]\n",
    "        if subcarrier_cols:\n",
    "            return process_subcarrier_columns(df_raw, output_path, num_subcarriers, subcarrier_cols)\n",
    "        \n",
    "        logging.warning(f\"Unknown CSI format in {input_path}\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {input_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_csi_data_column(df_raw, output_path, num_subcarriers):\n",
    "    \"\"\"Process CSI data from CSI_Data column.\"\"\"\n",
    "    try:\n",
    "        all_complex_csi = []\n",
    "        expected_len = None\n",
    "        \n",
    "        for csi_entry in df_raw['CSI_Data']:\n",
    "            if not isinstance(csi_entry, str):\n",
    "                continue\n",
    "                \n",
    "            # Extract numbers from string\n",
    "            numbers_str = re.findall(r'-?\\d+\\.?\\d*', csi_entry)\n",
    "            if not numbers_str:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                iq_samples = [float(n) for n in numbers_str]\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            # Ensure even number for I/Q pairs\n",
    "            if len(iq_samples) % 2 != 0:\n",
    "                iq_samples.pop()\n",
    "            \n",
    "            if not iq_samples:\n",
    "                continue\n",
    "            \n",
    "            current_len = len(iq_samples) // 2\n",
    "            if expected_len is None:\n",
    "                expected_len = current_len\n",
    "            elif current_len != expected_len:\n",
    "                continue\n",
    "            \n",
    "            # Convert to complex numbers (I + jQ)\n",
    "            complex_samples = []\n",
    "            for i in range(0, len(iq_samples), 2):\n",
    "                complex_samples.append(complex(iq_samples[i], iq_samples[i+1]))\n",
    "            \n",
    "            all_complex_csi.append(complex_samples)\n",
    "        \n",
    "        if not all_complex_csi:\n",
    "            return False\n",
    "        \n",
    "        return save_processed_csi(all_complex_csi, output_path, num_subcarriers)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing CSI_Data column: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_complex_columns(df_raw, output_path, num_subcarriers, real_cols, imag_cols):\n",
    "    \"\"\"Process CSI data from separate real/imaginary columns.\"\"\"\n",
    "    try:\n",
    "        # Match real and imaginary columns\n",
    "        matched_pairs = []\n",
    "        for real_col in real_cols:\n",
    "            # Find corresponding imaginary column\n",
    "            base_name = real_col.replace('_real', '').replace('real', '')\n",
    "            imag_col = None\n",
    "            \n",
    "            for ic in imag_cols:\n",
    "                if base_name in ic or ic.replace('_imag', '').replace('imag', '') == base_name:\n",
    "                    imag_col = ic\n",
    "                    break\n",
    "            \n",
    "            if imag_col:\n",
    "                matched_pairs.append((real_col, imag_col))\n",
    "        \n",
    "        if not matched_pairs:\n",
    "            return False\n",
    "        \n",
    "        all_complex_csi = []\n",
    "        for _, row in df_raw.iterrows():\n",
    "            complex_samples = []\n",
    "            for real_col, imag_col in matched_pairs:\n",
    "                try:\n",
    "                    real_val = float(row[real_col])\n",
    "                    imag_val = float(row[imag_col])\n",
    "                    complex_samples.append(complex(real_val, imag_val))\n",
    "                except (ValueError, KeyError):\n",
    "                    continue\n",
    "            \n",
    "            if complex_samples:\n",
    "                all_complex_csi.append(complex_samples)\n",
    "        \n",
    "        if not all_complex_csi:\n",
    "            return False\n",
    "        \n",
    "        return save_processed_csi(all_complex_csi, output_path, num_subcarriers)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing complex columns: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_subcarrier_columns(df_raw, output_path, num_subcarriers, subcarrier_cols):\n",
    "    \"\"\"Process already processed subcarrier data.\"\"\"\n",
    "    try:\n",
    "        # If data is already in subcarrier format, just select top channels\n",
    "        data = df_raw[subcarrier_cols].values\n",
    "        \n",
    "        if data.shape[1] == 0:\n",
    "            return False\n",
    "        \n",
    "        # Select top subcarriers based on variance\n",
    "        variances = np.var(data, axis=0)\n",
    "        top_indices = np.argsort(variances)[-num_subcarriers:]\n",
    "        \n",
    "        selected_data = data[:, top_indices]\n",
    "        \n",
    "        # Save as processed format\n",
    "        df_processed = pd.DataFrame(\n",
    "            selected_data, \n",
    "            columns=[f'subcarrier_{i}' for i in range(len(top_indices))]\n",
    "        )\n",
    "        \n",
    "        df_processed.to_csv(output_path, index=False)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing subcarrier columns: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_processed_csi(complex_csi_data, output_path, num_subcarriers):\n",
    "    \"\"\"Convert complex CSI to amplitude/phase features and save.\"\"\"\n",
    "    try:\n",
    "        # Convert to numpy array and transpose (subcarriers x time)\n",
    "        csi_array = np.array(complex_csi_data).T\n",
    "        \n",
    "        # Extract amplitude and phase\n",
    "        amplitudes = np.abs(csi_array)\n",
    "        phases = np.angle(csi_array)\n",
    "        \n",
    "        # Select top subcarriers based on amplitude variance\n",
    "        amp_variances = np.var(amplitudes, axis=1)\n",
    "        if len(amp_variances) <= num_subcarriers:\n",
    "            top_indices = np.arange(len(amp_variances))\n",
    "        else:\n",
    "            top_indices = np.argsort(amp_variances)[-num_subcarriers:]\n",
    "        \n",
    "        selected_amplitudes = amplitudes[top_indices]\n",
    "        selected_phases = phases[top_indices]\n",
    "        \n",
    "        # Flatten amplitude variations (detrend)\n",
    "        from scipy.signal import detrend\n",
    "        flattened_amplitudes = np.array([detrend(amp) for amp in selected_amplitudes])\n",
    "        \n",
    "        # Unwrap phases\n",
    "        unwrapped_phases = np.array([np.unwrap(phase) for phase in selected_phases])\n",
    "        \n",
    "        # Create features dataframe\n",
    "        features = {}\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            features[f'subcarrier_{i}'] = flattened_amplitudes[i]\n",
    "            features[f'subcarrier_{i}_phase'] = unwrapped_phases[i]\n",
    "        \n",
    "        # For compatibility with existing code, use just amplitude features as main subcarriers\n",
    "        main_features = {f'subcarrier_{i}': flattened_amplitudes[i] for i in range(len(top_indices))}\n",
    "        \n",
    "        df_processed = pd.DataFrame(main_features)\n",
    "        df_processed.to_csv(output_path, index=False)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving processed CSI: {e}\")\n",
    "        return False\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION & LOADING (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "class LightweightCSIPreprocessor:\n",
    "    \"\"\"Extracts basic features from CSI data with improved error handling.\"\"\"\n",
    "    \n",
    "    def extract_basic_features(self, csi_data):\n",
    "        \"\"\"Extract energy and difference features from CSI data.\"\"\"\n",
    "        try:\n",
    "            if len(csi_data.shape) != 2:\n",
    "                logging.warning(f\"Unexpected CSI data shape: {csi_data.shape}\")\n",
    "                return np.zeros((2, csi_data.shape[-1]), dtype=np.float32)\n",
    "            \n",
    "            features = []\n",
    "            for ch_idx in range(csi_data.shape[0]):\n",
    "                ch_data = csi_data[ch_idx]\n",
    "                \n",
    "                # Energy variations\n",
    "                energy = np.abs(ch_data) ** 2\n",
    "                \n",
    "                # First-order differences\n",
    "                diff = np.diff(ch_data, prepend=ch_data[0] if len(ch_data) > 0 else 0)\n",
    "                \n",
    "                features.append(np.stack([energy, diff]))\n",
    "            \n",
    "            result = np.array(features, dtype=np.float32)\n",
    "            \n",
    "            # Reshape to (total_features, time_samples)\n",
    "            return result.reshape(-1, result.shape[-1])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Feature extraction error: {e}\")\n",
    "            # Return dummy features\n",
    "            dummy_size = csi_data.shape[-1] if len(csi_data.shape) > 0 else 1024\n",
    "            return np.zeros((HYPERPARAMETERS['csi_channels'] * 2, dummy_size), dtype=np.float32)\n",
    "\n",
    "class OptimizedCSIAudioDataset(Dataset):\n",
    "    \"\"\"Improved dataset with better error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df.reset_index(drop=True)\n",
    "        self.config = HYPERPARAMETERS\n",
    "        self.csi_preprocessor = LightweightCSIPreprocessor()\n",
    "        \n",
    "        # Validate files\n",
    "        self._validate_files()\n",
    "    \n",
    "    def _validate_files(self):\n",
    "        \"\"\"Validate that files exist and are readable.\"\"\"\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx in range(len(self.data_df)):\n",
    "            row = self.data_df.iloc[idx]\n",
    "            csi_path = row['csi_path']\n",
    "            audio_path = row['audio_path']\n",
    "            \n",
    "            try:\n",
    "                if os.path.exists(csi_path) and os.path.exists(audio_path):\n",
    "                    # Quick validation\n",
    "                    info = sf.info(audio_path)\n",
    "                    if info.duration >= 1.0:  # At least 1 second of audio\n",
    "                        valid_indices.append(idx)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        self.data_df = self.data_df.iloc[valid_indices].reset_index(drop=True)\n",
    "        logging.info(f\"Dataset validated: {len(self.data_df)} valid samples\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_df.iloc[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load CSI data\n",
    "            csi_df = pd.read_csv(row['csi_path'])\n",
    "            csi_cols = [col for col in csi_df.columns if col.startswith('subcarrier_') and not col.endswith('_phase')]\n",
    "            \n",
    "            if len(csi_cols) == 0:\n",
    "                return None\n",
    "            \n",
    "            # Take only the required number of channels\n",
    "            csi_cols = csi_cols[:self.config['csi_channels']]\n",
    "            csi_data = csi_df[csi_cols].values.T\n",
    "            \n",
    "            # Load audio data\n",
    "            audio, sr = sf.read(row['audio_path'], dtype='float32')\n",
    "            if audio.ndim > 1:\n",
    "                audio = audio.mean(axis=1)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if sr != self.config['sample_rate']:\n",
    "                audio = librosa.resample(\n",
    "                    audio, orig_sr=sr, target_sr=self.config['sample_rate'], res_type='kaiser_fast'\n",
    "                )\n",
    "            \n",
    "            # Align lengths\n",
    "            min_len = min(csi_data.shape[1], len(audio))\n",
    "            if min_len < self.config['chunk_size']:\n",
    "                return None\n",
    "            \n",
    "            csi_data = csi_data[:, :min_len]\n",
    "            audio = audio[:min_len]\n",
    "            \n",
    "            # Extract random chunk\n",
    "            max_start = min_len - self.config['chunk_size']\n",
    "            start_idx = random.randint(0, max_start) if max_start > 0 else 0\n",
    "            end_idx = start_idx + self.config['chunk_size']\n",
    "            \n",
    "            csi_chunk = csi_data[:, start_idx:end_idx]\n",
    "            audio_chunk = audio[start_idx:end_idx].copy()\n",
    "            \n",
    "            # Extract features\n",
    "            csi_features = self.csi_preprocessor.extract_basic_features(csi_chunk)\n",
    "            \n",
    "            # Normalize features\n",
    "            for i in range(csi_features.shape[0]):\n",
    "                feat = csi_features[i]\n",
    "                if np.std(feat) > 1e-8:\n",
    "                    csi_features[i] = (feat - np.mean(feat)) / np.std(feat)\n",
    "                csi_features[i] = np.clip(csi_features[i], -3, 3)\n",
    "            \n",
    "            # Normalize audio\n",
    "            if np.max(np.abs(audio_chunk)) > 0:\n",
    "                audio_chunk = audio_chunk / np.max(np.abs(audio_chunk)) * 0.8\n",
    "            \n",
    "            return torch.from_numpy(csi_features).float(), torch.from_numpy(audio_chunk).float()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error loading sample {idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE (FIXED DIMENSIONS)\n",
    "# ============================================================================\n",
    "\n",
    "class OptimizedGenerator(nn.Module):\n",
    "    \"\"\"Fixed Generator with proper dimension handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate input dimensions properly\n",
    "        self.latent_dim = config['latent_dim']\n",
    "        self.csi_channels = config['csi_channels']\n",
    "        self.chunk_size = config['chunk_size']\n",
    "        \n",
    "        # CSI features: csi_channels * 2 features per channel * chunk_size\n",
    "        csi_input_dim = self.csi_channels * 2 * self.chunk_size\n",
    "        total_input_dim = self.latent_dim + csi_input_dim\n",
    "        \n",
    "        logging.info(f\"Generator input dimensions: latent={self.latent_dim}, csi={csi_input_dim}, total={total_input_dim}\")\n",
    "        \n",
    "        # Fixed FC layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(total_input_dim, 2048),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(1024, 64 * 32),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 48, 4, 2, 1),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.ConvTranspose1d(48, 32, 4, 2, 1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.ConvTranspose1d(32, 16, 4, 2, 1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.ConvTranspose1d(16, 1, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(self.chunk_size)\n",
    "\n",
    "    def forward(self, noise, csi):\n",
    "        batch_size = noise.size(0)\n",
    "        \n",
    "        # Flatten CSI features\n",
    "        csi_flat = csi.view(batch_size, -1)\n",
    "        \n",
    "        # Concatenate noise and CSI\n",
    "        x = torch.cat([noise, csi_flat], dim=1)\n",
    "        \n",
    "        # Process through FC layers\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Reshape for conv layers\n",
    "        x = x.view(batch_size, 64, 32)\n",
    "        \n",
    "        # Process through conv layers\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Adaptive pooling to ensure correct output size\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x.squeeze(1)\n",
    "\n",
    "class OptimizedDiscriminator(nn.Module):\n",
    "    \"\"\"Fixed Discriminator with spectral normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers with spectral normalization\n",
    "        self.convs = nn.ModuleList([\n",
    "            spectral_norm(nn.Conv1d(1, 32, 5, 2, 2)),\n",
    "            spectral_norm(nn.Conv1d(32, 64, 5, 2, 2)),\n",
    "            spectral_norm(nn.Conv1d(64, 128, 5, 2, 2)),\n",
    "            spectral_norm(nn.Conv1d(128, 256, 5, 2, 2)),\n",
    "        ])\n",
    "        \n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.BatchNorm1d(256),\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = spectral_norm(nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, audio, csi=None, return_features=False):\n",
    "        x = audio.unsqueeze(1) if len(audio.shape) == 2 else audio\n",
    "        features = []\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x)\n",
    "            \n",
    "            if i > 0:  # Skip batch norm for first layer\n",
    "                x = self.batch_norms[i-1](x)\n",
    "            \n",
    "            x = nn.LeakyReLU(0.2)(x)\n",
    "            \n",
    "            if i >= 1:  # Add dropout for later layers\n",
    "                x = self.dropout(x)\n",
    "            \n",
    "            if return_features:\n",
    "                features.append(x.clone())\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = torch.mean(x, dim=2)\n",
    "        output = self.fc(x)\n",
    "        \n",
    "        return (output, features) if return_features else output\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS FUNCTIONS & TRAINING (WITH COSINE SIMILARITY TRACKING)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_cosine_similarity(real_audio, generated_audio):\n",
    "    \"\"\"Compute cosine similarity between real and generated audio batches.\"\"\"\n",
    "    try:\n",
    "        real_flat = real_audio.detach().cpu().numpy().flatten().reshape(1, -1)\n",
    "        gen_flat = generated_audio.detach().cpu().numpy().flatten().reshape(1, -1)\n",
    "        \n",
    "        # Normalize to avoid division by zero\n",
    "        real_norm = np.linalg.norm(real_flat)\n",
    "        gen_norm = np.linalg.norm(gen_flat)\n",
    "        \n",
    "        if real_norm == 0 or gen_norm == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        similarity = cosine_similarity(real_flat, gen_flat)[0, 0]\n",
    "        return float(similarity)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Cosine similarity computation failed: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def feature_matching_loss(real_features, fake_features):\n",
    "    \"\"\"Compute feature matching loss.\"\"\"\n",
    "    loss = 0\n",
    "    try:\n",
    "        for real_feat, fake_feat in zip(real_features, fake_features):\n",
    "            loss += nn.functional.l1_loss(fake_feat.mean(0), real_feat.mean(0))\n",
    "        return loss\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Feature matching loss computation failed: {e}\")\n",
    "        return torch.tensor(0.0, device=real_features[0].device if real_features else 'cpu')\n",
    "\n",
    "def gradient_penalty(critic, real_data, fake_data, device):\n",
    "    \"\"\"Compute gradient penalty for WGAN-GP.\"\"\"\n",
    "    try:\n",
    "        batch_size = real_data.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, device=device)\n",
    "        \n",
    "        # Ensure same dimensions\n",
    "        if len(real_data.shape) != len(fake_data.shape):\n",
    "            fake_data = fake_data.view_as(real_data)\n",
    "        \n",
    "        interpolated = (alpha * real_data + (1 - alpha) * fake_data).requires_grad_(True)\n",
    "        \n",
    "        d_interpolated = critic(interpolated)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_interpolated),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return penalty\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Gradient penalty computation failed: {e}\")\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "def train_gan(gen, disc, loader, config):\n",
    "    \"\"\"Main training loop with cosine similarity tracking.\"\"\"\n",
    "    \n",
    "    # Optimizers\n",
    "    opt_g = optim.Adam(gen.parameters(), lr=config['lr_generator'], betas=(config['beta1'], config['beta2']))\n",
    "    opt_d = optim.Adam(disc.parameters(), lr=config['lr_discriminator'], betas=(config['beta1'], config['beta2']))\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(config['use_mixed_precision'] and config['device'].type == 'cuda'))\n",
    "    \n",
    "    # Loss and similarity tracking\n",
    "    g_losses, d_losses, cosine_similarities = [], [], []\n",
    "    \n",
    "    gen.train()\n",
    "    disc.train()\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        epoch_g_loss, epoch_d_loss, epoch_cosine_sim = 0.0, 0.0, 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        for batch_data in progress_bar:\n",
    "            if batch_data is None or batch_data[0] is None or batch_data[1] is None:\n",
    "                continue\n",
    "            \n",
    "            csi_batch, audio_batch = batch_data\n",
    "            if csi_batch.size(0) == 0:\n",
    "                continue\n",
    "            \n",
    "            csi_batch = csi_batch.to(config['device'])\n",
    "            audio_batch = audio_batch.to(config['device'])\n",
    "            batch_size = audio_batch.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(config['n_critic']):\n",
    "                opt_d.zero_grad()\n",
    "                \n",
    "                # Generate noise\n",
    "                noise = torch.randn(batch_size, config['latent_dim'], device=config['device'])\n",
    "                \n",
    "                with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
    "                    # Generate fake audio\n",
    "                    fake_audio = gen(noise, csi_batch).detach()\n",
    "                    \n",
    "                    # Discriminator outputs\n",
    "                    real_pred = disc(audio_batch)\n",
    "                    fake_pred = disc(fake_audio)\n",
    "                    \n",
    "                    # WGAN-GP loss\n",
    "                    gp = gradient_penalty(disc, audio_batch, fake_audio, config['device'])\n",
    "                    d_loss = torch.mean(fake_pred) - torch.mean(real_pred) + config['lambda_gradient_penalty'] * gp\n",
    "                \n",
    "                # Backward and step\n",
    "                scaler.scale(d_loss).backward()\n",
    "                scaler.step(opt_d)\n",
    "                scaler.update()\n",
    "            \n",
    "            # Train Generator\n",
    "            opt_g.zero_grad()\n",
    "            \n",
    "            noise = torch.randn(batch_size, config['latent_dim'], device=config['device'])\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
    "                # Generate fake audio\n",
    "                gen_audio = gen(noise, csi_batch)\n",
    "                \n",
    "                # Discriminator predictions with features\n",
    "                gen_pred, fake_features = disc(gen_audio, return_features=True)\n",
    "                _, real_features = disc(audio_batch, return_features=True)\n",
    "                \n",
    "                # Generator losses\n",
    "                g_loss_adv = -torch.mean(gen_pred)\n",
    "                g_loss_fm = feature_matching_loss(real_features, fake_features) * config['lambda_feature_matching']\n",
    "                g_loss = g_loss_adv + g_loss_fm\n",
    "            \n",
    "            # Backward and step\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(opt_g)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            cosine_sim = compute_cosine_similarity(audio_batch, gen_audio)\n",
    "            \n",
    "            # Accumulate losses\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_cosine_sim += cosine_sim\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'D_loss': f'{d_loss.item():.4f}',\n",
    "                'G_loss': f'{g_loss.item():.4f}',\n",
    "                'Cosine': f'{cosine_sim:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if num_batches % 20 == 0:\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Record epoch averages\n",
    "        if num_batches > 0:\n",
    "            avg_d_loss = epoch_d_loss / num_batches\n",
    "            avg_g_loss = epoch_g_loss / num_batches\n",
    "            avg_cosine_sim = epoch_cosine_sim / num_batches\n",
    "            \n",
    "            d_losses.append(avg_d_loss)\n",
    "            g_losses.append(avg_g_loss)\n",
    "            cosine_similarities.append(avg_cosine_sim)\n",
    "            \n",
    "            logging.info(f\"Epoch {epoch+1}: D_loss={avg_d_loss:.4f}, G_loss={avg_g_loss:.4f}, Cosine_Sim={avg_cosine_sim:.4f}\")\n",
    "    \n",
    "    return g_losses, d_losses, cosine_similarities\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED PLOTTING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_metrics(g_losses, d_losses, cosine_similarities, save_path):\n",
    "    \"\"\"Plot training losses and cosine similarities.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    epochs = range(1, len(g_losses) + 1)\n",
    "    ax1.plot(epochs, g_losses, label='Generator Loss', color='blue', linewidth=2)\n",
    "    ax1.plot(epochs, d_losses, label='Discriminator Loss', color='red', linewidth=2)\n",
    "    ax1.set_title('Training Losses Over Time', fontsize=16)\n",
    "    ax1.set_xlabel('Epochs', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot cosine similarities\n",
    "    ax2.plot(epochs, cosine_similarities, label='Cosine Similarity', color='green', linewidth=2)\n",
    "    ax2.set_title('Cosine Similarity Between Real and Generated Audio', fontsize=16)\n",
    "    ax2.set_xlabel('Epochs', fontsize=12)\n",
    "    ax2.set_ylabel('Cosine Similarity', fontsize=12)\n",
    "    ax2.set_ylim(-1, 1)\n",
    "    ax2.legend(fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logging.info(f\"Training metrics plot saved to {save_path}\")\n",
    "\n",
    "def plot_audio_comparison(generator, dataloader, device, save_path, sample_rate, num_examples=3):\n",
    "    \"\"\"Generate side-by-side comparison of real vs. generated audio.\"\"\"\n",
    "    \n",
    "    generator.eval()\n",
    "    \n",
    "    try:\n",
    "        # Get a batch of data\n",
    "        for batch_data in dataloader:\n",
    "            if batch_data is not None and batch_data[0] is not None:\n",
    "                csi_batch, real_audio_batch = batch_data\n",
    "                break\n",
    "        else:\n",
    "            logging.warning(\"No valid data found for audio comparison\")\n",
    "            return\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not load data for comparison: {e}\")\n",
    "        return\n",
    "    \n",
    "    csi_batch = csi_batch.to(device)\n",
    "    batch_size = min(num_examples, csi_batch.size(0))\n",
    "    \n",
    "    # Generate audio\n",
    "    noise = torch.randn(batch_size, HYPERPARAMETERS['latent_dim'], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_audio_batch = generator(noise, csi_batch[:batch_size]).cpu().numpy()\n",
    "    \n",
    "    real_audio_batch = real_audio_batch[:batch_size].numpy()\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(batch_size * 2, 2, figsize=(16, 4 * batch_size))\n",
    "    if batch_size == 1:\n",
    "        axes = axes.reshape(2, 2)\n",
    "    \n",
    "    fig.suptitle(\"Audio Generation Comparison (Real vs. Generated)\", fontsize=20)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        row_offset = i * 2\n",
    "        \n",
    "        # Waveform comparison\n",
    "        real_w_ax = axes[row_offset, 0]\n",
    "        gen_w_ax = axes[row_offset, 1]\n",
    "        \n",
    "        time_axis = np.linspace(0, len(real_audio_batch[i]) / sample_rate, len(real_audio_batch[i]))\n",
    "        \n",
    "        real_w_ax.plot(time_axis, real_audio_batch[i], color='blue', alpha=0.8)\n",
    "        real_w_ax.set_title(f\"Example {i+1}: Real Audio Waveform\")\n",
    "        real_w_ax.set_xlabel(\"Time (s)\")\n",
    "        real_w_ax.set_ylabel(\"Amplitude\")\n",
    "        real_w_ax.set_ylim(-1, 1)\n",
    "        real_w_ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        gen_w_ax.plot(time_axis, generated_audio_batch[i], color='red', alpha=0.8)\n",
    "        gen_w_ax.set_title(f\"Example {i+1}: Generated Audio Waveform\")\n",
    "        gen_w_ax.set_xlabel(\"Time (s)\")\n",
    "        gen_w_ax.set_ylabel(\"Amplitude\")\n",
    "        gen_w_ax.set_ylim(-1, 1)\n",
    "        gen_w_ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Spectrogram comparison\n",
    "        real_s_ax = axes[row_offset + 1, 0]\n",
    "        gen_s_ax = axes[row_offset + 1, 1]\n",
    "        \n",
    "        # Compute spectrograms\n",
    "        D_real = librosa.amplitude_to_db(np.abs(librosa.stft(real_audio_batch[i])), ref=np.max)\n",
    "        D_gen = librosa.amplitude_to_db(np.abs(librosa.stft(generated_audio_batch[i])), ref=np.max)\n",
    "        \n",
    "        librosa.display.specshow(D_real, sr=sample_rate, x_axis='time', y_axis='log', ax=real_s_ax)\n",
    "        real_s_ax.set_title(f\"Example {i+1}: Real Audio Spectrogram\")\n",
    "        \n",
    "        librosa.display.specshow(D_gen, sr=sample_rate, x_axis='time', y_axis='log', ax=gen_s_ax)\n",
    "        gen_s_ax.set_title(f\"Example {i+1}: Generated Audio Spectrogram\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logging.info(f\"Audio comparison plot saved to {save_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    \"\"\"Extract timestamp from filename with improved pattern matching.\"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "    \n",
    "    patterns = [\n",
    "        r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\\.\\d+)',\n",
    "        r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})',\n",
    "        r'(\\d{4}\\d{2}\\d{2}_\\d{2}\\d{2}\\d{2})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Improved collate function with better error handling.\"\"\"\n",
    "    # Filter out None items\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        csi_data, audio_data = zip(*batch)\n",
    "        csi_batch = torch.stack(csi_data)\n",
    "        audio_batch = torch.stack(audio_data)\n",
    "        return csi_batch, audio_batch\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Collate function error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with comprehensive error handling.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"=\" * 80)\n",
    "        logging.info(\"WIFI CSI-TO-AUDIO GENERATION - FIXED VERSION\")\n",
    "        logging.info(\"=\" * 80)\n",
    "        \n",
    "        # Step 1: Process raw CSI data\n",
    "        logging.info(\"\\nSTEP 1: Processing Raw CSI Data\")\n",
    "        logging.info(\"-\" * 40)\n",
    "        \n",
    "        if os.path.exists(RAW_CSI_DIRECTORY):\n",
    "            process_raw_csi_data(RAW_CSI_DIRECTORY, PROCESSED_CSI_DIRECTORY, HYPERPARAMETERS['csi_channels'])\n",
    "        else:\n",
    "            logging.warning(f\"Raw CSI directory not found: {RAW_CSI_DIRECTORY}\")\n",
    "            logging.info(\"Assuming processed CSI files already exist...\")\n",
    "        \n",
    "        # Step 2: Determine audio parameters\n",
    "        logging.info(\"\\nSTEP 2: Determining Audio Parameters\")\n",
    "        logging.info(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            audio_files = [f for f in os.listdir(AUDIO_DIRECTORY) if f.endswith('.wav')]\n",
    "            if not audio_files:\n",
    "                raise FileNotFoundError(f\"No .wav files found in {AUDIO_DIRECTORY}\")\n",
    "            \n",
    "            first_audio = audio_files[0]\n",
    "            info = sf.info(os.path.join(AUDIO_DIRECTORY, first_audio))\n",
    "            \n",
    "            HYPERPARAMETERS['sample_rate'] = info.samplerate\n",
    "            HYPERPARAMETERS['sequence_length'] = int(info.samplerate * HYPERPARAMETERS['audio_duration'])\n",
    "            \n",
    "            logging.info(f\"Sample rate: {info.samplerate} Hz\")\n",
    "            logging.info(f\"Sequence length: {HYPERPARAMETERS['sequence_length']} samples\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to determine audio parameters: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Step 3: Match CSI and audio files\n",
    "        logging.info(\"\\nSTEP 3: Matching CSI and Audio Files\")\n",
    "        logging.info(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Get processed CSI files\n",
    "            csi_files = [f for f in os.listdir(PROCESSED_CSI_DIRECTORY) if f.endswith('.csv')]\n",
    "            audio_files = [f for f in os.listdir(AUDIO_DIRECTORY) if f.endswith('.wav')]\n",
    "            \n",
    "            logging.info(f\"Found {len(csi_files)} CSI files and {len(audio_files)} audio files\")\n",
    "            \n",
    "            # Extract timestamps and match\n",
    "            csi_map = {extract_timestamp(f): f for f in csi_files}\n",
    "            audio_map = {extract_timestamp(f): f for f in audio_files}\n",
    "            \n",
    "            # Remove None keys\n",
    "            csi_map = {k: v for k, v in csi_map.items() if k is not None}\n",
    "            audio_map = {k: v for k, v in audio_map.items() if k is not None}\n",
    "            \n",
    "            common_timestamps = sorted(set(csi_map.keys()) & set(audio_map.keys()))\n",
    "            \n",
    "            logging.info(f\"Found {len(common_timestamps)} matching timestamp pairs\")\n",
    "            \n",
    "            if len(common_timestamps) == 0:\n",
    "                raise ValueError(\"No matching CSI-audio pairs found. Check file naming and timestamps.\")\n",
    "            \n",
    "            # Create data pairs\n",
    "            data_pairs = []\n",
    "            for ts in common_timestamps:\n",
    "                data_pairs.append({\n",
    "                    'csi_path': os.path.join(PROCESSED_CSI_DIRECTORY, csi_map[ts]),\n",
    "                    'audio_path': os.path.join(AUDIO_DIRECTORY, audio_map[ts])\n",
    "                })\n",
    "            \n",
    "            pairs_df = pd.DataFrame(data_pairs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to match files: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Step 4: Split data\n",
    "        logging.info(\"\\nSTEP 4: Splitting Data\")\n",
    "        logging.info(\"-\" * 40)\n",
    "        \n",
    "        train_df, test_df = train_test_split(\n",
    "            pairs_df,\n",
    "            test_size=1 - HYPERPARAMETERS['train_test_split'],\n",
    "            random_state=HYPERPARAMETERS['random_seed']\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Training samples: {len(train_df)}\")\n",
    "        logging.info(f\"Test samples: {len(test_df)}\")\n",
    "        \n",
    "        # Step 5: Create datasets and loaders\n",
    "        logging.info(\"\\nSTEP 5: Creating Datasets\")\n",
    "        logging.info(\"-\" * 40)\n",
    "        \n",
    "        train_dataset = OptimizedCSIAudioDataset(train_df)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=HYPERPARAMETERS['batch_size'],\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Create test loader for plots\n",
    "        test_dataset = OptimizedCSIAudioDataset(test_df if not test_df.empty else train_df)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=HYPERPARAMETERS['batch_size'],\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Step 6: Initialize models\n",
    "        logging.info(\"\\nSTEP 6: Initializing Models\")\n",
    "        logging.info(\"-\" * 40)\n",
    "        \n",
    "        generator = OptimizedGenerator(HYPERPARAMETERS).to(HYPERPARAMETERS['device'])\n",
    "        discriminator = OptimizedDiscriminator().to(HYPERPARAMETERS['device'])\n",
    "        \n",
    "        # Log model parameters\n",
    "        total_g_params = sum(p.numel() for p in generator.parameters())\n",
    "        total_d_params = sum(p.numel() for p in discriminator.parameters())\n",
    "        \n",
    "        logging.info(f\"Generator parameters: {total_g_params:,}\")\n",
    "        logging.info(f\"Discriminator parameters: {total_d_params:,}\")\n",
    "        \n",
    "        # Step 7: Train models\n",
    "        logging.info(\"\\nSTEP 7: Starting Training\")\n",
    "        logging.info(\"-\" * 40)\n",
    "        \n",
    "        g_losses, d_losses, cosine_similarities = train_gan(\n",
    "            generator, discriminator, train_loader, HYPERPARAMETERS\n",
    "        )\n",
    "        \n",
    "        # Step 8: Save results\n",
    "        logging.info(\"\\nSTEP 8: Saving Results\\n\" + \"-\"*40)\n",
    "        os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "        # Save models\n",
    "        torch.save(generator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \"generator.pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(OUTPUT_DIRECTORY, \"discriminator.pth\"))\n",
    "\n",
    "        # Save training history always (even if empty)\n",
    "        history_df = pd.DataFrame({\n",
    "            'epoch': list(range(1, len(g_losses) + 1)),\n",
    "            'generator_loss': g_losses,\n",
    "            'discriminator_loss': d_losses,\n",
    "            'cosine_similarity': cosine_similarities\n",
    "        })\n",
    "        history_csv = os.path.join(OUTPUT_DIRECTORY, 'training_history.csv')\n",
    "        history_df.to_csv(history_csv, index=False)\n",
    "        logging.info(f\"Training history saved to {history_csv} ({len(history_df)} rows)\")\n",
    "\n",
    "        # Plot training metrics (safe-guard empty lists)\n",
    "        metrics_plot = os.path.join(OUTPUT_DIRECTORY, \"training_metrics.png\")\n",
    "        try:\n",
    "            plot_training_metrics(g_losses, d_losses, cosine_similarities, metrics_plot)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to plot training metrics: {e}\")\n",
    "\n",
    "        # Plot audio comparison (safe-guard)\n",
    "        audio_plot = os.path.join(OUTPUT_DIRECTORY, \"audio_comparison.png\")\n",
    "        try:\n",
    "            plot_audio_comparison(generator, test_loader, HYPERPARAMETERS['device'],\n",
    "                                  audio_plot, HYPERPARAMETERS['sample_rate'])\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to plot audio comparison: {e}\")\n",
    "\n",
    "        logging.info(\"Results saving complete.\")\n",
    "\n",
    "        \n",
    "        # Generate plots\n",
    "        if g_losses and d_losses and cosine_similarities:\n",
    "            plot_training_metrics(\n",
    "                g_losses, d_losses, cosine_similarities,\n",
    "                os.path.join(OUTPUT_DIRECTORY, \"training_metrics.png\")\n",
    "            )\n",
    "            \n",
    "            plot_audio_comparison(\n",
    "                generator, test_loader, HYPERPARAMETERS['device'],\n",
    "                os.path.join(OUTPUT_DIRECTORY, \"audio_comparison.png\"),\n",
    "                HYPERPARAMETERS['sample_rate']\n",
    "            )\n",
    "        \n",
    "        logging.info(\"\\n\" + \"=\" * 80)\n",
    "        logging.info(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        logging.info(f\"Results saved to: {OUTPUT_DIRECTORY}\")\n",
    "        logging.info(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Critical error in main execution: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 10:51:59,570 - INFO - Selected CSI file: data/processed_csi\\top8_filtered_csi_data_csi_data_2025-05-07_20-19-42.459.csv\n",
      "2025-09-12 10:51:59,572 - INFO - Selected Audio file: data/audio\\2025-05-07_20-19-42.459.wav\n",
      "2025-09-12 10:51:59,580 - INFO - Sample rate: 44100, Sequence length: 441000\n",
      "2025-09-12 10:51:59,750 - INFO - Generator initialized - total input per chunk: 32816\n",
      "2025-09-12 10:51:59,918 - INFO - Loaded checkpoint from output/generator.pth\n",
      "2025-09-12 10:52:16,055 - INFO - All plots and audio files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import scipy.signal as signal\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ===========================\n",
    "# Config and Hyperparameters\n",
    "# ===========================\n",
    "TARGET_TIMESTAMP = \"2025-05-07_20-19-42.459\"\n",
    "\n",
    "HYPERPARAMETERS = {\n",
    "    \"latent_dim\": 48,\n",
    "    \"csi_channels\": 8,\n",
    "    \"sample_rate\": None,\n",
    "    \"audio_channels\": 1,\n",
    "    \"audio_duration\": 10.0,\n",
    "    \"chunk_size\": 2048,\n",
    "    \"overlap\": 256,\n",
    "    \"drum_enhancement_factor\": 2.0,\n",
    "    \"drum_attack_enhancement\": 1.2,\n",
    "    \"drum_sustain_factor\": 1.5,\n",
    "    \"lowpass_cutoff\": 12000,\n",
    "    \"drum_freq_range\": [40, 300],\n",
    "    \"kick_freq_range\": [30, 120],\n",
    "    \"snare_freq_range\": [120, 400],\n",
    "    \"cymbal_freq_range\": [1000, 12000],\n",
    "    \"enable_noise_reduction\": True,\n",
    "    \"preserve_drums\": True,\n",
    "    \"apply_lowpass_filter\": True,\n",
    "    \"apply_dynamic_range_compression\": True,\n",
    "    \"noise_floor_threshold\": -50,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "# ===========================\n",
    "# Utility Functions\n",
    "# ===========================\n",
    "def extract_timestamp(filename):\n",
    "    m = re.search(r\"(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\\.\\d+)\", filename)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def determine_sample_rate_length(audio_dir):\n",
    "    wavs = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
    "    if not wavs:\n",
    "        raise FileNotFoundError(\"No WAV files found in audio directory\")\n",
    "    path = os.path.join(audio_dir, wavs[0])\n",
    "    info = sf.info(path)\n",
    "    sr = info.samplerate\n",
    "    duration_samples = int(sr * HYPERPARAMETERS['audio_duration'])\n",
    "    HYPERPARAMETERS['sample_rate'] = sr\n",
    "    HYPERPARAMETERS['sequence_length'] = duration_samples\n",
    "    logging.info(f\"Sample rate: {sr}, Sequence length: {duration_samples}\")\n",
    "    return sr, duration_samples\n",
    "\n",
    "def find_matching_files(timestamp):\n",
    "    csi_dir = \"data/processed_csi\"\n",
    "    audio_dir = \"data/audio\"\n",
    "    csi_files = [f for f in os.listdir(csi_dir) if f.endswith(\".csv\")]\n",
    "    audio_files = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
    "    \n",
    "    csi_map = {extract_timestamp(f): f for f in csi_files}\n",
    "    audio_map = {extract_timestamp(f): f for f in audio_files}\n",
    "    \n",
    "    if timestamp not in csi_map or timestamp not in audio_map:\n",
    "        raise FileNotFoundError(f\"Timestamp {timestamp} not found in CSI or Audio data\")\n",
    "    \n",
    "    csi_path = os.path.join(csi_dir, csi_map[timestamp])\n",
    "    audio_path = os.path.join(audio_dir, audio_map[timestamp])\n",
    "    logging.info(f\"Selected CSI file: {csi_path}\")\n",
    "    logging.info(f\"Selected Audio file: {audio_path}\")\n",
    "    return csi_path, audio_path\n",
    "\n",
    "def load_and_align_data(csi_path, audio_path):\n",
    "    df = pd.read_csv(csi_path)\n",
    "    csi_cols = [col for col in df.columns if col.startswith(\"subcarrier_\")]\n",
    "    \n",
    "    if not csi_cols:\n",
    "        raise ValueError(f\"No columns with prefix 'subcarrier_' found in CSI file: {csi_path}\")\n",
    "\n",
    "    csi_data = df[csi_cols].values.T[:HYPERPARAMETERS['csi_channels']]\n",
    "    \n",
    "    audio, orig_sr = sf.read(audio_path)\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "    \n",
    "    target_sr = HYPERPARAMETERS['sample_rate']\n",
    "    if orig_sr != target_sr:\n",
    "        audio = librosa.resample(y=audio, orig_sr=orig_sr, target_sr=target_sr)\n",
    "    \n",
    "    seq_len = HYPERPARAMETERS['sequence_length']\n",
    "    if len(audio) > seq_len:\n",
    "        audio = audio[:seq_len]\n",
    "    else:\n",
    "        audio = np.pad(audio, (0, seq_len - len(audio)))\n",
    "    \n",
    "    aligned_csi = np.array([\n",
    "        np.interp(np.linspace(0, len(channel)-1, seq_len), np.arange(len(channel)), channel)\n",
    "        for channel in csi_data\n",
    "    ])\n",
    "    \n",
    "    return torch.tensor(aligned_csi).float().unsqueeze(0).to(HYPERPARAMETERS['device']), audio\n",
    "\n",
    "def normalize_csi(csi_tensor):\n",
    "    csi = csi_tensor.clone()\n",
    "    for i in range(csi.shape[1]):\n",
    "        ch = csi[0, i]\n",
    "        if ch.std() > 1e-8:\n",
    "            ch = (ch - ch.mean()) / ch.std()\n",
    "        csi[0, i] = ch.clamp(-3, 3)\n",
    "    return csi\n",
    "\n",
    "def extract_basic_features(csi_np):\n",
    "    features = []\n",
    "    for ch in csi_np:\n",
    "        energy = np.abs(ch) ** 2\n",
    "        diff = np.diff(ch, prepend=ch[0])\n",
    "        features.append(np.stack([energy, diff]))\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED DRUM PROCESSOR CLASS\n",
    "# ============================================================================\n",
    "class FixedDrumProcessor:\n",
    "    def __init__(self, sample_rate):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.nyquist = sample_rate // 2\n",
    "        \n",
    "    def remove_static(self, audio):\n",
    "        try:\n",
    "            audio_length = len(audio)\n",
    "            \n",
    "            if audio_length < 1024: frame_size, hop_size = 512, 128\n",
    "            elif audio_length < 4096: frame_size, hop_size = 1024, 256\n",
    "            else: frame_size, hop_size = 2048, 512\n",
    "            \n",
    "            frame_size = min(frame_size, audio_length)\n",
    "            hop_size = min(hop_size, frame_size // 2)\n",
    "            \n",
    "            if audio_length <= frame_size: return self.simple_denoise(audio)\n",
    "            \n",
    "            _, _, stft = signal.stft(audio, fs=self.sample_rate, nperseg=frame_size, noverlap=frame_size-hop_size)\n",
    "            magnitude, phase = np.abs(stft), np.angle(stft)\n",
    "            noise_profile = np.percentile(magnitude, 10, axis=1, keepdims=True)\n",
    "            mask = np.where(magnitude / (noise_profile + 1e-12) > 3.0, 1.0, 0.1)\n",
    "            \n",
    "            clean_stft = magnitude * mask * np.exp(1j * phase)\n",
    "            _, clean_audio = signal.istft(clean_stft, fs=self.sample_rate, nperseg=frame_size, noverlap=frame_size-hop_size)\n",
    "            \n",
    "            return clean_audio[:len(audio)]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Static removal failed: {e}\")\n",
    "            return audio\n",
    "\n",
    "    def simple_denoise(self, audio):\n",
    "        return audio\n",
    "\n",
    "    def detect_drums_fixed(self, audio):\n",
    "        try:\n",
    "            onset_env = librosa.onset.onset_strength(y=audio, sr=self.sample_rate, hop_length=512)\n",
    "            onsets = librosa.onset.onset_detect(onset_envelope=onset_env, sr=self.sample_rate, hop_length=512, units='time')\n",
    "            return onset_env, onsets\n",
    "        except Exception:\n",
    "            return np.zeros_like(audio), np.array([])\n",
    "\n",
    "    def enhance_drums(self, audio):\n",
    "        try:\n",
    "            _, onsets = self.detect_drums_fixed(audio)\n",
    "            return self.enhance_envelopes_safe(audio, onsets), onsets\n",
    "        except:\n",
    "            return audio, []\n",
    "\n",
    "    def enhance_envelopes_safe(self, audio, drum_positions):\n",
    "        enhanced_audio = audio.copy()\n",
    "        if len(drum_positions) == 0: return enhanced_audio\n",
    "        \n",
    "        drum_samples = (drum_positions * self.sample_rate).astype(int)\n",
    "        for ds in drum_samples[drum_samples < len(audio)]:\n",
    "            attack_s = int(0.01 * self.sample_rate)\n",
    "            env_len = min(attack_s, len(audio) - ds)\n",
    "            if env_len > 0:\n",
    "                envelope = np.linspace(1, HYPERPARAMETERS['drum_attack_enhancement'], env_len)\n",
    "                enhanced_audio[ds:ds+env_len] *= envelope\n",
    "        return enhanced_audio\n",
    "    \n",
    "    def process_drums(self, audio):\n",
    "        clean_audio = self.remove_static(audio)\n",
    "        enhanced_audio, onsets = self.enhance_drums(clean_audio)\n",
    "        if np.max(np.abs(enhanced_audio)) > 1e-8:\n",
    "            enhanced_audio /= np.max(np.abs(enhanced_audio))\n",
    "        return enhanced_audio, onsets\n",
    "\n",
    "# ============================================================================\n",
    "# POST PROCESS DRUM RICH FUNCTION\n",
    "# ============================================================================\n",
    "def post_process_drum_rich(audio, onsets, sample_rate, drum_type='kick', gain=2.0):\n",
    "    rich = audio.copy()\n",
    "    n = len(audio)\n",
    "    transient_duration = 0.05\n",
    "    total = int(transient_duration * sample_rate)\n",
    "    t = np.linspace(0, transient_duration, total, endpoint=False)\n",
    "    \n",
    "    if drum_type == 'kick':\n",
    "        transient = np.sin(2 * np.pi * 60 * t) * np.exp(-t * 50)\n",
    "    else: # Snare\n",
    "        noise = np.random.randn(total)\n",
    "        sos = signal.butter(4, [120, 400], btype='band', fs=sample_rate, output='sos')\n",
    "        transient = signal.sosfilt(sos, noise) * np.exp(-t * 30)\n",
    "\n",
    "    if np.max(np.abs(transient)) > 1e-8:\n",
    "        transient /= np.max(np.abs(transient))\n",
    "    transient *= gain\n",
    "\n",
    "    for onset in onsets:\n",
    "        idx = int(onset * sample_rate)\n",
    "        end = min(n, idx + total)\n",
    "        if idx < n:\n",
    "            rich[idx:end] += transient[:end-idx]\n",
    "\n",
    "    if np.max(np.abs(rich)) > 1e-8:\n",
    "        rich /= np.max(np.abs(rich))\n",
    "    return rich\n",
    "\n",
    "# ===========================\n",
    "# Generator Model Definition (FIXED)\n",
    "# ===========================\n",
    "class LightweightResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.1, inplace=True)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.leaky_relu(x + residual, 0.1, inplace=True)\n",
    "\n",
    "class OptimizedGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, csi_channels):\n",
    "        super().__init__()\n",
    "        self.csi_input_dim = csi_channels * 2 * HYPERPARAMETERS['chunk_size']\n",
    "        self.total_input_dim = latent_dim + self.csi_input_dim\n",
    "        logging.info(f\"Generator initialized - total input per chunk: {self.total_input_dim}\")\n",
    "        \n",
    "        # FIX: Revert to separate fc layers to match the trained model's architecture\n",
    "        self.fc1 = nn.Linear(self.total_input_dim, 256, bias=False)\n",
    "        self.fc2 = nn.Linear(256, 128, bias=False)\n",
    "        self.fc3 = nn.Linear(128, 64 * 32, bias=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 48, 4, 2, 1, bias=False), nn.BatchNorm1d(48), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.ConvTranspose1d(48, 32, 4, 2, 1, bias=False), nn.BatchNorm1d(32), nn.LeakyReLU(0.1, inplace=True),\n",
    "            LightweightResidualBlock(32),\n",
    "            nn.ConvTranspose1d(32, 16, 4, 2, 1, bias=False), nn.BatchNorm1d(16), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 1, 4, 2, 1), nn.Tanh()\n",
    "        )\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(HYPERPARAMETERS['chunk_size'])\n",
    "\n",
    "    def forward(self, noise, csi_data):\n",
    "        csi_flat = csi_data.view(noise.size(0), -1)\n",
    "        x = torch.cat([noise, csi_flat], dim=1)\n",
    "        \n",
    "        # FIX: Update forward pass to use the separate fc layers\n",
    "        x = F.leaky_relu(self.fc1(x), 0.1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.1)\n",
    "        \n",
    "        x = x.view(noise.size(0), 64, 32)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x.view(noise.size(0), HYPERPARAMETERS['chunk_size'])\n",
    "\n",
    "# ===========================\n",
    "# Generation Function\n",
    "# ===========================\n",
    "def generate_drum_audio(generator, csi_tensor):\n",
    "    generator.eval()\n",
    "    device = HYPERPARAMETERS[\"device\"]\n",
    "    chunk, overlap = HYPERPARAMETERS[\"chunk_size\"], HYPERPARAMETERS[\"overlap\"]\n",
    "    total_len = HYPERPARAMETERS[\"sequence_length\"]\n",
    "    csi_arr = csi_tensor.cpu().numpy()[0]\n",
    "    generated_audio = np.zeros(total_len, dtype=np.float32)\n",
    "    step_size = chunk - overlap\n",
    "    num_chunks = int(np.ceil((total_len - overlap) / step_size))\n",
    "    base_noise = torch.randn(1, HYPERPARAMETERS['latent_dim'], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_chunks):\n",
    "            start = i * step_size\n",
    "            end = min(start + chunk, total_len)\n",
    "            csi_chunk_np = np.zeros((csi_arr.shape[0], chunk))\n",
    "            actual_len = end - start\n",
    "            if actual_len > 0: csi_chunk_np[:, :actual_len] = csi_arr[:, start:end]\n",
    "\n",
    "            csi_features = extract_basic_features(csi_chunk_np)\n",
    "            csi_chunk = torch.tensor(csi_features.reshape(1, -1, chunk)).float().to(device)\n",
    "            \n",
    "            noise_variation = torch.randn(1, HYPERPARAMETERS['latent_dim'], device=device) * 0.1\n",
    "            current_noise = base_noise * 0.9 + noise_variation * 0.1\n",
    "            \n",
    "            chunk_audio = generator(current_noise, csi_chunk).cpu().numpy().squeeze()\n",
    "            \n",
    "            if i > 0:\n",
    "                fade_out, fade_in = np.linspace(1, 0, overlap), np.linspace(0, 1, overlap)\n",
    "                generated_audio[start:start+overlap] = generated_audio[start:start+overlap] * fade_out + chunk_audio[:overlap] * fade_in\n",
    "                generated_audio[start+overlap:end] = chunk_audio[overlap:actual_len]\n",
    "            else:\n",
    "                generated_audio[start:end] = chunk_audio[:actual_len]\n",
    "    return np.clip(generated_audio, -1, 1)\n",
    "\n",
    "# ===========================\n",
    "# Plotting functions\n",
    "# ===========================\n",
    "def save_time_series(y, sr, filename, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def save_mel_spectrogram(y, sr, filename, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# ===========================\n",
    "# Main\n",
    "# ===========================\n",
    "def main():\n",
    "    os.makedirs(\"output/audio\", exist_ok=True)\n",
    "    os.makedirs(\"output/plots\", exist_ok=True)\n",
    "\n",
    "    csi_path, audio_path = find_matching_files(TARGET_TIMESTAMP)\n",
    "    sr, _ = determine_sample_rate_length(\"data/audio\")\n",
    "\n",
    "    csi_tensor, audio_orig = load_and_align_data(csi_path, audio_path)\n",
    "    norm_csi = normalize_csi(csi_tensor)\n",
    "\n",
    "    gen_model = OptimizedGenerator(HYPERPARAMETERS['latent_dim'], HYPERPARAMETERS['csi_channels']).to(HYPERPARAMETERS['device'])\n",
    "    \n",
    "    # FIX: Search for checkpoint in multiple common locations\n",
    "    checkpoint_paths = [\"output/generator.pth\", \"generator.pth\"]\n",
    "    loaded = False\n",
    "    for cp in checkpoint_paths:\n",
    "        if os.path.exists(cp):\n",
    "            try:\n",
    "                gen_model.load_state_dict(torch.load(cp, map_location=HYPERPARAMETERS['device']))\n",
    "                logging.info(f\"Loaded checkpoint from {cp}\")\n",
    "                loaded = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to load checkpoint {cp} with error: {e}. Trying next path.\")\n",
    "    \n",
    "    if not loaded:\n",
    "        logging.warning(\"No valid checkpoint found. Using untrained model.\")\n",
    "\n",
    "\n",
    "    drum_processor = FixedDrumProcessor(sr)\n",
    "    rc('font', **{'size': 22})\n",
    "    # Process all audio versions\n",
    "    gen_audio_raw = generate_drum_audio(gen_model, norm_csi)\n",
    "    gen_audio_processed, gen_onsets = drum_processor.process_drums(gen_audio_raw)\n",
    "    gen_audio_rich = post_process_drum_rich(gen_audio_processed, gen_onsets, sr, drum_type='kick')\n",
    "    orig_audio_processed, _ = drum_processor.process_drums(audio_orig)\n",
    "\n",
    "    # Save all audio files\n",
    "    sf.write(f\"output/audio/original_raw_{TARGET_TIMESTAMP}.wav\", audio_orig, sr)\n",
    "    sf.write(f\"output/audio/original_processed_{TARGET_TIMESTAMP}.wav\", orig_audio_processed, sr)\n",
    "    sf.write(f\"output/audio/generated_raw_{TARGET_TIMESTAMP}.wav\", gen_audio_raw, sr)\n",
    "    sf.write(f\"output/audio/generated_processed_{TARGET_TIMESTAMP}.wav\", gen_audio_processed, sr)\n",
    "    sf.write(f\"output/audio/generated_rich_{TARGET_TIMESTAMP}.wav\", gen_audio_rich, sr)\n",
    "\n",
    "    # --- Generate Plots ---\n",
    "    prefix = f\"output/plots/{TARGET_TIMESTAMP}\"\n",
    "\n",
    "    # Time Series Plots\n",
    "    save_time_series(audio_orig, sr, f\"{prefix}_original_raw_time.png\", \"Original Raw Waveform\")\n",
    "    save_time_series(orig_audio_processed, sr, f\"{prefix}_original_processed_time.png\", \"Original Processed Waveform\")\n",
    "    save_time_series(gen_audio_raw, sr, f\"{prefix}_generated_raw_time.png\", \"Generated Raw Waveform\")\n",
    "    save_time_series(gen_audio_processed, sr, f\"{prefix}_generated_processed_time.png\", \"Generated Processed Waveform\")\n",
    "    save_time_series(gen_audio_rich, sr, f\"{prefix}_generated_rich_time.png\", \"Generated Rich Waveform\")\n",
    "    \n",
    "    # Mel Spectrograms\n",
    "    save_mel_spectrogram(audio_orig, sr, f\"{prefix}_original_raw_mel.png\", \"Original Raw Mel Spectrogram\")\n",
    "    save_mel_spectrogram(orig_audio_processed, sr, f\"{prefix}_original_processed_mel.png\", \"Original Processed Mel Spectrogram\")\n",
    "    save_mel_spectrogram(gen_audio_raw, sr, f\"{prefix}_generated_raw_mel.png\", \"Generated Raw Mel Spectrogram\")\n",
    "    save_mel_spectrogram(gen_audio_processed, sr, f\"{prefix}_generated_processed_mel.png\", \"Generated Processed Mel Spectrogram\")\n",
    "    save_mel_spectrogram(gen_audio_rich, sr, f\"{prefix}_generated_rich_mel.png\", \"Generated Rich Mel Spectrogram\")\n",
    "\n",
    "    # Combined Plots\n",
    "    # Waveform Comparison\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    librosa.display.waveshow(orig_audio_processed, sr=sr, color='blue', alpha=0.7, label='Original')\n",
    "    librosa.display.waveshow(gen_audio_rich, sr=sr, color='magenta', alpha=0.7, label='Generated')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{prefix}_combined_waveform.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Frequency Spectrum Comparison\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    orig_fft = np.fft.rfft(orig_audio_processed)\n",
    "    gen_fft = np.fft.rfft(gen_audio_rich)\n",
    "    freqs = np.fft.rfftfreq(len(orig_audio_processed), d=1./sr)\n",
    "    plt.semilogy(freqs, np.abs(orig_fft), label='Original Processed', alpha=0.7)\n",
    "    plt.semilogy(freqs, np.abs(gen_fft), label='Generated Rich', alpha=0.7)\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{prefix}_combined_spectrum.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    logging.info(\"All plots and audio files saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 11:12:02,287 - INFO - Enhanced analyzer initialized for timestamp: 2025-05-07_20-08-24.123\n",
      "2025-09-12 11:12:02,289 - INFO - Output directory: output\\analysis\n",
      "2025-09-12 11:12:02,291 - INFO - ============================================================\n",
      "2025-09-12 11:12:02,293 - INFO - STARTING FOCUSED AUDIO-CSI ANALYSIS\n",
      "2025-09-12 11:12:02,294 - INFO - Target Timestamp: 2025-05-07_20-08-24.123\n",
      "2025-09-12 11:12:02,295 - INFO - ============================================================\n",
      "2025-09-12 11:12:02,297 - INFO - Looking for files with timestamp: 2025-05-07_20-08-24.123\n",
      "2025-09-12 11:12:02,309 - INFO - Found original audio file: 2025-05-07_20-08-24.123.wav\n",
      "2025-09-12 11:12:02,310 - INFO - Found enhanced audio file: enhanced_audio_2025-05-07_20-08-24.123.wav\n",
      "2025-09-12 11:12:02,313 - INFO - Found CSI file: top8_filtered_csi_data_2025-05-07_20-08-24.123.csv\n",
      "2025-09-12 11:12:02,317 - INFO - Audio loaded from 2025-05-07_20-08-24.123.wav: 440320 samples at 44100 Hz\n",
      "2025-09-12 11:12:02,323 - INFO - Audio loaded from enhanced_audio_2025-05-07_20-08-24.123.wav: 441000 samples at 44100 Hz\n",
      "2025-09-12 11:12:02,375 - INFO - Complex CSI data loaded: 8 subcarriers, 1139 time samples\n",
      "2025-09-12 11:12:02,377 - INFO - Audio signals trimmed to 440320 samples for consistency.\n",
      "2025-09-12 11:12:02,379 - INFO - Generating CSI temporal plots...\n",
      "2025-09-12 11:12:04,151 - INFO - CSI time series plot saved.\n",
      "2025-09-12 11:12:07,403 - INFO - CSI heatmap plot saved.\n",
      "2025-09-12 11:12:07,406 - INFO - Generating Spectrogram Comparison (Original vs Enhanced Audio)...\n",
      "2025-09-12 11:12:10,202 - INFO - Spectrogram comparison plot saved.\n",
      "2025-09-12 11:12:10,205 - INFO - Generating Pearson correlation analysis (Audio vs CSI)...\n",
      "2025-09-12 11:12:11,220 - INFO - Pearson correlation boxplot saved.\n",
      "2025-09-12 11:12:11,222 - INFO - Generating Cosine Similarity box plot (Original vs Enhanced Audio)...\n",
      "2025-09-12 11:12:14,912 - INFO - Audio comparison cosine similarity box plot saved.\n",
      "2025-09-12 11:12:14,913 - INFO - Generating Feature Correlation boxplot (Original vs Enhanced Audio)...\n",
      "2025-09-12 11:12:18,060 - INFO - Feature correlation boxplot saved.\n",
      "2025-09-12 11:12:18,061 - INFO - Generating Cross-Correlation Heatmap (Audio RMS vs CSI Phase)...\n",
      "2025-09-12 11:12:20,046 - INFO - Cross-correlation heatmap saved.\n",
      "2025-09-12 11:12:20,048 - INFO - ============================================================\n",
      "2025-09-12 11:12:20,049 - INFO - FOCUSED ANALYSIS COMPLETE!\n",
      "2025-09-12 11:12:20,050 - INFO - ============================================================\n",
      "2025-09-12 11:12:20,051 - INFO - All outputs saved in: output\\analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class EnhancedAudioCSIAnalyzer:\n",
    "    \"\"\"Enhanced analyzer focused on specific timestamp with vibrant visualizations\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dir=\"data/audio\", enhanced_audio_dir=\"data/audio_enhanced\", csi_dir=\"data/csi\", output_dir=\"output\", target_timestamp=\"2025-05-07_20-19-42.459\"):\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.enhanced_audio_dir = Path(enhanced_audio_dir)\n",
    "        self.csi_dir = Path(csi_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.target_timestamp = target_timestamp\n",
    "        \n",
    "        # Create analysis subdirectory in output\n",
    "        self.analysis_dir = self.output_dir / \"analysis\"\n",
    "        self.analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Analysis parameters\n",
    "        self.sr = None\n",
    "        self.n_fft = 2048\n",
    "        self.hop_length = 512\n",
    "        self.csi_rate = 100 # Assuming CSI is sampled at 100 Hz\n",
    "        \n",
    "        logging.info(f\"Enhanced analyzer initialized for timestamp: {target_timestamp}\")\n",
    "        logging.info(f\"Output directory: {self.analysis_dir}\")\n",
    "    \n",
    "    def find_target_files(self):\n",
    "        \"\"\"Find the specific files matching the target timestamp\"\"\"\n",
    "        logging.info(f\"Looking for files with timestamp: {self.target_timestamp}\")\n",
    "        \n",
    "        # Find original audio file\n",
    "        audio_pattern = f\"*{self.target_timestamp}*.wav\"\n",
    "        audio_files = list(self.audio_dir.glob(audio_pattern))\n",
    "        if not audio_files:\n",
    "            logging.error(f\"No original audio file found for timestamp {self.target_timestamp}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Find enhanced audio file\n",
    "        enhanced_audio_files = list(self.enhanced_audio_dir.glob(audio_pattern))\n",
    "        if not enhanced_audio_files:\n",
    "            logging.error(f\"No enhanced audio file found for timestamp {self.target_timestamp}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Find CSI file\n",
    "        csi_pattern = f\"*{self.target_timestamp}*.csv\"\n",
    "        csi_files = list(self.csi_dir.glob(csi_pattern))\n",
    "        if not csi_files:\n",
    "            logging.error(f\"No CSI file found for timestamp {self.target_timestamp}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        audio_file = audio_files[0]\n",
    "        enhanced_audio_file = enhanced_audio_files[0]\n",
    "        csi_file = csi_files[0]\n",
    "        \n",
    "        logging.info(f\"Found original audio file: {audio_file.name}\")\n",
    "        logging.info(f\"Found enhanced audio file: {enhanced_audio_file.name}\")\n",
    "        logging.info(f\"Found CSI file: {csi_file.name}\")\n",
    "        \n",
    "        return audio_file, enhanced_audio_file, csi_file\n",
    "    \n",
    "    def load_audio(self, audio_file):\n",
    "        \"\"\"Load and preprocess audio file\"\"\"\n",
    "        try:\n",
    "            audio, sr = sf.read(audio_file)\n",
    "            if len(audio.shape) > 1:\n",
    "                audio = np.mean(audio, axis=1)\n",
    "            if self.sr is None:\n",
    "                self.sr = sr\n",
    "            logging.info(f\"Audio loaded from {audio_file.name}: {len(audio)} samples at {sr} Hz\")\n",
    "            return audio, sr\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio {audio_file}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def load_csi(self, csi_file):\n",
    "        \"\"\"Load and preprocess CSI file, robustly handling complex numbers.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csi_file)\n",
    "            subcarrier_cols = [col for col in df.columns if col.startswith('subcarrier_')]\n",
    "            if not subcarrier_cols:\n",
    "                logging.warning(f\"No subcarrier columns found in {csi_file}\")\n",
    "                return None\n",
    "            \n",
    "            csi_df = df[subcarrier_cols]\n",
    "            \n",
    "            # Explicitly convert all columns to string type before using the .str accessor.\n",
    "            csi_df = csi_df.astype(str)\n",
    "            \n",
    "            # Convert string representation of complex numbers to actual complex numbers\n",
    "            csi_complex_data = csi_df.apply(lambda col: col.str.replace('i', 'j').apply(complex)).values.T\n",
    "            \n",
    "            logging.info(f\"Complex CSI data loaded: {csi_complex_data.shape[0]} subcarriers, {csi_complex_data.shape[1]} time samples\")\n",
    "            return csi_complex_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading CSI {csi_file}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _align_signals(self, audio_features, csi_data):\n",
    "        \"\"\"Align audio features to the CSI time axis via interpolation.\"\"\"\n",
    "        num_csi_frames = csi_data.shape[1]\n",
    "        csi_time = np.linspace(0, num_csi_frames / self.csi_rate, num=num_csi_frames)\n",
    "        \n",
    "        audio_time = librosa.times_like(audio_features, sr=self.sr, hop_length=self.hop_length)\n",
    "        \n",
    "        end_time = min(csi_time[-1], audio_time[-1])\n",
    "        valid_csi_indices = csi_time <= end_time\n",
    "        valid_audio_indices = audio_time <= end_time\n",
    "\n",
    "        csi_time_trunc = csi_time[valid_csi_indices]\n",
    "        csi_data_trunc = csi_data[:, valid_csi_indices]\n",
    "        \n",
    "        audio_time_trunc = audio_time[valid_audio_indices]\n",
    "        audio_features_trunc = audio_features[:, valid_audio_indices]\n",
    "\n",
    "        aligned_audio_features = np.zeros((audio_features.shape[0], csi_data_trunc.shape[1]))\n",
    "        for i in range(audio_features.shape[0]):\n",
    "            aligned_audio_features[i, :] = np.interp(csi_time_trunc, audio_time_trunc, audio_features_trunc[i, :])\n",
    "            \n",
    "        return aligned_audio_features, csi_data_trunc\n",
    "\n",
    "    def plot_csi_time_series(self, csi_complex_data):\n",
    "        \"\"\"Plot and save the CSI subcarrier time series graph.\"\"\"\n",
    "        try:\n",
    "            csi_amplitude = np.abs(csi_complex_data)\n",
    "            n_subcarriers, n_time = csi_amplitude.shape\n",
    "            time_axis = np.linspace(0, n_time / self.csi_rate, n_time)\n",
    "            \n",
    "            plt.figure(figsize=(16, 8))\n",
    "            colors = plt.cm.magma(np.linspace(0, 1, n_subcarriers))\n",
    "            for i in range(n_subcarriers):\n",
    "                plt.plot(time_axis, csi_amplitude[i], color=colors[i], linewidth=1.5, alpha=0.6)\n",
    "            rc('font', **{'size': 22})\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('CSI Amplitude')\n",
    "            plt.grid(True, alpha=0.4)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.analysis_dir / f\"01_csi_time_series_{self.target_timestamp}.png\", dpi=300)\n",
    "            plt.close()\n",
    "            logging.info(\"CSI time series plot saved.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate CSI time series plot: {e}\")\n",
    "\n",
    "    def plot_csi_heatmap(self, csi_complex_data):\n",
    "        \"\"\"Plot and save the CSI amplitude heatmap.\"\"\"\n",
    "        try:\n",
    "            csi_amplitude = np.abs(csi_complex_data)\n",
    "            n_subcarriers, n_time = csi_amplitude.shape\n",
    "            time_axis = np.linspace(0, n_time / self.csi_rate, n_time)\n",
    "            \n",
    "            plt.figure(figsize=(16, 8))\n",
    "            plt.imshow(csi_amplitude, aspect='auto', cmap='magma', extent=[0, time_axis[-1], n_subcarriers, 0], interpolation='bilinear')\n",
    "            rc('font', **{'size': 22})\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Subcarrier Index')\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.set_label('CSI Amplitude')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.analysis_dir / f\"02_csi_heatmap_{self.target_timestamp}.png\", dpi=300)\n",
    "            plt.close()\n",
    "            logging.info(\"CSI heatmap plot saved.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate CSI heatmap plot: {e}\")\n",
    "\n",
    "    def plot_spectrogram_comparison(self, audio_original, audio_enhanced, sr):\n",
    "        \"\"\"Plot and save a side-by-side comparison of mel spectrograms.\"\"\"\n",
    "        try:\n",
    "            S_orig = librosa.feature.melspectrogram(y=audio_original, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "            S_enh = librosa.feature.melspectrogram(y=audio_enhanced, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "            \n",
    "            S_db_orig = librosa.power_to_db(S_orig, ref=np.max)\n",
    "            S_db_enh = librosa.power_to_db(S_enh, ref=np.max)\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), sharex=True, sharey=True)\n",
    "\n",
    "            img1 = librosa.display.specshow(S_db_orig, sr=sr, hop_length=self.hop_length, x_axis='time', y_axis='mel', ax=ax1, cmap='magma')\n",
    "            ax1.set_ylabel('Frequency (Hz)')\n",
    "            fig.colorbar(img1, ax=ax1, format='%+2.0f dB', label='Intensity (dB)')\n",
    "\n",
    "            img2 = librosa.display.specshow(S_db_enh, sr=sr, hop_length=self.hop_length, x_axis='time', y_axis='mel', ax=ax2, cmap='magma')\n",
    "            ax2.set_xlabel('Time (s)')\n",
    "            ax2.set_ylabel('Frequency (Hz)')\n",
    "            fig.colorbar(img2, ax=ax2, format='%+2.0f dB', label='Intensity (dB)')\n",
    "            rc('font', **{'size': 16})\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.analysis_dir / f\"03_spectrogram_comparison_{self.target_timestamp}.png\", dpi=300)\n",
    "            plt.close()\n",
    "            logging.info(\"Spectrogram comparison plot saved.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate spectrogram comparison: {e}\")\n",
    "\n",
    "    def generate_pearson_correlation_analysis(self, audio, csi_complex_data, sr):\n",
    "        \"\"\"Generate Pearson correlation analysis with a multi-box plot.\"\"\"\n",
    "        try:\n",
    "            rms = librosa.feature.rms(y=audio, hop_length=self.hop_length)[0]\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=self.hop_length)[0]\n",
    "            zcr = librosa.feature.zero_crossing_rate(y=audio, hop_length=self.hop_length)[0]\n",
    "            onset_env = librosa.onset.onset_strength(y=audio, sr=sr, hop_length=self.hop_length)\n",
    "            audio_features = np.array([rms, spectral_centroid, zcr, onset_env])\n",
    "            feature_names = ['RMS Energy', 'Spectral Centroid', 'ZCR', 'Onset Strength']\n",
    "            \n",
    "            csi_amplitude = np.abs(csi_complex_data)\n",
    "            \n",
    "            audio_features_aligned, csi_amplitude_aligned = self._align_signals(audio_features, csi_amplitude)\n",
    "            \n",
    "            n_features, n_subcarriers = len(feature_names), csi_amplitude_aligned.shape[0]\n",
    "            correlation_matrix = np.zeros((n_features, n_subcarriers))\n",
    "            \n",
    "            for i in range(n_features):\n",
    "                for j in range(n_subcarriers):\n",
    "                    if np.std(audio_features_aligned[i]) > 0 and np.std(csi_amplitude_aligned[j]) > 0:\n",
    "                        corr, _ = pearsonr(audio_features_aligned[i], csi_amplitude_aligned[j])\n",
    "                        correlation_matrix[i, j] = corr if not np.isnan(corr) else 0\n",
    "                    else:\n",
    "                        correlation_matrix[i, j] = 0\n",
    "            rc('font', **{'size': 16})\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            abs_corr_data = [np.abs(correlation_matrix[i, :]) for i in range(n_features)]\n",
    "            box = plt.boxplot(abs_corr_data, vert=True, patch_artist=True, tick_labels=feature_names)\n",
    "            \n",
    "            for patch in box['boxes']:\n",
    "                patch.set_facecolor('lightgrey')\n",
    "            for median in box['medians']:\n",
    "                median.set_color('black')\n",
    "\n",
    "            plt.ylabel('Absolute Correlation')\n",
    "            plt.tick_params(axis='x', rotation=45)\n",
    "            plt.grid(True, alpha=0.4, axis='y')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.analysis_dir / f\"04_pearson_correlation_boxplot_{self.target_timestamp}.png\", dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            logging.info(\"Pearson correlation boxplot saved.\")\n",
    "            return correlation_matrix\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating Pearson correlation analysis: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_cosine_similarity_boxplot(self, audio_original, audio_enhanced, sr):\n",
    "        \"\"\"Generate cosine similarity boxplot between original and enhanced audio.\"\"\"\n",
    "        try:\n",
    "            data_to_plot = []\n",
    "            feature_names = []\n",
    "            win_len = 20\n",
    "\n",
    "            def get_windowed_similarity(feat_orig, feat_enh):\n",
    "                min_frames = min(len(feat_orig), len(feat_enh))\n",
    "                feat_orig = feat_orig[:min_frames]\n",
    "                feat_enh = feat_enh[:min_frames]\n",
    "                \n",
    "                if len(feat_orig) < win_len: return []\n",
    "                \n",
    "                orig_wins = as_strided(feat_orig, shape=(len(feat_orig) - win_len + 1, win_len), strides=(feat_orig.strides[0], feat_orig.strides[0]))\n",
    "                enh_wins = as_strided(feat_enh, shape=(len(feat_enh) - win_len + 1, win_len), strides=(feat_enh.strides[0], feat_enh.strides[0]))\n",
    "                \n",
    "                return [cosine_similarity(orig.reshape(1, -1), enh.reshape(1, -1))[0, 0] for orig, enh in zip(orig_wins, enh_wins)]\n",
    "            \n",
    "            def get_vector_similarity(feat_orig, feat_enh):\n",
    "                min_frames = min(feat_orig.shape[1], feat_enh.shape[1])\n",
    "                feat_orig = feat_orig[:, :min_frames]\n",
    "                feat_enh = feat_enh[:, :min_frames]\n",
    "                return [cosine_similarity(feat_orig[:, i].reshape(1,-1), feat_enh[:, i].reshape(1,-1))[0,0] for i in range(min_frames)]\n",
    "\n",
    "            # RMS\n",
    "            rms_orig = librosa.feature.rms(y=audio_original, hop_length=self.hop_length)[0]\n",
    "            rms_enh = librosa.feature.rms(y=audio_enhanced, hop_length=self.hop_length)[0]\n",
    "            data_to_plot.append(get_windowed_similarity(rms_orig, rms_enh))\n",
    "            feature_names.append('RMS')\n",
    "\n",
    "            # Spectral Centroid\n",
    "            sc_orig = librosa.feature.spectral_centroid(y=audio_original, sr=sr, hop_length=self.hop_length)[0]\n",
    "            sc_enh = librosa.feature.spectral_centroid(y=audio_enhanced, sr=sr, hop_length=self.hop_length)[0]\n",
    "            data_to_plot.append(get_windowed_similarity(sc_orig, sc_enh))\n",
    "            feature_names.append('Spectral Centroid')\n",
    "\n",
    "            # Zero-Crossing Rate\n",
    "            zcr_orig = librosa.feature.zero_crossing_rate(y=audio_original, hop_length=self.hop_length)[0]\n",
    "            zcr_enh = librosa.feature.zero_crossing_rate(y=audio_enhanced, hop_length=self.hop_length)[0]\n",
    "            data_to_plot.append(get_windowed_similarity(zcr_orig, zcr_enh))\n",
    "            feature_names.append('ZCR')\n",
    "\n",
    "            # Spectral Contrast\n",
    "            spec_con_orig = librosa.feature.spectral_contrast(y=audio_original, sr=sr, hop_length=self.hop_length)\n",
    "            spec_con_enh = librosa.feature.spectral_contrast(y=audio_enhanced, sr=sr, hop_length=self.hop_length)\n",
    "            data_to_plot.append(get_vector_similarity(spec_con_orig, spec_con_enh))\n",
    "            feature_names.append('Spectral Contrast')\n",
    "            \n",
    "            rc('font', **{'size': 22})\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            box = plt.boxplot(data_to_plot, vert=True, patch_artist=True, tick_labels=feature_names)\n",
    "            \n",
    "            for patch in box['boxes']:\n",
    "                patch.set_facecolor('lightgrey')\n",
    "            for median in box['medians']:\n",
    "                median.set_color('black')\n",
    "\n",
    "            plt.ylabel('Cosine Similarity Score')\n",
    "            plt.tick_params(axis='x', rotation=45)\n",
    "            plt.grid(True, alpha=0.4, axis='y')\n",
    "            plt.ylim(0, 1.05)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.analysis_dir / f\"05_cosine_similarity_boxplot_{self.target_timestamp}.png\", dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            logging.info(\"Audio comparison cosine similarity box plot saved.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating audio comparison boxplot: {e}\")\n",
    "\n",
    "    def generate_feature_correlation_boxplot(self, audio_original, audio_enhanced, sr):\n",
    "        \"\"\"Generate a boxplot of Pearson correlations between original and enhanced audio features.\"\"\"\n",
    "        try:\n",
    "            features = {\n",
    "                \"RMS\": (\n",
    "                    librosa.feature.rms(y=audio_original, hop_length=self.hop_length)[0],\n",
    "                    librosa.feature.rms(y=audio_enhanced, hop_length=self.hop_length)[0]\n",
    "                ),\n",
    "                \"Spectral Centroid\": (\n",
    "                    librosa.feature.spectral_centroid(y=audio_original, sr=sr, hop_length=self.hop_length)[0],\n",
    "                    librosa.feature.spectral_centroid(y=audio_enhanced, sr=sr, hop_length=self.hop_length)[0]\n",
    "                ),\n",
    "                \"ZCR\": (\n",
    "                    librosa.feature.zero_crossing_rate(y=audio_original, hop_length=self.hop_length)[0],\n",
    "                    librosa.feature.zero_crossing_rate(y=audio_enhanced, hop_length=self.hop_length)[0]\n",
    "                ),\n",
    "                \"Onset Strength\": (\n",
    "                    librosa.onset.onset_strength(y=audio_original, sr=sr, hop_length=self.hop_length),\n",
    "                    librosa.onset.onset_strength(y=audio_enhanced, sr=sr, hop_length=self.hop_length)\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            correlations_data = []\n",
    "            feature_names = []\n",
    "            win_len = 20\n",
    "\n",
    "            for name, (orig, enh) in features.items():\n",
    "                min_frames = min(len(orig), len(enh))\n",
    "                orig = orig[:min_frames]\n",
    "                enh = enh[:min_frames]\n",
    "\n",
    "                if len(orig) < win_len: continue\n",
    "                \n",
    "                orig_wins = as_strided(orig, shape=(len(orig) - win_len + 1, win_len), strides=(orig.strides[0], orig.strides[0]))\n",
    "                enh_wins = as_strided(enh, shape=(len(enh) - win_len + 1, win_len), strides=(enh.strides[0], enh.strides[0]))\n",
    "                \n",
    "                window_corrs = []\n",
    "                for o_win, e_win in zip(orig_wins, enh_wins):\n",
    "                    if np.std(o_win) > 1e-6 and np.std(e_win) > 1e-6:\n",
    "                        corr, _ = pearsonr(o_win, e_win)\n",
    "                        window_corrs.append(np.abs(corr))\n",
    "                \n",
    "                if window_corrs:\n",
    "                    correlations_data.append(window_corrs)\n",
    "                    feature_names.append(name)\n",
    "            rc('font', **{'size': 16})\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            box = plt.boxplot(correlations_data, vert=True, patch_artist=True, tick_labels=feature_names)\n",
    "\n",
    "            for patch in box['boxes']:\n",
    "                patch.set_facecolor('lightgrey')\n",
    "            for median in box['medians']:\n",
    "                median.set_color('black')\n",
    "            \n",
    "            plt.ylabel('Absolute Pearson Correlation')\n",
    "            plt.tick_params(axis='x', rotation=45)\n",
    "            plt.grid(True, alpha=0.4, axis='y')\n",
    "            plt.ylim(0, 1.05)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.analysis_dir / f\"06_feature_correlation_boxplot_{self.target_timestamp}.png\", dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            logging.info(\"Feature correlation boxplot saved.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating feature correlation boxplot: {e}\")\n",
    "\n",
    "    def generate_cross_correlation_heatmap(self, audio, csi_complex_data, sr, max_lag_s=0.5):\n",
    "        \"\"\"Generates a heatmap of cross-correlation between audio RMS and CSI phase changes.\"\"\"\n",
    "        try:\n",
    "            csi_phase = np.unwrap(np.angle(csi_complex_data), axis=1)\n",
    "            csi_phase_diff = np.diff(csi_phase, axis=1)\n",
    "            \n",
    "            rms = librosa.feature.rms(y=audio, hop_length=self.hop_length)[0].reshape(1, -1)\n",
    "            \n",
    "            rms_aligned, csi_phase_diff_aligned = self._align_signals(rms, csi_phase_diff)\n",
    "            rms_aligned_1d = rms_aligned.flatten()\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            rms_scaled = scaler.fit_transform(rms_aligned_1d.reshape(-1, 1)).flatten()\n",
    "            csi_scaled = scaler.fit_transform(csi_phase_diff_aligned.T).T\n",
    "\n",
    "            max_lag_samples = int(max_lag_s * self.csi_rate)\n",
    "            n_subcarriers, n_samples = csi_scaled.shape\n",
    "            xcorr_matrix = np.zeros((n_subcarriers, 2 * max_lag_samples + 1))\n",
    "            \n",
    "            for i in range(n_subcarriers):\n",
    "                correlation = signal.correlate(csi_scaled[i, :], rms_scaled, mode='full')\n",
    "                mid_point = len(correlation) // 2\n",
    "                start_idx = mid_point - max_lag_samples\n",
    "                end_idx = mid_point + max_lag_samples + 1\n",
    "                xcorr_matrix[i, :] = correlation[start_idx:end_idx]\n",
    "            rc('font', **{'size': 22})\n",
    "            plt.figure(figsize=(16, 8))\n",
    "            lags_sec = np.linspace(-max_lag_s, max_lag_s, 2 * max_lag_samples + 1)\n",
    "            \n",
    "            sns.heatmap(xcorr_matrix, cmap='coolwarm', cbar_kws={'label': 'Cross-Correlation Coefficient'}, yticklabels=10)\n",
    "            \n",
    "            tick_positions = np.linspace(0, len(lags_sec) - 1, 11)\n",
    "            tick_labels = [f\"{lag:.2f}\" for lag in np.linspace(-max_lag_s, max_lag_s, 11)]\n",
    "            plt.xticks(tick_positions, tick_labels)\n",
    "            \n",
    "            plt.xlabel('Time Lag (s) [CSI leads <> Audio leads]')\n",
    "            plt.ylabel('Subcarrier Index')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.analysis_dir / f\"07_cross_correlation_heatmap_{self.target_timestamp}.png\", dpi=300)\n",
    "            plt.close()\n",
    "            logging.info(\"Cross-correlation heatmap saved.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate cross-correlation heatmap: {e}\")\n",
    "\n",
    "    def run_enhanced_analysis(self):\n",
    "        \"\"\"Run the complete enhanced analysis for the target timestamp\"\"\"\n",
    "        logging.info(\"=\"*60)\n",
    "        logging.info(\"STARTING FOCUSED AUDIO-CSI ANALYSIS\")\n",
    "        logging.info(f\"Target Timestamp: {self.target_timestamp}\")\n",
    "        logging.info(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            audio_file, enhanced_audio_file, csi_file = self.find_target_files()\n",
    "            if audio_file is None or csi_file is None or enhanced_audio_file is None:\n",
    "                return\n",
    "            \n",
    "            audio_original, sr_orig = self.load_audio(audio_file)\n",
    "            audio_enhanced, sr_enh = self.load_audio(enhanced_audio_file)\n",
    "            csi_complex_data = self.load_csi(csi_file)\n",
    "            \n",
    "            if audio_original is None or csi_complex_data is None or audio_enhanced is None:\n",
    "                logging.error(\"Failed to load one or more data files.\")\n",
    "                return\n",
    "\n",
    "            min_len = min(len(audio_original), len(audio_enhanced))\n",
    "            audio_original = audio_original[:min_len]\n",
    "            audio_enhanced = audio_enhanced[:min_len]\n",
    "            logging.info(f\"Audio signals trimmed to {min_len} samples for consistency.\")\n",
    "            \n",
    "            logging.info(\"Generating CSI temporal plots...\")\n",
    "            self.plot_csi_time_series(csi_complex_data)\n",
    "            self.plot_csi_heatmap(csi_complex_data)\n",
    "            \n",
    "            logging.info(\"Generating Spectrogram Comparison (Original vs Enhanced Audio)...\")\n",
    "            self.plot_spectrogram_comparison(audio_original, audio_enhanced, self.sr)\n",
    "            \n",
    "            logging.info(\"Generating Pearson correlation analysis (Audio vs CSI)...\")\n",
    "            self.generate_pearson_correlation_analysis(audio_original, csi_complex_data, self.sr)\n",
    "            \n",
    "            logging.info(\"Generating Cosine Similarity box plot (Original vs Enhanced Audio)...\")\n",
    "            self.generate_cosine_similarity_boxplot(audio_original, audio_enhanced, self.sr)\n",
    "\n",
    "            logging.info(\"Generating Feature Correlation boxplot (Original vs Enhanced Audio)...\")\n",
    "            self.generate_feature_correlation_boxplot(audio_original, audio_enhanced, self.sr)\n",
    "            \n",
    "            logging.info(\"Generating Cross-Correlation Heatmap (Audio RMS vs CSI Phase)...\")\n",
    "            self.generate_cross_correlation_heatmap(audio_original, csi_complex_data, self.sr)\n",
    "            \n",
    "            logging.info(\"=\"*60)\n",
    "            logging.info(\"FOCUSED ANALYSIS COMPLETE!\")\n",
    "            logging.info(\"=\"*60)\n",
    "            logging.info(f\"All outputs saved in: {self.analysis_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during the main analysis pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the enhanced analysis\"\"\"\n",
    "    analyzer = EnhancedAudioCSIAnalyzer(\n",
    "        audio_dir=\"data/audio\",\n",
    "        enhanced_audio_dir=\"output/audio\",\n",
    "        csi_dir=\"data/processed_csi\", \n",
    "        output_dir=\"output\",\n",
    "        target_timestamp=\"2025-05-07_20-08-24.123\"\n",
    "    )\n",
    "    analyzer.run_enhanced_analysis()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 22:45:29,723 - INFO - Found 75 CSI files to process.\n",
      "2025-09-11 22:45:29,726 - INFO - Processing file: csi_data_2024-08-17_23-16-37.224.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 22:45:30,684 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-08-17_23-16-37.224_heatmap.png\n",
      "2025-09-11 22:45:30,686 - INFO - Processing file: csi_data_2024-09-25_15-55-29.664.csv\n",
      "2025-09-11 22:45:31,794 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-09-25_15-55-29.664_heatmap.png\n",
      "2025-09-11 22:45:31,796 - INFO - Processing file: csi_data_2024-09-25_15-55-29.664_1.csv\n",
      "2025-09-11 22:45:32,942 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-09-25_15-55-29.664_1_heatmap.png\n",
      "2025-09-11 22:45:32,945 - INFO - Processing file: csi_data_2024-09-25_16-27-42.805.csv\n",
      "2025-09-11 22:45:34,103 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-09-25_16-27-42.805_heatmap.png\n",
      "2025-09-11 22:45:34,105 - INFO - Processing file: csi_data_2024-09-25_22-01-04.590.csv\n",
      "2025-09-11 22:45:35,029 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-09-25_22-01-04.590_heatmap.png\n",
      "2025-09-11 22:45:35,030 - INFO - Processing file: csi_data_2024-10-27_19-23-35.326.csv\n",
      "2025-09-11 22:45:35,914 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-10-27_19-23-35.326_heatmap.png\n",
      "2025-09-11 22:45:35,919 - INFO - Processing file: csi_data_2024-10-27_20-06-08.714.csv\n",
      "2025-09-11 22:45:36,017 - WARNING - Could not parse CSI string: '3878.86 -24 9 -24 9 -24 10 -25...'. Error: invalid literal for int() with base 10: '3878.86'\n",
      "2025-09-11 22:45:36,018 - ERROR - Failed to process csi_data_2024-10-27_20-06-08.714.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 22:45:36,020 - INFO - Processing file: csi_data_2024-10-27_20-19-52.347.csv\n",
      "2025-09-11 22:45:36,105 - ERROR - Failed to process csi_data_2024-10-27_20-19-52.347.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 22:45:36,107 - INFO - Processing file: csi_data_2024-10-27_20-21-47.356.csv\n",
      "2025-09-11 22:45:37,014 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-10-27_20-21-47.356_heatmap.png\n",
      "2025-09-11 22:45:37,017 - INFO - Processing file: csi_data_2024-10-27_20-27-39.60.csv\n",
      "2025-09-11 22:45:37,854 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-10-27_20-27-39.60_heatmap.png\n",
      "2025-09-11 22:45:37,855 - INFO - Processing file: csi_data_2024-10-27_20-28-20.897.csv\n",
      "2025-09-11 22:45:38,772 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2024-10-27_20-28-20.897_heatmap.png\n",
      "2025-09-11 22:45:38,773 - INFO - Processing file: csi_data_2025-05-06_16-23-31.748.csv\n",
      "2025-09-11 22:45:38,781 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-06_16-23-31.748.csv. Skipping.\n",
      "2025-09-11 22:45:38,783 - INFO - Processing file: csi_data_2025-05-06_16-37-43.261.csv\n",
      "2025-09-11 22:45:38,793 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-06_16-37-43.261.csv. Skipping.\n",
      "2025-09-11 22:45:38,795 - INFO - Processing file: csi_data_2025-05-07_13-34-06.154.csv\n",
      "2025-09-11 22:45:38,805 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_13-34-06.154.csv. Skipping.\n",
      "2025-09-11 22:45:38,806 - INFO - Processing file: csi_data_2025-05-07_13-56-11.157.csv\n",
      "2025-09-11 22:45:38,816 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_13-56-11.157.csv. Skipping.\n",
      "2025-09-11 22:45:38,818 - INFO - Processing file: csi_data_2025-05-07_13-56-56.847.csv\n",
      "2025-09-11 22:45:39,780 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_13-56-56.847_heatmap.png\n",
      "2025-09-11 22:45:39,782 - INFO - Processing file: csi_data_2025-05-07_14-05-11.578.csv\n",
      "2025-09-11 22:45:40,651 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_14-05-11.578_heatmap.png\n",
      "2025-09-11 22:45:40,653 - INFO - Processing file: csi_data_2025-05-07_19-43-03.811.csv\n",
      "2025-09-11 22:45:40,656 - ERROR - Failed to process csi_data_2025-05-07_19-43-03.811.csv. Error: No columns to parse from file\n",
      "2025-09-11 22:45:40,657 - INFO - Processing file: csi_data_2025-05-07_19-44-02.289.csv\n",
      "2025-09-11 22:45:40,662 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_19-44-02.289.csv. Skipping.\n",
      "2025-09-11 22:45:40,663 - INFO - Processing file: csi_data_2025-05-07_19-44-29.54.csv\n",
      "2025-09-11 22:45:40,673 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_19-44-29.54.csv. Skipping.\n",
      "2025-09-11 22:45:40,674 - INFO - Processing file: csi_data_2025-05-07_19-45-00.183.csv\n",
      "2025-09-11 22:45:40,701 - WARNING - Could not parse CSI string: '-79 16 11 0 -3 6 -6 7 -5 5 -9 ...'. Error: invalid literal for int() with base 10: '-'\n",
      "2025-09-11 22:45:42,553 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-45-00.183_heatmap.png\n",
      "2025-09-11 22:45:42,557 - INFO - Processing file: csi_data_2025-05-07_19-45-29.505.csv\n",
      "2025-09-11 22:45:44,228 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-45-29.505_heatmap.png\n",
      "2025-09-11 22:45:44,230 - INFO - Processing file: csi_data_2025-05-07_19-45-56.181.csv\n",
      "2025-09-11 22:45:44,361 - ERROR - Failed to process csi_data_2025-05-07_19-45-56.181.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 22:45:44,363 - INFO - Processing file: csi_data_2025-05-07_19-46-29.918.csv\n",
      "2025-09-11 22:45:44,370 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_19-46-29.918.csv. Skipping.\n",
      "2025-09-11 22:45:44,371 - INFO - Processing file: csi_data_2025-05-07_19-46-57.63.csv\n",
      "2025-09-11 22:45:44,390 - WARNING - Could not parse CSI string: '98 32 6 0 -4 3 -6 0 -2 0 -11 -...'. Error: invalid literal for int() with base 10: '(10'\n",
      "2025-09-11 22:45:46,171 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-46-57.63_heatmap.png\n",
      "2025-09-11 22:45:46,174 - INFO - Processing file: csi_data_2025-05-07_19-47-24.449.csv\n",
      "2025-09-11 22:45:46,272 - ERROR - Failed to process csi_data_2025-05-07_19-47-24.449.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 22:45:46,275 - INFO - Processing file: csi_data_2025-05-07_19-47-51.453.csv\n",
      "2025-09-11 22:45:48,120 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-47-51.453_heatmap.png\n",
      "2025-09-11 22:45:48,125 - INFO - Processing file: csi_data_2025-05-07_19-49-32.600.csv\n",
      "2025-09-11 22:45:48,222 - ERROR - Failed to process csi_data_2025-05-07_19-49-32.600.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 22:45:48,225 - INFO - Processing file: csi_data_2025-05-07_19-50-25.260.csv\n",
      "2025-09-11 22:45:48,231 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_19-50-25.260.csv. Skipping.\n",
      "2025-09-11 22:45:48,232 - INFO - Processing file: csi_data_2025-05-07_19-50-56.583.csv\n",
      "2025-09-11 22:45:49,933 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-50-56.583_heatmap.png\n",
      "2025-09-11 22:45:49,938 - INFO - Processing file: csi_data_2025-05-07_19-51-23.647.csv\n",
      "2025-09-11 22:45:49,944 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_19-51-23.647.csv. Skipping.\n",
      "2025-09-11 22:45:49,945 - INFO - Processing file: csi_data_2025-05-07_19-51-50.679.csv\n",
      "2025-09-11 22:45:51,828 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-51-50.679_heatmap.png\n",
      "2025-09-11 22:45:51,832 - INFO - Processing file: csi_data_2025-05-07_19-52-17.730.csv\n",
      "2025-09-11 22:45:51,955 - ERROR - Failed to process csi_data_2025-05-07_19-52-17.730.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 22:45:51,959 - INFO - Processing file: csi_data_2025-05-07_19-52-45.189.csv\n",
      "2025-09-11 22:45:53,754 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-52-45.189_heatmap.png\n",
      "2025-09-11 22:45:53,757 - INFO - Processing file: csi_data_2025-05-07_19-53-13.777.csv\n",
      "2025-09-11 22:45:55,772 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-53-13.777_heatmap.png\n",
      "2025-09-11 22:45:55,775 - INFO - Processing file: csi_data_2025-05-07_19-53-40.821.csv\n",
      "2025-09-11 22:45:57,555 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-53-40.821_heatmap.png\n",
      "2025-09-11 22:45:57,560 - INFO - Processing file: csi_data_2025-05-07_19-54-08.41.csv\n",
      "2025-09-11 22:45:57,566 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_19-54-08.41.csv. Skipping.\n",
      "2025-09-11 22:45:57,567 - INFO - Processing file: csi_data_2025-05-07_19-57-21.338.csv\n",
      "2025-09-11 22:45:59,085 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-57-21.338_heatmap.png\n",
      "2025-09-11 22:45:59,089 - INFO - Processing file: csi_data_2025-05-07_19-57-49.127.csv\n",
      "2025-09-11 22:46:00,816 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_19-57-49.127_heatmap.png\n",
      "2025-09-11 22:46:00,820 - INFO - Processing file: csi_data_2025-05-07_20-00-30.518.csv\n",
      "2025-09-11 22:46:02,496 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-00-30.518_heatmap.png\n",
      "2025-09-11 22:46:02,499 - INFO - Processing file: csi_data_2025-05-07_20-00-57.375.csv\n",
      "2025-09-11 22:46:03,931 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-00-57.375_heatmap.png\n",
      "2025-09-11 22:46:03,934 - INFO - Processing file: csi_data_2025-05-07_20-01-24.106.csv\n",
      "2025-09-11 22:46:05,642 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-01-24.106_heatmap.png\n",
      "2025-09-11 22:46:05,649 - INFO - Processing file: csi_data_2025-05-07_20-01-51.808.csv\n",
      "2025-09-11 22:46:05,676 - WARNING - Could not parse CSI string: '98 32 6 0 13 5 9 -1 1 -1 2 0 -...'. Error: invalid literal for int() with base 10: '-'\n",
      "2025-09-11 22:46:07,414 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-01-51.808_heatmap.png\n",
      "2025-09-11 22:46:07,418 - INFO - Processing file: csi_data_2025-05-07_20-02-18.833.csv\n",
      "2025-09-11 22:46:09,115 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-02-18.833_heatmap.png\n",
      "2025-09-11 22:46:09,118 - INFO - Processing file: csi_data_2025-05-07_20-02-46.107.csv\n",
      "2025-09-11 22:46:11,331 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-02-46.107_heatmap.png\n",
      "2025-09-11 22:46:11,335 - INFO - Processing file: csi_data_2025-05-07_20-03-13.66.csv\n",
      "2025-09-11 22:46:11,342 - ERROR - Failed to process csi_data_2025-05-07_20-03-13.66.csv. Error: Error tokenizing data. C error: Expected 27 fields in line 28, saw 61\n",
      "\n",
      "2025-09-11 22:46:11,343 - INFO - Processing file: csi_data_2025-05-07_20-03-39.886.csv\n",
      "2025-09-11 22:46:11,439 - ERROR - Failed to process csi_data_2025-05-07_20-03-39.886.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 22:46:11,441 - INFO - Processing file: csi_data_2025-05-07_20-04-07.160.csv\n",
      "2025-09-11 22:46:11,448 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_20-04-07.160.csv. Skipping.\n",
      "2025-09-11 22:46:11,449 - INFO - Processing file: csi_data_2025-05-07_20-04-34.37.csv\n",
      "2025-09-11 22:46:11,456 - ERROR - Failed to process csi_data_2025-05-07_20-04-34.37.csv. Error: Error tokenizing data. C error: Expected 27 fields in line 23, saw 34\n",
      "\n",
      "2025-09-11 22:46:11,457 - INFO - Processing file: csi_data_2025-05-07_20-05-00.814.csv\n",
      "2025-09-11 22:46:13,213 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-05-00.814_heatmap.png\n",
      "2025-09-11 22:46:13,216 - INFO - Processing file: csi_data_2025-05-07_20-05-39.689.csv\n",
      "2025-09-11 22:46:13,224 - ERROR - Failed to process csi_data_2025-05-07_20-05-39.689.csv. Error: Error tokenizing data. C error: Expected 27 fields in line 18, saw 167\n",
      "\n",
      "2025-09-11 22:46:13,226 - INFO - Processing file: csi_data_2025-05-07_20-07-57.121.csv\n",
      "2025-09-11 22:46:13,264 - ERROR - Failed to process csi_data_2025-05-07_20-07-57.121.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 22:46:13,266 - INFO - Processing file: csi_data_2025-05-07_20-08-24.123.csv\n",
      "2025-09-11 22:46:14,901 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-08-24.123_heatmap.png\n",
      "2025-09-11 22:46:14,903 - INFO - Processing file: csi_data_2025-05-07_20-08-50.980.csv\n",
      "2025-09-11 22:46:14,981 - ERROR - Failed to process csi_data_2025-05-07_20-08-50.980.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 22:46:14,983 - INFO - Processing file: csi_data_2025-05-07_20-09-47.667.csv\n",
      "2025-09-11 22:46:16,575 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-09-47.667_heatmap.png\n",
      "2025-09-11 22:46:16,578 - INFO - Processing file: csi_data_2025-05-07_20-11-23.600.csv\n",
      "2025-09-11 22:46:18,295 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-11-23.600_heatmap.png\n",
      "2025-09-11 22:46:18,299 - INFO - Processing file: csi_data_2025-05-07_20-11-50.303.csv\n",
      "2025-09-11 22:46:19,847 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-11-50.303_heatmap.png\n",
      "2025-09-11 22:46:19,851 - INFO - Processing file: csi_data_2025-05-07_20-12-17.504.csv\n",
      "2025-09-11 22:46:21,273 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-12-17.504_heatmap.png\n",
      "2025-09-11 22:46:21,277 - INFO - Processing file: csi_data_2025-05-07_20-12-44.406.csv\n",
      "2025-09-11 22:46:21,283 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_20-12-44.406.csv. Skipping.\n",
      "2025-09-11 22:46:21,284 - INFO - Processing file: csi_data_2025-05-07_20-17-52.449.csv\n",
      "2025-09-11 22:46:22,895 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-17-52.449_heatmap.png\n",
      "2025-09-11 22:46:22,899 - INFO - Processing file: csi_data_2025-05-07_20-18-19.423.csv\n",
      "2025-09-11 22:46:23,005 - ERROR - Failed to process csi_data_2025-05-07_20-18-19.423.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 22:46:23,008 - INFO - Processing file: csi_data_2025-05-07_20-18-46.953.csv\n",
      "2025-09-11 22:46:24,455 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-18-46.953_heatmap.png\n",
      "2025-09-11 22:46:24,458 - INFO - Processing file: csi_data_2025-05-07_20-19-15.194.csv\n",
      "2025-09-11 22:46:26,010 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-19-15.194_heatmap.png\n",
      "2025-09-11 22:46:26,014 - INFO - Processing file: csi_data_2025-05-07_20-19-42.459.csv\n",
      "2025-09-11 22:46:26,110 - ERROR - Failed to process csi_data_2025-05-07_20-19-42.459.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 22:46:26,113 - INFO - Processing file: csi_data_2025-05-07_20-20-10.797.csv\n",
      "2025-09-11 22:46:26,204 - ERROR - Failed to process csi_data_2025-05-07_20-20-10.797.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 22:46:26,208 - INFO - Processing file: csi_data_2025-05-07_20-20-39.379.csv\n",
      "2025-09-11 22:46:26,216 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_20-20-39.379.csv. Skipping.\n",
      "2025-09-11 22:46:26,218 - INFO - Processing file: csi_data_2025-05-07_20-23-34.853.csv\n",
      "2025-09-11 22:46:26,226 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_20-23-34.853.csv. Skipping.\n",
      "2025-09-11 22:46:26,228 - INFO - Processing file: csi_data_2025-05-07_20-24-01.936.csv\n",
      "2025-09-11 22:46:26,306 - ERROR - Failed to process csi_data_2025-05-07_20-24-01.936.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 22:46:26,308 - INFO - Processing file: csi_data_2025-05-07_20-24-29.141.csv\n",
      "2025-09-11 22:46:27,902 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-24-29.141_heatmap.png\n",
      "2025-09-11 22:46:27,905 - INFO - Processing file: csi_data_2025-05-07_20-24-56.276.csv\n",
      "2025-09-11 22:46:29,187 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-24-56.276_heatmap.png\n",
      "2025-09-11 22:46:29,190 - INFO - Processing file: csi_data_2025-05-07_20-25-23.103.csv\n",
      "2025-09-11 22:46:30,620 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-25-23.103_heatmap.png\n",
      "2025-09-11 22:46:30,623 - INFO - Processing file: csi_data_2025-05-07_20-25-50.96.csv\n",
      "2025-09-11 22:46:30,629 - ERROR - No valid CSI data could be parsed from csi_data_2025-05-07_20-25-50.96.csv. Skipping.\n",
      "2025-09-11 22:46:30,630 - INFO - Processing file: csi_data_2025-05-07_20-26-17.592.csv\n",
      "2025-09-11 22:46:32,140 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-26-17.592_heatmap.png\n",
      "2025-09-11 22:46:32,145 - INFO - Processing file: csi_data_2025-05-07_20-26-44.394.csv\n",
      "2025-09-11 22:46:32,282 - ERROR - Failed to process csi_data_2025-05-07_20-26-44.394.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 22:46:32,287 - INFO - Processing file: csi_data_2025-05-07_20-27-13.951.csv\n",
      "2025-09-11 22:46:33,645 - INFO - Successfully saved heatmap to output\\csi_heatmaps\\csi_data_2025-05-07_20-27-13.951_heatmap.png\n",
      "2025-09-11 22:46:33,649 - INFO - --- All files processed. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc  # Corrected the import for 'rc'\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# --- Setup Global Configuration ---\n",
    "# Set font size globally once for efficiency\n",
    "rc('font', **{'size': 22})\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "def parse_csi_string(csi_string):\n",
    "    \"\"\"\n",
    "    Parses the string representation of CSI data into a list of complex numbers.\n",
    "    Example input: '[108 -64 6 0 -21 14 ...]'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove brackets and split by space\n",
    "        csi_string = csi_string.strip('[]')\n",
    "        csi_values = [int(v) for v in csi_string.split() if v]\n",
    "        \n",
    "        # Group into I and Q pairs and convert to complex numbers.\n",
    "        # The first two values are often metadata, so we skip them.\n",
    "        # CSI data starts from the 3rd value.\n",
    "        i_values = csi_values[2::2]\n",
    "        q_values = csi_values[3::2]\n",
    "        \n",
    "        csi_complex = [complex(i, q) for i, q in zip(i_values, q_values)]\n",
    "        return csi_complex\n",
    "    except (ValueError, IndexError) as e:\n",
    "        logging.warning(f\"Could not parse CSI string: '{csi_string[:30]}...'. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_heatmap_for_file(csv_path, output_dir):\n",
    "    \"\"\"\n",
    "    Reads a CSI data CSV, processes it, and saves a heatmap image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Processing file: {csv_path.name}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Check if the required 'CSI_Data' column exists\n",
    "        if 'CSI_Data' not in df.columns:\n",
    "            logging.error(f\"'CSI_Data' column not found in {csv_path.name}. Skipping file.\")\n",
    "            return\n",
    "\n",
    "        # Apply the parsing function to each row\n",
    "        # dropna() handles rows where parsing might fail\n",
    "        csi_parsed = df['CSI_Data'].apply(parse_csi_string).dropna()\n",
    "        \n",
    "        if csi_parsed.empty:\n",
    "            logging.error(f\"No valid CSI data could be parsed from {csv_path.name}. Skipping.\")\n",
    "            return\n",
    "            \n",
    "        # Stack the lists of complex numbers into a 2D numpy array\n",
    "        csi_matrix = np.stack(csi_parsed.values)\n",
    "        \n",
    "        # Calculate the amplitude (magnitude) of the complex numbers\n",
    "        amplitude_matrix = np.abs(csi_matrix)\n",
    "        \n",
    "        # --- Generate and Save the Heatmap ---\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        sns.heatmap(\n",
    "            amplitude_matrix.T,  # Transpose for subcarriers on y-axis\n",
    "            cmap='coolwarm',\n",
    "            xticklabels=100,     # Show a tick every 100 packets\n",
    "            yticklabels=5        # Show a tick every 5 subcarriers\n",
    "        )\n",
    "        \n",
    "        plt.xlabel('Time (Packet Number)')\n",
    "        plt.ylabel('Subcarrier Index')\n",
    "        \n",
    "        output_filename = output_dir / f\"{csv_path.stem}_heatmap.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_filename, dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        logging.info(f\"Successfully saved heatmap to {output_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {csv_path.name}. Error: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to find and process all CSI CSV files.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    # Directory containing your raw CSI CSV files\n",
    "    CSI_INPUT_DIR = Path(\"data/csi\") \n",
    "    # Directory where the output heatmaps will be saved\n",
    "    HEATMAP_OUTPUT_DIR = Path(\"output/csi_heatmaps\")\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    HEATMAP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find all CSV files in the input directory\n",
    "    csi_files = list(CSI_INPUT_DIR.glob(\"*.csv\"))\n",
    "    \n",
    "    if not csi_files:\n",
    "        logging.warning(f\"No CSV files found in directory: {CSI_INPUT_DIR}\")\n",
    "        return\n",
    "        \n",
    "    logging.info(f\"Found {len(csi_files)} CSI files to process.\")\n",
    "    \n",
    "    # Process each file\n",
    "    for file_path in csi_files:\n",
    "        generate_heatmap_for_file(file_path, HEATMAP_OUTPUT_DIR)\n",
    "        \n",
    "    logging.info(\"--- All files processed. ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 21:09:27,101 - INFO - Found 62 matching file pairs. Starting analysis...\n",
      "2025-09-11 21:09:27,103 - INFO - --- Processing pair for timestamp: 2024-08-17_23-16-37.224 ---\n",
      "2025-09-11 21:09:27,112 - INFO - Loaded audio: 2024-08-17_23-16-37-224.wav\n",
      "2025-09-11 21:09:27,124 - INFO - Loaded and parsed csi_data_2024-08-17_23-16-37.224.csv: 33 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:27,156 - WARNING - Data is short, reducing max lag to 10 packets.\n",
      "2025-09-11 21:09:28,301 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2024-08-17_23-16-37.224_xcorr_heatmap.png\n",
      "2025-09-11 21:09:28,304 - INFO - --- Processing pair for timestamp: 2024-09-25_15-55-29.664 ---\n",
      "2025-09-11 21:09:28,307 - INFO - Loaded audio: 2024-09-25_15-55-29.664_1.wav\n",
      "2025-09-11 21:09:28,370 - INFO - Loaded and parsed csi_data_2024-09-25_15-55-29.664_1.csv: 389 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:29,714 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2024-09-25_15-55-29.664_xcorr_heatmap.png\n",
      "2025-09-11 21:09:29,716 - INFO - --- Processing pair for timestamp: 2024-09-25_16-27-42.805 ---\n",
      "2025-09-11 21:09:29,724 - INFO - Loaded audio: 2024-09-25_16-27-42.805.wav\n",
      "2025-09-11 21:09:29,782 - INFO - Loaded and parsed csi_data_2024-09-25_16-27-42.805.csv: 398 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:30,972 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2024-09-25_16-27-42.805_xcorr_heatmap.png\n",
      "2025-09-11 21:09:30,974 - INFO - --- Processing pair for timestamp: 2024-09-25_22-01-04.590 ---\n",
      "2025-09-11 21:09:30,979 - INFO - Loaded audio: 2024-09-25_22-01-04.590.wav\n",
      "2025-09-11 21:09:30,998 - INFO - Loaded and parsed csi_data_2024-09-25_22-01-04.590.csv: 98 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:31,016 - WARNING - Data is short, reducing max lag to 32 packets.\n",
      "2025-09-11 21:09:32,095 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2024-09-25_22-01-04.590_xcorr_heatmap.png\n",
      "2025-09-11 21:09:32,098 - INFO - --- Processing pair for timestamp: 2024-10-27_20-21-47.356 ---\n",
      "2025-09-11 21:09:32,104 - INFO - Loaded audio: 2024-10-27_20-21-47.356.wav\n",
      "2025-09-11 21:09:32,119 - INFO - Loaded and parsed csi_data_2024-10-27_20-21-47.356.csv: 50 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:32,144 - WARNING - Data is short, reducing max lag to 16 packets.\n",
      "2025-09-11 21:09:33,299 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2024-10-27_20-21-47.356_xcorr_heatmap.png\n",
      "2025-09-11 21:09:33,301 - INFO - --- Processing pair for timestamp: 2024-10-27_20-28-20.897 ---\n",
      "2025-09-11 21:09:33,309 - INFO - Loaded audio: 2024-10-27_20-28-20.897.wav\n",
      "2025-09-11 21:09:33,325 - INFO - Loaded and parsed csi_data_2024-10-27_20-28-20.897.csv: 24 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:33,375 - WARNING - Data is short, reducing max lag to 7 packets.\n",
      "2025-09-11 21:09:34,388 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2024-10-27_20-28-20.897_xcorr_heatmap.png\n",
      "2025-09-11 21:09:34,390 - INFO - --- Processing pair for timestamp: 2025-05-07_13-34-06.154 ---\n",
      "2025-09-11 21:09:34,397 - INFO - Loaded audio: 2025-05-07_13-34-06.154.wav\n",
      "2025-09-11 21:09:34,406 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_13-34-06.154.csv.\n",
      "2025-09-11 21:09:34,408 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:34,410 - INFO - --- Processing pair for timestamp: 2025-05-07_13-56-11.157 ---\n",
      "2025-09-11 21:09:34,417 - INFO - Loaded audio: 2025-05-07_13-56-11.157.wav\n",
      "2025-09-11 21:09:34,427 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_13-56-11.157.csv.\n",
      "2025-09-11 21:09:34,428 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:34,431 - INFO - --- Processing pair for timestamp: 2025-05-07_13-56-56.847 ---\n",
      "2025-09-11 21:09:34,439 - INFO - Loaded audio: 2025-05-07_13-56-56.847.wav\n",
      "2025-09-11 21:09:34,460 - INFO - Loaded and parsed csi_data_2025-05-07_13-56-56.847.csv: 144 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:34,479 - WARNING - Data is short, reducing max lag to 47 packets.\n",
      "2025-09-11 21:09:35,690 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_13-56-56.847_xcorr_heatmap.png\n",
      "2025-09-11 21:09:35,692 - INFO - --- Processing pair for timestamp: 2025-05-07_14-05-11.578 ---\n",
      "2025-09-11 21:09:35,698 - INFO - Loaded audio: 2025-05-07_14-05-11.578.wav\n",
      "2025-09-11 21:09:35,709 - INFO - Loaded and parsed csi_data_2025-05-07_14-05-11.578.csv: 25 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:35,729 - WARNING - Data is short, reducing max lag to 8 packets.\n",
      "2025-09-11 21:09:36,734 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_14-05-11.578_xcorr_heatmap.png\n",
      "2025-09-11 21:09:36,737 - INFO - --- Processing pair for timestamp: 2025-05-07_19-43-03.811 ---\n",
      "2025-09-11 21:09:36,744 - INFO - Loaded audio: 2025-05-07_19-43-03.811.wav\n",
      "2025-09-11 21:09:36,747 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_19-43-03.811.csv. Error: No columns to parse from file\n",
      "2025-09-11 21:09:36,748 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:36,750 - INFO - --- Processing pair for timestamp: 2025-05-07_19-44-02.289 ---\n",
      "2025-09-11 21:09:36,757 - INFO - Loaded audio: 2025-05-07_19-44-02.289.wav\n",
      "2025-09-11 21:09:36,767 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_19-44-02.289.csv.\n",
      "2025-09-11 21:09:36,768 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:36,770 - INFO - --- Processing pair for timestamp: 2025-05-07_19-45-00.183 ---\n",
      "2025-09-11 21:09:36,777 - INFO - Loaded audio: 2025-05-07_19-45-00.183.wav\n",
      "2025-09-11 21:09:36,934 - INFO - Loaded and parsed csi_data_2025-05-07_19-45-00.183.csv: 1297 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:38,395 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-45-00.183_xcorr_heatmap.png\n",
      "2025-09-11 21:09:38,399 - INFO - --- Processing pair for timestamp: 2025-05-07_19-45-29.505 ---\n",
      "2025-09-11 21:09:38,405 - INFO - Loaded audio: 2025-05-07_19-45-29.505.wav\n",
      "2025-09-11 21:09:38,567 - INFO - Loaded and parsed csi_data_2025-05-07_19-45-29.505.csv: 1236 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:39,912 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-45-29.505_xcorr_heatmap.png\n",
      "2025-09-11 21:09:39,915 - INFO - --- Processing pair for timestamp: 2025-05-07_19-45-56.181 ---\n",
      "2025-09-11 21:09:39,921 - INFO - Loaded audio: 2025-05-07_19-45-56.181.wav\n",
      "2025-09-11 21:09:40,089 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_19-45-56.181.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 21:09:40,092 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:40,094 - INFO - --- Processing pair for timestamp: 2025-05-07_19-46-29.918 ---\n",
      "2025-09-11 21:09:40,100 - INFO - Loaded audio: 2025-05-07_19-46-29.918.wav\n",
      "2025-09-11 21:09:40,110 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_19-46-29.918.csv.\n",
      "2025-09-11 21:09:40,113 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:40,114 - INFO - --- Processing pair for timestamp: 2025-05-07_19-47-24.449 ---\n",
      "2025-09-11 21:09:40,122 - INFO - Loaded audio: 2025-05-07_19-47-24.449.wav\n",
      "2025-09-11 21:09:40,286 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_19-47-24.449.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 21:09:40,292 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:40,295 - INFO - --- Processing pair for timestamp: 2025-05-07_19-47-51.453 ---\n",
      "2025-09-11 21:09:40,303 - INFO - Loaded audio: 2025-05-07_19-47-51.453.wav\n",
      "2025-09-11 21:09:40,475 - INFO - Loaded and parsed csi_data_2025-05-07_19-47-51.453.csv: 1298 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:41,900 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-47-51.453_xcorr_heatmap.png\n",
      "2025-09-11 21:09:41,903 - INFO - --- Processing pair for timestamp: 2025-05-07_19-49-32.600 ---\n",
      "2025-09-11 21:09:41,910 - INFO - Loaded audio: 2025-05-07_19-49-32.600.wav\n",
      "2025-09-11 21:09:42,078 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_19-49-32.600.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 21:09:42,083 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:42,086 - INFO - --- Processing pair for timestamp: 2025-05-07_19-50-25.260 ---\n",
      "2025-09-11 21:09:42,094 - INFO - Loaded audio: 2025-05-07_19-50-25.260.wav\n",
      "2025-09-11 21:09:42,104 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_19-50-25.260.csv.\n",
      "2025-09-11 21:09:42,107 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:42,108 - INFO - --- Processing pair for timestamp: 2025-05-07_19-50-56.583 ---\n",
      "2025-09-11 21:09:42,117 - INFO - Loaded audio: 2025-05-07_19-50-56.583.wav\n",
      "2025-09-11 21:09:42,293 - INFO - Loaded and parsed csi_data_2025-05-07_19-50-56.583.csv: 1228 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:43,657 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-50-56.583_xcorr_heatmap.png\n",
      "2025-09-11 21:09:43,661 - INFO - --- Processing pair for timestamp: 2025-05-07_19-51-23.647 ---\n",
      "2025-09-11 21:09:43,668 - INFO - Loaded audio: 2025-05-07_19-51-23.647.wav\n",
      "2025-09-11 21:09:43,677 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_19-51-23.647.csv.\n",
      "2025-09-11 21:09:43,681 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:43,683 - INFO - --- Processing pair for timestamp: 2025-05-07_19-51-50.679 ---\n",
      "2025-09-11 21:09:43,690 - INFO - Loaded audio: 2025-05-07_19-51-50.679.wav\n",
      "2025-09-11 21:09:43,869 - INFO - Loaded and parsed csi_data_2025-05-07_19-51-50.679.csv: 1448 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:45,410 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-51-50.679_xcorr_heatmap.png\n",
      "2025-09-11 21:09:45,413 - INFO - --- Processing pair for timestamp: 2025-05-07_19-52-17.730 ---\n",
      "2025-09-11 21:09:45,419 - INFO - Loaded audio: 2025-05-07_19-52-17.730.wav\n",
      "2025-09-11 21:09:45,629 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_19-52-17.730.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 21:09:45,636 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:09:45,638 - INFO - --- Processing pair for timestamp: 2025-05-07_19-52-45.189 ---\n",
      "2025-09-11 21:09:45,644 - INFO - Loaded audio: 2025-05-07_19-52-45.189.wav\n",
      "2025-09-11 21:09:45,841 - INFO - Loaded and parsed csi_data_2025-05-07_19-52-45.189.csv: 1396 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:47,289 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-52-45.189_xcorr_heatmap.png\n",
      "2025-09-11 21:09:47,292 - INFO - --- Processing pair for timestamp: 2025-05-07_19-53-13.777 ---\n",
      "2025-09-11 21:09:47,298 - INFO - Loaded audio: 2025-05-07_19-53-13.777.wav\n",
      "2025-09-11 21:09:47,489 - INFO - Loaded and parsed csi_data_2025-05-07_19-53-13.777.csv: 1426 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:48,925 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-53-13.777_xcorr_heatmap.png\n",
      "2025-09-11 21:09:48,929 - INFO - --- Processing pair for timestamp: 2025-05-07_19-53-40.821 ---\n",
      "2025-09-11 21:09:48,935 - INFO - Loaded audio: 2025-05-07_19-53-40.821.wav\n",
      "2025-09-11 21:09:49,101 - INFO - Loaded and parsed csi_data_2025-05-07_19-53-40.821.csv: 1504 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:50,403 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-53-40.821_xcorr_heatmap.png\n",
      "2025-09-11 21:09:50,405 - INFO - --- Processing pair for timestamp: 2025-05-07_19-57-21.338 ---\n",
      "2025-09-11 21:09:50,414 - INFO - Loaded audio: 2025-05-07_19-57-21.338.wav\n",
      "2025-09-11 21:09:50,615 - INFO - Loaded and parsed csi_data_2025-05-07_19-57-21.338.csv: 1032 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:52,365 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-57-21.338_xcorr_heatmap.png\n",
      "2025-09-11 21:09:52,369 - INFO - --- Processing pair for timestamp: 2025-05-07_19-57-49.127 ---\n",
      "2025-09-11 21:09:52,374 - INFO - Loaded audio: 2025-05-07_19-57-49.127.wav\n",
      "2025-09-11 21:09:52,508 - INFO - Loaded and parsed csi_data_2025-05-07_19-57-49.127.csv: 1239 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:53,609 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_19-57-49.127_xcorr_heatmap.png\n",
      "2025-09-11 21:09:53,611 - INFO - --- Processing pair for timestamp: 2025-05-07_20-00-30.518 ---\n",
      "2025-09-11 21:09:53,617 - INFO - Loaded audio: 2025-05-07_20-00-30.518.wav\n",
      "2025-09-11 21:09:53,741 - INFO - Loaded and parsed csi_data_2025-05-07_20-00-30.518.csv: 1261 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:54,892 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-00-30.518_xcorr_heatmap.png\n",
      "2025-09-11 21:09:54,897 - INFO - --- Processing pair for timestamp: 2025-05-07_20-00-57.375 ---\n",
      "2025-09-11 21:09:54,903 - INFO - Loaded audio: 2025-05-07_20-00-57.375.wav\n",
      "2025-09-11 21:09:55,016 - INFO - Loaded and parsed csi_data_2025-05-07_20-00-57.375.csv: 900 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:56,285 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-00-57.375_xcorr_heatmap.png\n",
      "2025-09-11 21:09:56,287 - INFO - --- Processing pair for timestamp: 2025-05-07_20-01-24.106 ---\n",
      "2025-09-11 21:09:56,293 - INFO - Loaded audio: 2025-05-07_20-01-24.106.wav\n",
      "2025-09-11 21:09:56,407 - INFO - Loaded and parsed csi_data_2025-05-07_20-01-24.106.csv: 1122 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:57,766 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-01-24.106_xcorr_heatmap.png\n",
      "2025-09-11 21:09:57,770 - INFO - --- Processing pair for timestamp: 2025-05-07_20-01-51.808 ---\n",
      "2025-09-11 21:09:57,775 - INFO - Loaded audio: 2025-05-07_20-01-51.808.wav\n",
      "2025-09-11 21:09:57,913 - INFO - Loaded and parsed csi_data_2025-05-07_20-01-51.808.csv: 1149 packets, 63 subcarriers.\n",
      "2025-09-11 21:09:59,203 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-01-51.808_xcorr_heatmap.png\n",
      "2025-09-11 21:09:59,206 - INFO - --- Processing pair for timestamp: 2025-05-07_20-02-18.833 ---\n",
      "2025-09-11 21:09:59,211 - INFO - Loaded audio: 2025-05-07_20-02-18.833.wav\n",
      "2025-09-11 21:09:59,408 - INFO - Loaded and parsed csi_data_2025-05-07_20-02-18.833.csv: 1060 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:00,688 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-02-18.833_xcorr_heatmap.png\n",
      "2025-09-11 21:10:00,692 - INFO - --- Processing pair for timestamp: 2025-05-07_20-02-46.107 ---\n",
      "2025-09-11 21:10:00,699 - INFO - Loaded audio: 2025-05-07_20-02-46.107.wav\n",
      "2025-09-11 21:10:00,902 - INFO - Loaded and parsed csi_data_2025-05-07_20-02-46.107.csv: 1151 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:02,110 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-02-46.107_xcorr_heatmap.png\n",
      "2025-09-11 21:10:02,113 - INFO - --- Processing pair for timestamp: 2025-05-07_20-03-39.886 ---\n",
      "2025-09-11 21:10:02,119 - INFO - Loaded audio: 2025-05-07_20-03-39.886.wav\n",
      "2025-09-11 21:10:02,261 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-03-39.886.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 21:10:02,267 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:02,270 - INFO - --- Processing pair for timestamp: 2025-05-07_20-04-07.160 ---\n",
      "2025-09-11 21:10:02,275 - INFO - Loaded audio: 2025-05-07_20-04-07.160.wav\n",
      "2025-09-11 21:10:02,284 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_20-04-07.160.csv.\n",
      "2025-09-11 21:10:02,285 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:02,287 - INFO - --- Processing pair for timestamp: 2025-05-07_20-05-00.814 ---\n",
      "2025-09-11 21:10:02,293 - INFO - Loaded audio: 2025-05-07_20-05-00.814.wav\n",
      "2025-09-11 21:10:02,415 - INFO - Loaded and parsed csi_data_2025-05-07_20-05-00.814.csv: 1143 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:03,815 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-05-00.814_xcorr_heatmap.png\n",
      "2025-09-11 21:10:03,818 - INFO - --- Processing pair for timestamp: 2025-05-07_20-05-39.689 ---\n",
      "2025-09-11 21:10:03,826 - INFO - Loaded audio: 2025-05-07_20-05-39.689.wav\n",
      "2025-09-11 21:10:03,833 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-05-39.689.csv. Error: Error tokenizing data. C error: Expected 27 fields in line 18, saw 167\n",
      "\n",
      "2025-09-11 21:10:03,834 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:03,836 - INFO - --- Processing pair for timestamp: 2025-05-07_20-07-57.121 ---\n",
      "2025-09-11 21:10:03,842 - INFO - Loaded audio: 2025-05-07_20-07-57.121.wav\n",
      "2025-09-11 21:10:03,894 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-07-57.121.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 21:10:03,896 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:03,898 - INFO - --- Processing pair for timestamp: 2025-05-07_20-08-24.123 ---\n",
      "2025-09-11 21:10:03,902 - INFO - Loaded audio: 2025-05-07_20-08-24.123.wav\n",
      "2025-09-11 21:10:04,049 - INFO - Loaded and parsed csi_data_2025-05-07_20-08-24.123.csv: 1139 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:05,351 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-08-24.123_xcorr_heatmap.png\n",
      "2025-09-11 21:10:05,354 - INFO - --- Processing pair for timestamp: 2025-05-07_20-08-50.980 ---\n",
      "2025-09-11 21:10:05,360 - INFO - Loaded audio: 2025-05-07_20-08-50.980.wav\n",
      "2025-09-11 21:10:05,454 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-08-50.980.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 21:10:05,458 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:05,459 - INFO - --- Processing pair for timestamp: 2025-05-07_20-09-47.667 ---\n",
      "2025-09-11 21:10:05,465 - INFO - Loaded audio: 2025-05-07_20-09-47.667.wav\n",
      "2025-09-11 21:10:05,585 - INFO - Loaded and parsed csi_data_2025-05-07_20-09-47.667.csv: 1174 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:06,826 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-09-47.667_xcorr_heatmap.png\n",
      "2025-09-11 21:10:06,829 - INFO - --- Processing pair for timestamp: 2025-05-07_20-11-23.600 ---\n",
      "2025-09-11 21:10:06,834 - INFO - Loaded audio: 2025-05-07_20-11-23.600.wav\n",
      "2025-09-11 21:10:06,971 - INFO - Loaded and parsed csi_data_2025-05-07_20-11-23.600.csv: 933 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:08,190 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-11-23.600_xcorr_heatmap.png\n",
      "2025-09-11 21:10:08,193 - INFO - --- Processing pair for timestamp: 2025-05-07_20-11-50.303 ---\n",
      "2025-09-11 21:10:08,198 - INFO - Loaded audio: 2025-05-07_20-11-50.303.wav\n",
      "2025-09-11 21:10:08,304 - INFO - Loaded and parsed csi_data_2025-05-07_20-11-50.303.csv: 861 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:09,574 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-11-50.303_xcorr_heatmap.png\n",
      "2025-09-11 21:10:09,578 - INFO - --- Processing pair for timestamp: 2025-05-07_20-12-17.504 ---\n",
      "2025-09-11 21:10:09,584 - INFO - Loaded audio: 2025-05-07_20-12-17.504.wav\n",
      "2025-09-11 21:10:09,689 - INFO - Loaded and parsed csi_data_2025-05-07_20-12-17.504.csv: 778 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:10,982 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-12-17.504_xcorr_heatmap.png\n",
      "2025-09-11 21:10:10,983 - INFO - --- Processing pair for timestamp: 2025-05-07_20-12-44.406 ---\n",
      "2025-09-11 21:10:10,988 - INFO - Loaded audio: 2025-05-07_20-12-44.406.wav\n",
      "2025-09-11 21:10:10,995 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_20-12-44.406.csv.\n",
      "2025-09-11 21:10:10,997 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:10,998 - INFO - --- Processing pair for timestamp: 2025-05-07_20-17-52.449 ---\n",
      "2025-09-11 21:10:11,004 - INFO - Loaded audio: 2025-05-07_20-17-52.449.wav\n",
      "2025-09-11 21:10:11,120 - INFO - Loaded and parsed csi_data_2025-05-07_20-17-52.449.csv: 1042 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:12,404 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-17-52.449_xcorr_heatmap.png\n",
      "2025-09-11 21:10:12,406 - INFO - --- Processing pair for timestamp: 2025-05-07_20-18-19.423 ---\n",
      "2025-09-11 21:10:12,411 - INFO - Loaded audio: 2025-05-07_20-18-19.423.wav\n",
      "2025-09-11 21:10:12,530 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-18-19.423.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 21:10:12,533 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:12,535 - INFO - --- Processing pair for timestamp: 2025-05-07_20-18-46.953 ---\n",
      "2025-09-11 21:10:12,541 - INFO - Loaded audio: 2025-05-07_20-18-46.953.wav\n",
      "2025-09-11 21:10:12,647 - INFO - Loaded and parsed csi_data_2025-05-07_20-18-46.953.csv: 925 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:13,942 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-18-46.953_xcorr_heatmap.png\n",
      "2025-09-11 21:10:13,944 - INFO - --- Processing pair for timestamp: 2025-05-07_20-19-15.194 ---\n",
      "2025-09-11 21:10:13,949 - INFO - Loaded audio: 2025-05-07_20-19-15.194.wav\n",
      "2025-09-11 21:10:14,054 - INFO - Loaded and parsed csi_data_2025-05-07_20-19-15.194.csv: 1034 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:15,304 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-19-15.194_xcorr_heatmap.png\n",
      "2025-09-11 21:10:15,307 - INFO - --- Processing pair for timestamp: 2025-05-07_20-19-42.459 ---\n",
      "2025-09-11 21:10:15,313 - INFO - Loaded audio: 2025-05-07_20-19-42.459.wav\n",
      "2025-09-11 21:10:15,477 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-19-42.459.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 21:10:15,481 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:15,483 - INFO - --- Processing pair for timestamp: 2025-05-07_20-20-10.797 ---\n",
      "2025-09-11 21:10:15,488 - INFO - Loaded audio: 2025-05-07_20-20-10.797.wav\n",
      "2025-09-11 21:10:15,625 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-20-10.797.csv. Error: all input arrays must have the same shape\n",
      "2025-09-11 21:10:15,629 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:15,632 - INFO - --- Processing pair for timestamp: 2025-05-07_20-20-39.379 ---\n",
      "2025-09-11 21:10:15,638 - INFO - Loaded audio: 2025-05-07_20-20-39.379.wav\n",
      "2025-09-11 21:10:15,648 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_20-20-39.379.csv.\n",
      "2025-09-11 21:10:15,650 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:15,651 - INFO - --- Processing pair for timestamp: 2025-05-07_20-23-34.853 ---\n",
      "2025-09-11 21:10:15,656 - INFO - Loaded audio: 2025-05-07_20-23-34.853.wav\n",
      "2025-09-11 21:10:15,665 - ERROR - No valid CSI data parsed from csi_data_2025-05-07_20-23-34.853.csv.\n",
      "2025-09-11 21:10:15,667 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:15,670 - INFO - --- Processing pair for timestamp: 2025-05-07_20-24-01.936 ---\n",
      "2025-09-11 21:10:15,678 - INFO - Loaded audio: 2025-05-07_20-24-01.936.wav\n",
      "2025-09-11 21:10:15,776 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-24-01.936.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 21:10:15,779 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:15,781 - INFO - --- Processing pair for timestamp: 2025-05-07_20-24-29.141 ---\n",
      "2025-09-11 21:10:15,786 - INFO - Loaded audio: 2025-05-07_20-24-29.141.wav\n",
      "2025-09-11 21:10:15,931 - INFO - Loaded and parsed csi_data_2025-05-07_20-24-29.141.csv: 1062 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:17,236 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-24-29.141_xcorr_heatmap.png\n",
      "2025-09-11 21:10:17,238 - INFO - --- Processing pair for timestamp: 2025-05-07_20-24-56.276 ---\n",
      "2025-09-11 21:10:17,243 - INFO - Loaded audio: 2025-05-07_20-24-56.276.wav\n",
      "2025-09-11 21:10:17,328 - INFO - Loaded and parsed csi_data_2025-05-07_20-24-56.276.csv: 717 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:18,535 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-24-56.276_xcorr_heatmap.png\n",
      "2025-09-11 21:10:18,537 - INFO - --- Processing pair for timestamp: 2025-05-07_20-25-23.103 ---\n",
      "2025-09-11 21:10:18,541 - INFO - Loaded audio: 2025-05-07_20-25-23.103.wav\n",
      "2025-09-11 21:10:18,654 - INFO - Loaded and parsed csi_data_2025-05-07_20-25-23.103.csv: 975 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:19,938 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-25-23.103_xcorr_heatmap.png\n",
      "2025-09-11 21:10:19,940 - INFO - --- Processing pair for timestamp: 2025-05-07_20-26-17.592 ---\n",
      "2025-09-11 21:10:19,945 - INFO - Loaded audio: 2025-05-07_20-26-17.592.wav\n",
      "2025-09-11 21:10:20,071 - INFO - Loaded and parsed csi_data_2025-05-07_20-26-17.592.csv: 752 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:21,262 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-26-17.592_xcorr_heatmap.png\n",
      "2025-09-11 21:10:21,264 - INFO - --- Processing pair for timestamp: 2025-05-07_20-26-44.394 ---\n",
      "2025-09-11 21:10:21,270 - INFO - Loaded audio: 2025-05-07_20-26-44.394.wav\n",
      "2025-09-11 21:10:21,421 - ERROR - Failed to load raw CSI from csi_data_2025-05-07_20-26-44.394.csv. Error: 'float' object has no attribute 'strip'\n",
      "2025-09-11 21:10:21,424 - WARNING - Skipping pair due to data loading failure.\n",
      "2025-09-11 21:10:21,426 - INFO - --- Processing pair for timestamp: 2025-05-07_20-27-13.951 ---\n",
      "2025-09-11 21:10:21,432 - INFO - Loaded audio: 2025-05-07_20-27-13.951.wav\n",
      "2025-09-11 21:10:21,522 - INFO - Loaded and parsed csi_data_2025-05-07_20-27-13.951.csv: 617 packets, 63 subcarriers.\n",
      "2025-09-11 21:10:22,785 - INFO - Successfully saved heatmap to output\\cross_correlation_heatmaps\\2025-05-07_20-27-13.951_xcorr_heatmap.png\n",
      "2025-09-11 21:10:22,787 - INFO - --- Batch processing complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Set global font size for all plots using rc ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.WARNING)\n",
    "\n",
    "# --- Data Parsing and Loading Functions ---\n",
    "\n",
    "def parse_csi_string(csi_string):\n",
    "    \"\"\"Parses the string representation of CSI data into a list of complex numbers.\"\"\"\n",
    "    try:\n",
    "        csi_values = [int(v) for v in csi_string.strip('[]').split() if v]\n",
    "        i_values = csi_values[2::2]\n",
    "        q_values = csi_values[3::2]\n",
    "        return [complex(i, q) for i, q in zip(i_values, q_values)]\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "def load_raw_csi_data(csi_path):\n",
    "    \"\"\"Loads a raw CSI CSV and returns a complex numpy array of (time, subcarriers).\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csi_path)\n",
    "        if 'CSI_Data' not in df.columns:\n",
    "            logging.error(f\"'CSI_Data' column not found in {csi_path.name}.\")\n",
    "            return None\n",
    "        \n",
    "        csi_parsed = df['CSI_Data'].apply(parse_csi_string).dropna()\n",
    "        if csi_parsed.empty:\n",
    "            logging.error(f\"No valid CSI data parsed from {csi_path.name}.\")\n",
    "            return None\n",
    "            \n",
    "        csi_matrix = np.stack(csi_parsed.values)\n",
    "        logging.info(f\"Loaded and parsed {csi_path.name}: {csi_matrix.shape[0]} packets, {csi_matrix.shape[1]} subcarriers.\")\n",
    "        return csi_matrix\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load raw CSI from {csi_path.name}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    \"\"\"Loads a WAV file and converts it to mono.\"\"\"\n",
    "    try:\n",
    "        audio, sr = sf.read(audio_path)\n",
    "        if audio.ndim > 1:\n",
    "            audio = np.mean(audio, axis=1)\n",
    "        logging.info(f\"Loaded audio: {audio_path.name}\")\n",
    "        return audio, sr\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio {audio_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- Main Analysis Function ---\n",
    "\n",
    "def generate_xcorr_heatmap_for_pair(audio_path, csi_path, output_dir, timestamp):\n",
    "    \"\"\"\n",
    "    Generates a cross-correlation heatmap between audio RMS and CSI phase change for a given pair.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Processing pair for timestamp: {timestamp} ---\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    audio, sr = load_audio(audio_path)\n",
    "    csi_complex = load_raw_csi_data(csi_path)\n",
    "    \n",
    "    if audio is None or csi_complex is None:\n",
    "        logging.warning(\"Skipping pair due to data loading failure.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 2. Extract Features\n",
    "        # Audio RMS Envelope\n",
    "        rms = librosa.feature.rms(y=audio, hop_length=512)[0]\n",
    "        \n",
    "        # CSI Phase Difference (sensitive to motion)\n",
    "        # Transpose to (subcarriers, time) for processing\n",
    "        csi_complex_T = csi_complex.T \n",
    "        csi_phase = np.unwrap(np.angle(csi_complex_T), axis=1)\n",
    "        csi_phase_diff = np.diff(csi_phase, axis=1)\n",
    "\n",
    "        # 3. Align Signals\n",
    "        # We align the audio RMS to the (shorter) CSI phase diff timeline\n",
    "        num_csi_packets = csi_phase_diff.shape[1]\n",
    "        rms_aligned = np.interp(\n",
    "            np.linspace(0, 1, num_csi_packets),\n",
    "            np.linspace(0, 1, len(rms)),\n",
    "            rms\n",
    "        )\n",
    "\n",
    "        # 4. Standardize for cross-correlation\n",
    "        scaler = StandardScaler()\n",
    "        rms_scaled = scaler.fit_transform(rms_aligned.reshape(-1, 1)).flatten()\n",
    "        csi_scaled = scaler.fit_transform(csi_phase_diff.T).T\n",
    "\n",
    "        # 5. Calculate Cross-Correlation\n",
    "        max_lag_samples = 100 # Look at +/- 100 packets lag\n",
    "        n_subcarriers = csi_scaled.shape[0]\n",
    "        n_packets = csi_scaled.shape[1]\n",
    "        \n",
    "        # Ensure max_lag isn't too large for the data\n",
    "        if n_packets <= 2 * max_lag_samples:\n",
    "            max_lag_samples = n_packets // 3\n",
    "            logging.warning(f\"Data is short, reducing max lag to {max_lag_samples} packets.\")\n",
    "\n",
    "        xcorr_matrix = np.zeros((n_subcarriers, 2 * max_lag_samples + 1))\n",
    "\n",
    "        for i in range(n_subcarriers):\n",
    "            correlation = signal.correlate(csi_scaled[i, :], rms_scaled, mode='full')\n",
    "            mid_point = len(correlation) // 2\n",
    "            start_idx = mid_point - max_lag_samples\n",
    "            end_idx = mid_point + max_lag_samples + 1\n",
    "            xcorr_matrix[i, :] = correlation[start_idx:end_idx]\n",
    "\n",
    "        # 6. Generate and Save Heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        rc('font', **{'size': 22})\n",
    "        sns.heatmap(\n",
    "            xcorr_matrix,\n",
    "            cmap='inferno_r',\n",
    "            center=0,\n",
    "            xticklabels=25, # Show a tick every 25 lags\n",
    "            yticklabels=10  # Show a tick every 10 subcarriers\n",
    "        )\n",
    "        \n",
    "        # Set x-axis labels to be the actual lag\n",
    "        x_ticks = plt.xticks()[0]\n",
    "        plt.xticks(x_ticks, [f\"{int(lag - max_lag_samples)}\" for lag in x_ticks])\n",
    "        \n",
    "        plt.xlabel('Time Lag (Packets)')\n",
    "        plt.ylabel('Subcarrier Index')\n",
    "        \n",
    "        output_filename = output_dir / f\"{timestamp}_xcorr_heatmap.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_filename, dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        logging.info(f\"Successfully saved heatmap to {output_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed during analysis for {timestamp}. Error: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    AUDIO_DIR = Path(\"data/audio\")\n",
    "    CSI_INPUT_DIR = Path(\"data/csi\")\n",
    "    BASE_OUTPUT_DIR = Path(\"output/cross_correlation_heatmaps\")\n",
    "    BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    TIMESTAMP_RE = re.compile(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})[.-](\\d{3})')\n",
    "\n",
    "    def normalize_timestamp(filename):\n",
    "        match = TIMESTAMP_RE.search(filename)\n",
    "        if match:\n",
    "            return f\"{match.group(1)}.{match.group(2)}\"\n",
    "        return None\n",
    "\n",
    "    audio_files = {normalize_timestamp(f.name): f for f in AUDIO_DIR.glob(\"*.wav\") if normalize_timestamp(f.name)}\n",
    "    csi_files = {normalize_timestamp(f.name): f for f in CSI_INPUT_DIR.glob(\"*.csv\") if normalize_timestamp(f.name)}\n",
    "    \n",
    "    common_timestamps = sorted(list(set(audio_files.keys()) & set(csi_files.keys())))\n",
    "    \n",
    "    if not common_timestamps:\n",
    "        logging.warning(\"No matching Audio/CSI file pairs found. Check your directories and filenames.\")\n",
    "        logging.warning(\"Ensure timestamps match (e.g., '..._15-55-29.664.wav' and '..._15-55-29-677.csv')\")\n",
    "        return\n",
    "        \n",
    "    logging.info(f\"Found {len(common_timestamps)} matching file pairs. Starting analysis...\")\n",
    "\n",
    "    for timestamp in common_timestamps:\n",
    "        audio_path = audio_files[timestamp]\n",
    "        csi_path = csi_files[timestamp]\n",
    "        \n",
    "        generate_xcorr_heatmap_for_pair(audio_path, csi_path, BASE_OUTPUT_DIR, timestamp)\n",
    "        \n",
    "    logging.info(\"--- Batch processing complete. ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # A small fix to the parsing function was needed.\n",
    "    # The original script had a typo `cvi_values`. This is corrected here.\n",
    "    def parse_csi_string(csi_string):\n",
    "        \"\"\"Parses the string representation of CSI data into a list of complex numbers.\"\"\"\n",
    "        try:\n",
    "            csi_values = [int(v) for v in csi_string.strip('[]').split() if v]\n",
    "            i_values = csi_values[2::2]\n",
    "            q_values = csi_values[3::2] # Corrected from cvi_values\n",
    "            return [complex(i, q) for i, q in zip(i_values, q_values)]\n",
    "        except (ValueError, IndexError):\n",
    "            return None\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6527049,
     "sourceId": 10549085,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
